{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AmhfxdCR86jB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedGroupKFold\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForWholeWordMask\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n",
    "from transformers.modeling_outputs  import BaseModelOutput,SequenceClassifierOutput\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "# imports the torch_xla package\n",
    "import wandb\n",
    "from torch.nn.parameter import Parameter\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1dMoHaCUn29I",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Model training')\n",
    "    # params of training\n",
    "    parser.add_argument(\n",
    "        \"--fold\", dest=\"fold\", help=\"Train fold\", default=None, type=int)\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        dest='batch_size',\n",
    "        help='Mini batch size of one gpu or cpu',\n",
    "        type=int,\n",
    "        default=None)\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6G2r5GS86jE"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Bojtddae86jG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    #pretraining = True\n",
    "    pretraining = False\n",
    "    load_pretrained = False\n",
    "    input_path = './input/'\n",
    "    input_type = '1'\n",
    "    #model_path = 'microsoft/deberta-v3-base' #  nghuyong/ernie-2.0-large-en studio-ousia/luke-large\n",
    "    model_path = 'microsoft/deberta-v3-large'\n",
    "    #model_type = 'base' #Peng's code use simple\n",
    "    model_type = 'base'\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5  # 1.5\n",
    "    num_warmup_steps = 0\n",
    "    max_input_length = 512\n",
    "    max_position_embeddings = 512\n",
    "    folds = [0]\n",
    "    epochs = 5  # 5\n",
    "    # layer - wise larning rate \n",
    "    discriminative_learning_rate = False\n",
    "    discriminative_learning_rate_num_groups = 1\n",
    "    discriminative_learning_rate_decay_rate = 0.99\n",
    "    # reinint layer\n",
    "    reinit_layers = 0\n",
    "    \n",
    "    #encoder_lr = 50e-6\n",
    "    encoder_lr=2e-6\n",
    "    #head_lr = 10e-4\n",
    "    head_lr=2e-6\n",
    "    \n",
    "    min_lr = 1e-7\n",
    "    eps = 1e-7\n",
    "    betas = (0.9, 0.999)\n",
    "    weight_decay = 0\n",
    "    dropout = 0\n",
    "    num_fold = 5\n",
    "    #batch_size = 16 #use batch_size as 8\n",
    "    batch_size = 2\n",
    "    seed = 1006\n",
    "    OUTPUT_DIR = './pretrain/'\n",
    "    num_workers = 2\n",
    "    device='cuda'\n",
    "    print_freq = 100\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FaoPHcM86jH"
   },
   "source": [
    "## logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RWkPxbhq86jI",
    "outputId": "1f97e316-be01-408d-c79b-1b128aaff9c9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===============lr_2e-06===============\n",
      "===============seed_1006===============\n",
      "===============total_epochs_5===============\n",
      "===============num_warmup_steps_0===============\n"
     ]
    }
   ],
   "source": [
    "def get_logger(filename=CFG.OUTPUT_DIR+ 'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    if not os.path.exists(CFG.OUTPUT_DIR):\n",
    "        os.makedirs(CFG.OUTPUT_DIR)\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "LOGGER.info('===============lr_{}==============='.format(CFG.encoder_lr))\n",
    "LOGGER.info('===============seed_{}==============='.format(CFG.seed))\n",
    "LOGGER.info('===============total_epochs_{}==============='.format(CFG.epochs))\n",
    "LOGGER.info('===============num_warmup_steps_{}==============='.format(CFG.num_warmup_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSqTonVY86jJ"
   },
   "source": [
    "# Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ACTTNzFL86jK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf = pd.read_csv(f\"{CFG.input_path}/prompts_train.csv\")\n",
    "sdf = pd.read_csv(f\"{CFG.input_path}/summaries_train.csv\")\n",
    "\n",
    "df = pdf.merge(sdf, on=\"prompt_id\")\n",
    "\n",
    "# 4 prompt ids, 4 folds\n",
    "id2fold = {\n",
    "    \"814d6b\": 0,\n",
    "    \"39c16e\": 1,\n",
    "    \"3b9047\": 2,\n",
    "    \"ebad26\": 3,\n",
    "}\n",
    "\n",
    "df[\"fold\"] = df[\"prompt_id\"].map(id2fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "FY5ZdDSh86jL",
    "outputId": "f25ff859-20f1-4a74-f363-eb0dee6c3ba1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>student_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00791789cc1f</td>\n",
       "      <td>1 element of an ideal tragedy is that it shoul...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>0086ef22de8f</td>\n",
       "      <td>The three elements of an ideal tragedy are:  H...</td>\n",
       "      <td>-0.970237</td>\n",
       "      <td>-0.417058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>0094589c7a22</td>\n",
       "      <td>Aristotle states that an ideal tragedy should ...</td>\n",
       "      <td>-0.387791</td>\n",
       "      <td>-0.584181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  prompt_id                                    prompt_question prompt_title  \\\n",
       "0    39c16e  Summarize at least 3 elements of an ideal trag...   On Tragedy   \n",
       "1    39c16e  Summarize at least 3 elements of an ideal trag...   On Tragedy   \n",
       "2    39c16e  Summarize at least 3 elements of an ideal trag...   On Tragedy   \n",
       "\n",
       "                                         prompt_text    student_id  \\\n",
       "0  Chapter 13 \\r\\nAs the sequel to what has alrea...  00791789cc1f   \n",
       "1  Chapter 13 \\r\\nAs the sequel to what has alrea...  0086ef22de8f   \n",
       "2  Chapter 13 \\r\\nAs the sequel to what has alrea...  0094589c7a22   \n",
       "\n",
       "                                                text   content   wording  fold  \n",
       "0  1 element of an ideal tragedy is that it shoul... -0.210614 -0.471415     1  \n",
       "1  The three elements of an ideal tragedy are:  H... -0.970237 -0.417058     1  \n",
       "2  Aristotle states that an ideal tragedy should ... -0.387791 -0.584181     1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2bILjGET86jN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vpC_oJw86jO"
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3p6AtTj_86jO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSHcmlTt86jP"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X-XxwPIR86jP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mask_lm_datacollator = DataCollatorForWholeWordMask(tokenizer)\n",
    "# def data_collator(batch):\n",
    "#     input_ids = [{'input_ids':i[0]} for i in batch]\n",
    "#     token_type_ids = [i[1] for i in batch]\n",
    "#     attention_mask = [i[2] for i in batch]\n",
    "#     labels = [i[3] for i in batch]\n",
    "#     masked_input = mask_lm_datacollator(input_ids)['input_ids']\n",
    "#     return masked_input,\\\n",
    "#                torch.stack(token_type_ids),\\\n",
    "#                torch.stack(attention_mask),\\\n",
    "#                torch.stack(labels)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "        tokenizer=tokenizer,\n",
    "        pad_to_multiple_of=16\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.prompt_title = df['prompt_title'].values.astype(str)\n",
    "        self.prompt_text = df['prompt_text'].values.astype(str)\n",
    "        self.prompt_question = df['prompt_question'].values.astype(str)\n",
    "        self.text = df['text'].values.astype(str)\n",
    "        self.content = df['content'].values\n",
    "        self.wording = df['wording'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.prompt_title)\n",
    "    \n",
    "    def tokenize(self, example):\n",
    "        sep = self.tokenizer.sep_token\n",
    "        if  CFG.input_type == '1':\n",
    "            prompt = sep.join([example[\"prompt_title\"], example[\"prompt_text\"], example[\"prompt_question\"]])\n",
    "        else:\n",
    "            prompt = example[\"prompt_question\"] \n",
    "        \n",
    "        labels = [float(example[\"content\"]), float(example[\"wording\"])]\n",
    "        \n",
    "        # Peng's code swap prompt and example['text']\n",
    "        tokenized = tokenizer(\n",
    "            example[\"text\"],\n",
    "            prompt,\n",
    "            #padding='max_length',\n",
    "            padding = False,\n",
    "            truncation=True,\n",
    "            max_length=CFG.max_input_length,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        example = {\n",
    "                    \"prompt_title\":self.prompt_title[item],\n",
    "                    \"prompt_text\":self.prompt_text[item],\n",
    "                    \"prompt_question\":self.prompt_question[item],\n",
    "                    \"text\":self.text[item],\n",
    "                    \"content\":self.content[item],\n",
    "                    \"wording\":self.wording[item],\n",
    "                  }\n",
    "        \n",
    "        out = self.tokenize(example)\n",
    "       \n",
    "        return {\n",
    "                'input_ids': torch.as_tensor(out['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids': torch.as_tensor(out['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask': torch.as_tensor(out['attention_mask'], dtype=torch.long),\n",
    "                'labels': torch.as_tensor(out['labels'], dtype=torch.float),\n",
    "        }\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP7TFS-c86jQ"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0A7qv3qn86jQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_params(module_lst):\n",
    "    for module in module_lst:\n",
    "        for param in module.parameters():\n",
    "            if param.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "    return\n",
    "\n",
    "class Custom_Bert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\"output_hidden_states\":True})\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "\n",
    "        dim = config.hidden_size\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        n_weights = 24\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(dim,1)\n",
    "        )\n",
    "        init_params([self.cls,self.attention])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                )\n",
    "\n",
    "        cls_outputs = torch.stack(\n",
    "            [self.dropout(layer) for layer in base_output['hidden_states'][-24:]], dim=0\n",
    "        )\n",
    "        cls_output = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * cls_outputs).sum(0)\n",
    "\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [torch.sum(self.attention(self.high_dropout(cls_output)) * cls_output, dim=1) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        output = self.cls(logits)\n",
    "        if labels is None:\n",
    "            return output\n",
    "\n",
    "        else:\n",
    "            return (nn.MSELoss()(torch.squeeze(output,1),labels), output)\n",
    "\n",
    "\n",
    "class Custom_Bert_Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "        #self.base = AutoModel.from_pretrained('./input/pretrain/pretrained_model/', config = config) #Peng's code\n",
    "        dim = config.hidden_size\n",
    "        self.dropout = nn.Dropout(p=0)\n",
    "        self.cls = nn.Linear(dim,2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids\n",
    "                               )\n",
    "        output = base_output.last_hidden_state\n",
    "        output = self.cls(torch.mean(output, dim=1))\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=nn.MSELoss()(output,labels),\n",
    "            logits=output, \n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "\n",
    "class GeMText(nn.Module):\n",
    "    def __init__(self, dim = 1, p=3, eps=1e-6):\n",
    "        super(GeMText, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.feat_mult = 1\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n",
    "        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n",
    "        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n",
    "        ret = ret.pow(1 / self.p)\n",
    "        return ret    \n",
    "\n",
    "class Custom_Bert_Pool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        self.config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=self.config)\n",
    "        self.pool = GeMText()\n",
    "        self.cls = nn.Linear(self.config.hidden_size,2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids\n",
    "                               )\n",
    "        output = base_output.last_hidden_state\n",
    "        output = self.cls(self.pool(output, attention_mask))\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=nn.SmoothL1Loss()(output,labels),\n",
    "            logits=output, \n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "class Custom_Bert_Mean(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.output_hidden_states=True\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "        dim = config.hidden_size\n",
    "        self.dropout = nn.Dropout(p=0)\n",
    "        self.cls = nn.Linear(dim,1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,labels=None):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                            )\n",
    "\n",
    "\n",
    "        output = base_output.hidden_states[-1]\n",
    "        output = self.cls(self.dropout(torch.mean(output, dim=1)))\n",
    "        if labels is None:\n",
    "            return output\n",
    "\n",
    "        else:\n",
    "            return (nn.MSELoss()(torch.squeeze(output,1),labels), output)\n",
    "\n",
    "class Custom_Bert_M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\"output_hidden_states\":True})\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "\n",
    "        dim = config.hidden_size\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        n_weights = 24\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.cls_0 = nn.Sequential(\n",
    "            nn.Linear(dim,1)\n",
    "        )\n",
    "\n",
    "        self.cls_1 = nn.Linear(dim,5)\n",
    "        init_params([self.cls_0,self.cls_1,self.attention])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                             )\n",
    "\n",
    "        cls_outputs = torch.stack(\n",
    "            [self.dropout(layer) for layer in base_output['hidden_states'][-24:]], dim=0\n",
    "        )\n",
    "        cls_output = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * cls_outputs).sum(0)\n",
    "\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [torch.sum(self.attention(self.high_dropout(cls_output)) * cls_output, dim=1) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        output_0 = self.cls_0(logits)\n",
    "        output_1 = self.cls_1(logits)\n",
    "        if labels is None:\n",
    "            return output_0\n",
    "\n",
    "        else:\n",
    "            regression_loss = nn.MSELoss()(torch.squeeze(output_0,1),labels)\n",
    "            labels = labels.double()\n",
    "            cls_labels = torch.where(labels==1.,4.0,labels)\n",
    "            cls_labels = torch.where(cls_labels==0.25,1.0,cls_labels)\n",
    "            cls_labels = torch.where(cls_labels==0.5,2.0,cls_labels)\n",
    "            cls_labels = torch.where(cls_labels==0.75,3.0,cls_labels)\n",
    "            cls_labels = cls_labels.long()\n",
    "            cls_loss = nn.CrossEntropyLoss()(output_1, cls_labels)\n",
    "            return ( 0.8 * regression_loss + 0.2 * cls_loss, output_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    if CFG.model_type == 'base':\n",
    "        model_config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        model_config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "\n",
    "        #print(model_config)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            CFG.model_path, config=model_config\n",
    "        )\n",
    "    if CFG.model_type == 'simple':\n",
    "        model = Custom_Bert_Simple()\n",
    "    if CFG.model_type == 'pool':\n",
    "        model = Custom_Bert_Pool()\n",
    "        if CFG.reinit_layers > 0:\n",
    "            print(\"==\"*40)\n",
    "            print(f\"Reinitialize the last {CFG.reinit_layers} layer(s).\")\n",
    "            for layer in model.base.encoder.layer[-CFG.reinit_layers:]:\n",
    "                print(\"===\")\n",
    "                layer.apply(model._init_weights)\n",
    "            print(\"==\"*40)\n",
    "        if CFG.load_pretrained:\n",
    "            model.load_state_dict(torch.load('./pretrained/microsoft_deberta-v3-base_best_ema.pth')['model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op1OHevU86jR"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "class ModelEMA:\n",
    "    \"\"\"Model Exponential Moving Average from https://github.com/rwightman/\n",
    "    pytorch-image-models Keep a moving average of everything in the model\n",
    "    state_dict (parameters and buffers).\n",
    "\n",
    "    This is intended to allow functionality like\n",
    "    https://www.tensorflow.org/api_docs/python/tf/train/\n",
    "    ExponentialMovingAverage\n",
    "    A smoothed version of the weights is necessary for some training\n",
    "    schemes to perform well.\n",
    "    This class is sensitive where it is initialized in the sequence\n",
    "    of model init, GPU assignment and distributed training wrappers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay=0.9999, updates=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (nn.Module): model to apply EMA.\n",
    "            decay (float): ema decay reate.\n",
    "            updates (int): counter of EMA updates.\n",
    "        \"\"\"\n",
    "        # Create EMA(FP32)\n",
    "        self.ema_model = deepcopy(model).eval()\n",
    "        self.ema = self.ema_model\n",
    "        self.updates = updates\n",
    "        # decay exponential ramp (to help early epochs)\n",
    "        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        # Update EMA parameters\n",
    "        with torch.no_grad():\n",
    "            self.updates += 1\n",
    "            d = self.decay(self.updates)\n",
    "            msd =  model.state_dict()# model state_dict\n",
    "            for k, v in self.ema.state_dict().items():\n",
    "                if v.dtype.is_floating_point:\n",
    "                    v *= d\n",
    "                    v += (1.0 - d) * msd[k].detach()\n",
    "\n",
    "class EMAHook:\n",
    "    \"\"\"EMAHook used in BEVDepth.\n",
    "\n",
    "    Modified from https://github.com/Megvii-Base\n",
    "    Detection/BEVDepth/blob/main/callbacks/ema.py.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, init_updates=0, decay=0.9990, resume=None, logger=None):\n",
    "        super().__init__()\n",
    "        self.init_updates = init_updates\n",
    "        self.resume = resume\n",
    "        self.decay = decay\n",
    "        self.ema_model = self.before_run(model)\n",
    "        self.logger = logger\n",
    "\n",
    "    def before_run(self, model):\n",
    "        from torch.nn.modules.batchnorm import SyncBatchNorm\n",
    "\n",
    "        bn_model_list = list()\n",
    "        bn_model_dist_group_list = list()\n",
    "        for model_ref in model.modules():\n",
    "            if isinstance(model_ref, SyncBatchNorm):\n",
    "                bn_model_list.append(model_ref)\n",
    "                bn_model_dist_group_list.append(model_ref.process_group)\n",
    "                model_ref.process_group = None\n",
    "        ema_model = ModelEMA(model, self.decay)\n",
    "\n",
    "        for bn_model, dist_group in zip(bn_model_list,\n",
    "                                        bn_model_dist_group_list):\n",
    "            bn_model.process_group = dist_group\n",
    "        ema_model.updates = self.init_updates\n",
    "\n",
    "        if self.resume is not None:\n",
    "            self.logger.info(f'resume ema checkpoint from {self.resume}')\n",
    "            cpt = torch.load(self.resume, map_location='cpu')\n",
    "            load_state_dict(ema_model.ema, cpt['state_dict'])\n",
    "            ema_model.updates = cpt['updates']\n",
    "\n",
    "        return ema_model\n",
    "\n",
    "    def after_train_iter(self, model):\n",
    "        self.ema_model.update(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "O__vnlBV86jR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qYJKjn8H86jR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.discriminative_learning_rate_num_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_llr_params(model, type='s'):\n",
    "    \"\"\"\n",
    "    Setup the optimizer.\n",
    "    We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
    "    Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
    "\n",
    "    MODIFIED VERSION:\n",
    "    * added support for differential learning rates per layer\n",
    "\n",
    "    reference: https://github.com/huggingface/transformers/blob/05fa1a7ac17bb7aa07b9e0c1e138ecb31a28bbfe/src/transformers/trainer.py#L804\n",
    "    \"\"\"\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
    "    ### ADDED\n",
    "    if CFG.discriminative_learning_rate:\n",
    "\n",
    "        num_layers = model.config.num_hidden_layers\n",
    "\n",
    "        learning_rate_powers = range(0, num_layers, num_layers//CFG.discriminative_learning_rate_num_groups)\n",
    "        layer_wise_learning_rates = [\n",
    "            pow(CFG.discriminative_learning_rate_decay_rate, power) * CFG.encoder_lr \n",
    "            for power in learning_rate_powers \n",
    "            for _ in range(num_layers//CFG.discriminative_learning_rate_num_groups)\n",
    "          ]\n",
    "        layer_wise_learning_rates = layer_wise_learning_rates[::-1]\n",
    "        print('Layer-wise learning rates:', layer_wise_learning_rates)\n",
    "\n",
    "        # group embedding paramters from the transformer encoder\n",
    "        embedding_layer = model.base.embeddings\n",
    "        optimizer_grouped_parameters = [\n",
    "          {\n",
    "              \"params\": [p for n, p in embedding_layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "              \"lr\": pow(CFG.discriminative_learning_rate_decay_rate, num_layers) * CFG.encoder_lr ,\n",
    "              \"weight_decay\": CFG.weight_decay,\n",
    "          },\n",
    "          {\n",
    "              \"params\": [p for n, p in embedding_layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "              \"lr\": pow(CFG.discriminative_learning_rate_decay_rate, num_layers) * CFG.encoder_lr ,\n",
    "              \"weight_decay\": 0.0,\n",
    "          },\n",
    "        ]\n",
    "\n",
    "        # group encoding paramters from the transformer encoder\n",
    "        encoding_layers = [layer for layer in model.base.encoder.layer]\n",
    "        for i, layer in enumerate(encoding_layers):\n",
    "            optimizer_grouped_parameters += [\n",
    "                {\n",
    "                    \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                    \"lr\": layer_wise_learning_rates[i],\n",
    "                    \"weight_decay\": CFG.weight_decay,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                    \"lr\": layer_wise_learning_rates[i],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "            ]    \n",
    "        print(f\"Detected unattached modules in model.encoder: {[n for n, p in model.base.encoder.named_parameters() if not n.startswith('layer')]}\")\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.base.encoder.named_parameters() if not n.startswith('layer') and not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": layer_wise_learning_rates[-1],\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.base.encoder.named_parameters() if not n.startswith('layer') and any(nd in n for nd in no_decay)],\n",
    "                \"lr\": layer_wise_learning_rates[-1],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # group paramters from the task specific head\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if 'base' not in n and not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.head_lr,\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if 'base' not in n and any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.head_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    ### END ADDED\n",
    "    else:\n",
    "        # group paramters for the entire network\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.encoder_lr,\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.encoder_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3HIHxVsj86jS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    start = end = time.time()\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        for key, value in batch.items():\n",
    "            batch[key] = value.to(device)\n",
    "        batch_size = batch['labels'].size(0)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**batch)\n",
    "        label = batch['labels']\n",
    "        loss, logits = model_output.loss, model_output.logits\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(logits.to('cpu').numpy())\n",
    "        labels.append(label.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, epoch, scheduler, device, valid_loader, start_time, best_score, best_score_ema,ema_hook,wandb, fold):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        for key, value in batch.items():\n",
    "            batch[key] = value.to(device)\n",
    "        batch_size = batch['labels'].size(0)\n",
    "        loss = model(**batch).loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        optimizer.step()\n",
    "        ema_hook.after_train_iter(model)\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        \n",
    "        wandb.log({\n",
    "                'train loss': loss.item(),\n",
    "                'step': global_step,\n",
    "                'epoch': epoch\n",
    "            })\n",
    "        \n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "            \n",
    "            # eval\n",
    "            avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, model, CFG.device)\n",
    "\n",
    "            # scoring\n",
    "            score = compute_mcrmse((predictions, valid_labels))\n",
    "\n",
    "\n",
    "            content_rmse, wording_rmse, mcrmse = list(score.values())\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch + 1} avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - content_rmse: {content_rmse:.4f} - wording_rmse: {wording_rmse:.4f} - mcrmse: {mcrmse:.4f}')\n",
    "            \n",
    "            \n",
    "            if best_score > score['mcrmse']:\n",
    "                if best_score != float('inf'):\n",
    "                    os.remove(CFG.OUTPUT_DIR + \"{}_best{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score))\n",
    "                best_score = score['mcrmse']\n",
    "                best_predictions = predictions\n",
    "                LOGGER.info(f'Epoch {epoch + 1} - Save Best Score: {best_score:.4f} Model')\n",
    "                torch.save({'model': model.state_dict(),\n",
    "                            'predictions': predictions},\n",
    "                           CFG.OUTPUT_DIR + \"{}_best{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score))\n",
    "            \n",
    "            \n",
    "            avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, ema_hook.ema_model.ema, CFG.device)\n",
    "            # ema scoring\n",
    "            ema_score = compute_mcrmse((predictions, valid_labels))\n",
    "\n",
    "\n",
    "            content_rmse, wording_rmse, mcrmse = list(ema_score.values())\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch + 1} avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - ema_content_rmse: {content_rmse:.4f} - ema_wording_rmse: {wording_rmse:.4f} - ema_mcrmse: {mcrmse:.4f}')\n",
    "            \n",
    "            \n",
    "            if best_score_ema > ema_score['mcrmse']:\n",
    "                if best_score_ema != float('inf'):\n",
    "                    os.remove(CFG.OUTPUT_DIR + \"{}_best_ema{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score_ema))\n",
    "                best_score_ema = ema_score['mcrmse']\n",
    "                best_predictions = predictions\n",
    "                LOGGER.info(f'Epoch {epoch + 1} - ema_Save Best Score: {best_score_ema:.4f} Model')\n",
    "                torch.save({'model': ema_hook.ema_model.ema.state_dict(),\n",
    "                            'predictions': predictions},\n",
    "                           CFG.OUTPUT_DIR + \"{}_best_ema{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold,best_score_ema))\n",
    "            \n",
    "            wandb.log({\n",
    "            'learning rate': optimizer.param_groups[0]['lr'],\n",
    "            'validation mcrmse': score['mcrmse'],\n",
    "            'validation ema mcrmse': ema_score['mcrmse'],\n",
    "            'step': global_step,\n",
    "            'epoch': epoch,\n",
    "        })\n",
    "            \n",
    "            model.train()\n",
    "    return losses.avg, best_score, best_score_ema\n",
    "\n",
    "\n",
    "\n",
    "def train_loop():\n",
    "    LOGGER.info(f\"========== training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    #wandb.init(project='commit_summary', entity='whhhhh')\n",
    "    wandb.init(project='commit_summary')\n",
    "    \n",
    "    wandb.config = dict(epochs=CFG.epochs, \n",
    "                            batch_size=CFG.batch_size, \n",
    "                            learning_rate=CFG.encoder_lr,\n",
    "                            save_checkpoint=True,\n",
    "                            )\n",
    "    for fold in CFG.folds:\n",
    "        \n",
    "        if CFG.pretraining:\n",
    "            tr_data = pd.read_csv('tmp_pessudo.csv')\n",
    "            tr_data['prompt_title'] = ''\n",
    "            tr_data = tr_data[-(tr_data['prompt_question'].isin(pdf['prompt_question'].tolist()))]\n",
    "            va_data = df #df[df['fold']==fold].reset_index(drop=True)\n",
    "        else:\n",
    "            tr_data = df[df['fold']!=fold].reset_index(drop=True)\n",
    "            va_data = df[df['fold']==fold].reset_index(drop=True)\n",
    "        train_dataset = TrainDataset(tr_data, tokenizer)\n",
    "        valid_dataset = TrainDataset(va_data, tokenizer)\n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                                  batch_size=CFG.batch_size,\n",
    "                                  collate_fn = data_collator,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "        valid_loader = DataLoader(valid_dataset,\n",
    "                                  batch_size=CFG.batch_size * 2,\n",
    "                                  collate_fn = data_collator,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "        # ====================================================\n",
    "        # model & optimizer\n",
    "        # ====================================================\n",
    "        model = build_model()\n",
    "        #model = AutoModelForSequenceClassification.from_pretrained(CFG.model_path, num_labels=1)\n",
    "        model.to(CFG.device)\n",
    "        # for param in model.base.parameters():\n",
    "        #         param.requires_grad = False\n",
    "        ema_hook = EMAHook(model, init_updates=3000, logger=LOGGER)\n",
    "        def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "            param_optimizer = list(model.named_parameters())\n",
    "            no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_parameters = [\n",
    "                {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                 'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "                {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                 'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            ]\n",
    "            return optimizer_parameters\n",
    "\n",
    "        optimizer_parameters = get_optimizer_llr_params(model)\n",
    "        #optimizer_parameters = get_optimizer_params(model, CFG.encoder_lr, CFG.encoder_lr)\n",
    "        optimizer = AdamW(optimizer_parameters, eps=CFG.eps, betas=CFG.betas)\n",
    "\n",
    "\n",
    "        \n",
    "        # ====================================================\n",
    "        # scheduler\n",
    "        # ====================================================\n",
    "        def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "            cfg.num_warmup_steps = cfg.num_warmup_steps * num_train_steps\n",
    "            if cfg.scheduler == 'linear':\n",
    "                scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "                )\n",
    "            elif cfg.scheduler == 'cosine':\n",
    "                scheduler = get_cosine_schedule_with_warmup(\n",
    "                    optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps,\n",
    "                    num_cycles=cfg.num_cycles\n",
    "                )\n",
    "            return scheduler\n",
    "\n",
    "        num_train_steps = int(len(train_dataset) / CFG.batch_size * CFG.epochs)\n",
    "        scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "        # ====================================================\n",
    "        # loop\n",
    "        # ====================================================\n",
    "        # criterion = torch.nn.CrossEntropyLoss(ignore_index=- 1)\n",
    "\n",
    "        # criterion = LabelSmoothingLoss()\n",
    "        best_score = float('inf')\n",
    "        best_score_ema = float('inf')\n",
    "        for epoch in range(CFG.epochs):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # train\n",
    "            avg_loss, best_score, best_score_ema = train_fn(train_loader, model, optimizer, epoch, scheduler, CFG.device, valid_loader, start_time, best_score, best_score_ema ,ema_hook, wandb,fold)\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        del scheduler, optimizer, model\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbgHecjCyz5D",
    "outputId": "09f6fba5-dd11-4adc-a85c-8805c6d6c90d",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== training ==========\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeng_sun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fd3a64d204043d6bd7757b0a4621be3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669310567279657, max=1.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/workspace/commonlit/wandb/run-20230905_194635-ddbxl6jv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peng_sun/commit_summary/runs/ddbxl6jv' target=\"_blank\">balmy-aardvark-10</a></strong> to <a href='https://wandb.ai/peng_sun/commit_summary' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peng_sun/commit_summary' target=\"_blank\">https://wandb.ai/peng_sun/commit_summary</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peng_sun/commit_summary/runs/ddbxl6jv' target=\"_blank\">https://wandb.ai/peng_sun/commit_summary/runs/ddbxl6jv</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['pooler.dense.weight', 'classifier.bias', 'pooler.dense.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/3031] Elapsed 0m 1s (remain 66m 2s) Loss: 1.4844(1.4844) Grad: 4.8957  LR: 0.00000200  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 1.3510  time: 55s\n",
      "Epoch 1 - content_rmse: 1.1234 - wording_rmse: 1.2000 - mcrmse: 1.1617\n",
      "Epoch 1 - Save Best Score: 1.1617 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 1.3507  time: 112s\n",
      "Epoch 1 - ema_content_rmse: 1.1240 - ema_wording_rmse: 1.1991 - ema_mcrmse: 1.1616\n",
      "Epoch 1 - ema_Save Best Score: 1.1616 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][100/3031] Elapsed 2m 31s (remain 73m 24s) Loss: 0.6851(1.0032) Grad: 13.0923  LR: 0.00000200  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 1.2276  time: 206s\n",
      "Epoch 1 - content_rmse: 0.9957 - wording_rmse: 1.2099 - mcrmse: 1.1028\n",
      "Epoch 1 - Save Best Score: 1.1028 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 1.2336  time: 264s\n",
      "Epoch 1 - ema_content_rmse: 1.0023 - ema_wording_rmse: 1.2094 - ema_mcrmse: 1.1058\n",
      "Epoch 1 - ema_Save Best Score: 1.1058 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][200/3031] Elapsed 5m 3s (remain 71m 13s) Loss: 1.2758(0.9273) Grad: 24.0211  LR: 0.00000200  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 1.1732  time: 358s\n",
      "Epoch 1 - content_rmse: 0.9748 - wording_rmse: 1.1816 - mcrmse: 1.0782\n",
      "Epoch 1 - Save Best Score: 1.0782 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 1.1697  time: 416s\n",
      "Epoch 1 - ema_content_rmse: 0.9714 - ema_wording_rmse: 1.1814 - ema_mcrmse: 1.0764\n",
      "Epoch 1 - ema_Save Best Score: 1.0764 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][300/3031] Elapsed 7m 34s (remain 68m 46s) Loss: 0.3120(0.8488) Grad: 29.2804  LR: 0.00000200  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 1.0961  time: 509s\n",
      "Epoch 1 - content_rmse: 0.9055 - wording_rmse: 1.1714 - mcrmse: 1.0385\n",
      "Epoch 1 - Save Best Score: 1.0385 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 1.0900  time: 567s\n",
      "Epoch 1 - ema_content_rmse: 0.9082 - ema_wording_rmse: 1.1641 - ema_mcrmse: 1.0361\n",
      "Epoch 1 - ema_Save Best Score: 1.0361 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][400/3031] Elapsed 10m 5s (remain 66m 13s) Loss: 0.2973(0.8122) Grad: 6.1721  LR: 0.00000200  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 1.0418  time: 660s\n",
      "Epoch 1 - content_rmse: 0.9101 - wording_rmse: 1.1204 - mcrmse: 1.0153\n",
      "Epoch 1 - Save Best Score: 1.0153 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 1.0517  time: 718s\n",
      "Epoch 1 - ema_content_rmse: 0.9171 - ema_wording_rmse: 1.1235 - ema_mcrmse: 1.0203\n",
      "Epoch 1 - ema_Save Best Score: 1.0203 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][500/3031] Elapsed 12m 36s (remain 63m 41s) Loss: 0.7064(0.7436) Grad: 14.5187  LR: 0.00000199  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.9403  time: 811s\n",
      "Epoch 1 - content_rmse: 0.8565 - wording_rmse: 1.0710 - mcrmse: 0.9637\n",
      "Epoch 1 - Save Best Score: 0.9637 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.9273  time: 869s\n",
      "Epoch 1 - ema_content_rmse: 0.8342 - ema_wording_rmse: 1.0765 - ema_mcrmse: 0.9553\n",
      "Epoch 1 - ema_Save Best Score: 0.9553 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][600/3031] Elapsed 15m 7s (remain 61m 9s) Loss: 1.2595(0.7097) Grad: 19.8171  LR: 0.00000199  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.8600  time: 962s\n",
      "Epoch 1 - content_rmse: 0.7916 - wording_rmse: 1.0456 - mcrmse: 0.9186\n",
      "Epoch 1 - Save Best Score: 0.9186 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.8604  time: 1019s\n",
      "Epoch 1 - ema_content_rmse: 0.7903 - ema_wording_rmse: 1.0470 - ema_mcrmse: 0.9187\n",
      "Epoch 1 - ema_Save Best Score: 0.9187 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][700/3031] Elapsed 17m 38s (remain 58m 38s) Loss: 0.3901(0.6728) Grad: 14.7789  LR: 0.00000199  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.8208  time: 1113s\n",
      "Epoch 1 - content_rmse: 0.7361 - wording_rmse: 1.0487 - mcrmse: 0.8924\n",
      "Epoch 1 - Save Best Score: 0.8924 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.8420  time: 1170s\n",
      "Epoch 1 - ema_content_rmse: 0.7376 - ema_wording_rmse: 1.0677 - ema_mcrmse: 0.9027\n",
      "Epoch 1 - ema_Save Best Score: 0.9027 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][800/3031] Elapsed 20m 9s (remain 56m 7s) Loss: 0.1594(0.6351) Grad: 10.9168  LR: 0.00000199  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.7370  time: 1264s\n",
      "Epoch 1 - content_rmse: 0.7001 - wording_rmse: 0.9919 - mcrmse: 0.8460\n",
      "Epoch 1 - Save Best Score: 0.8460 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.7199  time: 1321s\n",
      "Epoch 1 - ema_content_rmse: 0.7001 - ema_wording_rmse: 0.9746 - ema_mcrmse: 0.8373\n",
      "Epoch 1 - ema_Save Best Score: 0.8373 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][900/3031] Elapsed 22m 40s (remain 53m 36s) Loss: 0.4428(0.5984) Grad: 17.7682  LR: 0.00000198  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.7800  time: 1415s\n",
      "Epoch 1 - content_rmse: 0.7170 - wording_rmse: 1.0227 - mcrmse: 0.8699\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.7638  time: 1469s\n",
      "Epoch 1 - ema_content_rmse: 0.7009 - ema_wording_rmse: 1.0180 - ema_mcrmse: 0.8595\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1000/3031] Elapsed 25m 5s (remain 50m 53s) Loss: 0.4901(0.5746) Grad: 68.9153  LR: 0.00000198  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.8870  time: 1560s\n",
      "Epoch 1 - content_rmse: 0.7767 - wording_rmse: 1.0820 - mcrmse: 0.9293\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.7308  time: 1615s\n",
      "Epoch 1 - ema_content_rmse: 0.7037 - ema_wording_rmse: 0.9831 - ema_mcrmse: 0.8434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1100/3031] Elapsed 27m 31s (remain 48m 14s) Loss: 0.1458(0.5546) Grad: 9.6354  LR: 0.00000197  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.5712  time: 1706s\n",
      "Epoch 1 - content_rmse: 0.5925 - wording_rmse: 0.8896 - mcrmse: 0.7410\n",
      "Epoch 1 - Save Best Score: 0.7410 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.6044  time: 1763s\n",
      "Epoch 1 - ema_content_rmse: 0.6006 - ema_wording_rmse: 0.9209 - ema_mcrmse: 0.7607\n",
      "Epoch 1 - ema_Save Best Score: 0.7607 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1200/3031] Elapsed 30m 1s (remain 45m 45s) Loss: 0.1910(0.5299) Grad: 11.2902  LR: 0.00000197  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.5125  time: 1856s\n",
      "Epoch 1 - content_rmse: 0.5562 - wording_rmse: 0.8460 - mcrmse: 0.7011\n",
      "Epoch 1 - Save Best Score: 0.7011 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.5152  time: 1914s\n",
      "Epoch 1 - ema_content_rmse: 0.5543 - ema_wording_rmse: 0.8505 - ema_mcrmse: 0.7024\n",
      "Epoch 1 - ema_Save Best Score: 0.7024 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1300/3031] Elapsed 32m 32s (remain 43m 16s) Loss: 0.1388(0.5110) Grad: 16.0914  LR: 0.00000196  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.5508  time: 2007s\n",
      "Epoch 1 - content_rmse: 0.5814 - wording_rmse: 0.8738 - mcrmse: 0.7276\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.5232  time: 2061s\n",
      "Epoch 1 - ema_content_rmse: 0.5764 - ema_wording_rmse: 0.8451 - ema_mcrmse: 0.7107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1400/3031] Elapsed 34m 57s (remain 40m 40s) Loss: 0.9057(0.4964) Grad: 39.4977  LR: 0.00000196  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.7102  time: 2152s\n",
      "Epoch 1 - content_rmse: 0.6709 - wording_rmse: 0.9850 - mcrmse: 0.8280\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.6249  time: 2207s\n",
      "Epoch 1 - ema_content_rmse: 0.5994 - ema_wording_rmse: 0.9437 - ema_mcrmse: 0.7715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1500/3031] Elapsed 37m 22s (remain 38m 6s) Loss: 0.1551(0.4873) Grad: 18.7207  LR: 0.00000195  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.5374  time: 2297s\n",
      "Epoch 1 - content_rmse: 0.5715 - wording_rmse: 0.8649 - mcrmse: 0.7182\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.5247  time: 2352s\n",
      "Epoch 1 - ema_content_rmse: 0.5808 - ema_wording_rmse: 0.8439 - ema_mcrmse: 0.7123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1600/3031] Elapsed 39m 47s (remain 35m 32s) Loss: 0.8879(0.4747) Grad: 31.8132  LR: 0.00000195  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4547  time: 2442s\n",
      "Epoch 1 - content_rmse: 0.5390 - wording_rmse: 0.7867 - mcrmse: 0.6629\n",
      "Epoch 1 - Save Best Score: 0.6629 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.5097  time: 2500s\n",
      "Epoch 1 - ema_content_rmse: 0.5745 - ema_wording_rmse: 0.8303 - ema_mcrmse: 0.7024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1700/3031] Elapsed 42m 16s (remain 33m 3s) Loss: 0.2813(0.4664) Grad: 14.5223  LR: 0.00000194  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4547  time: 2591s\n",
      "Epoch 1 - content_rmse: 0.5219 - wording_rmse: 0.7982 - mcrmse: 0.6600\n",
      "Epoch 1 - Save Best Score: 0.6600 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4651  time: 2648s\n",
      "Epoch 1 - ema_content_rmse: 0.5333 - ema_wording_rmse: 0.8036 - ema_mcrmse: 0.6685\n",
      "Epoch 1 - ema_Save Best Score: 0.6685 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1800/3031] Elapsed 44m 47s (remain 30m 35s) Loss: 0.5613(0.4565) Grad: 54.2189  LR: 0.00000193  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4456  time: 2742s\n",
      "Epoch 1 - content_rmse: 0.5205 - wording_rmse: 0.7876 - mcrmse: 0.6541\n",
      "Epoch 1 - Save Best Score: 0.6541 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4758  time: 2799s\n",
      "Epoch 1 - ema_content_rmse: 0.5616 - ema_wording_rmse: 0.7976 - ema_mcrmse: 0.6796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1900/3031] Elapsed 47m 15s (remain 28m 5s) Loss: 0.2627(0.4490) Grad: 11.5152  LR: 0.00000192  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4535  time: 2890s\n",
      "Epoch 1 - content_rmse: 0.5407 - wording_rmse: 0.7840 - mcrmse: 0.6623\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4613  time: 2944s\n",
      "Epoch 1 - ema_content_rmse: 0.5357 - ema_wording_rmse: 0.7973 - ema_mcrmse: 0.6665\n",
      "Epoch 1 - ema_Save Best Score: 0.6665 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2000/3031] Elapsed 49m 43s (remain 25m 35s) Loss: 1.2558(0.4399) Grad: 32.5414  LR: 0.00000192  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4667  time: 3038s\n",
      "Epoch 1 - content_rmse: 0.5706 - wording_rmse: 0.7796 - mcrmse: 0.6751\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4111  time: 3092s\n",
      "Epoch 1 - ema_content_rmse: 0.5244 - ema_wording_rmse: 0.7398 - ema_mcrmse: 0.6321\n",
      "Epoch 1 - ema_Save Best Score: 0.6321 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2100/3031] Elapsed 52m 11s (remain 23m 6s) Loss: 0.2586(0.4305) Grad: 23.3724  LR: 0.00000191  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4598  time: 3186s\n",
      "Epoch 1 - content_rmse: 0.5192 - wording_rmse: 0.8063 - mcrmse: 0.6627\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4492  time: 3240s\n",
      "Epoch 1 - ema_content_rmse: 0.5172 - ema_wording_rmse: 0.7943 - ema_mcrmse: 0.6557\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2200/3031] Elapsed 54m 36s (remain 20m 35s) Loss: 0.1747(0.4234) Grad: 13.6002  LR: 0.00000190  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4425  time: 3331s\n",
      "Epoch 1 - content_rmse: 0.5422 - wording_rmse: 0.7687 - mcrmse: 0.6555\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4293  time: 3385s\n",
      "Epoch 1 - ema_content_rmse: 0.5293 - ema_wording_rmse: 0.7606 - ema_mcrmse: 0.6449\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2300/3031] Elapsed 57m 1s (remain 18m 5s) Loss: 0.6557(0.4153) Grad: 34.5974  LR: 0.00000189  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4829  time: 3476s\n",
      "Epoch 1 - content_rmse: 0.5570 - wording_rmse: 0.8097 - mcrmse: 0.6833\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4769  time: 3530s\n",
      "Epoch 1 - ema_content_rmse: 0.5554 - ema_wording_rmse: 0.8033 - ema_mcrmse: 0.6794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2400/3031] Elapsed 59m 26s (remain 15m 35s) Loss: 0.1713(0.4092) Grad: 8.7942  LR: 0.00000188  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4894  time: 3621s\n",
      "Epoch 1 - content_rmse: 0.5783 - wording_rmse: 0.8027 - mcrmse: 0.6905\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4580  time: 3675s\n",
      "Epoch 1 - ema_content_rmse: 0.5523 - ema_wording_rmse: 0.7817 - ema_mcrmse: 0.6670\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2500/3031] Elapsed 61m 51s (remain 13m 6s) Loss: 0.0828(0.4032) Grad: 7.7281  LR: 0.00000187  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4932  time: 3766s\n",
      "Epoch 1 - content_rmse: 0.5384 - wording_rmse: 0.8346 - mcrmse: 0.6865\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4810  time: 3820s\n",
      "Epoch 1 - ema_content_rmse: 0.5145 - ema_wording_rmse: 0.8350 - ema_mcrmse: 0.6747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2600/3031] Elapsed 64m 16s (remain 10m 37s) Loss: 0.0297(0.3960) Grad: 3.8521  LR: 0.00000186  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4279  time: 3911s\n",
      "Epoch 1 - content_rmse: 0.5271 - wording_rmse: 0.7603 - mcrmse: 0.6437\n",
      "Epoch 1 - Save Best Score: 0.6437 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4356  time: 3968s\n",
      "Epoch 1 - ema_content_rmse: 0.5190 - ema_wording_rmse: 0.7758 - ema_mcrmse: 0.6474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2700/3031] Elapsed 66m 44s (remain 8m 9s) Loss: 0.1669(0.3898) Grad: 11.5246  LR: 0.00000185  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4995  time: 4059s\n",
      "Epoch 1 - content_rmse: 0.6167 - wording_rmse: 0.7866 - mcrmse: 0.7017\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4275  time: 4113s\n",
      "Epoch 1 - ema_content_rmse: 0.5325 - ema_wording_rmse: 0.7560 - ema_mcrmse: 0.6442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2800/3031] Elapsed 69m 9s (remain 5m 40s) Loss: 0.0944(0.3840) Grad: 13.5259  LR: 0.00000184  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.3595  time: 4204s\n",
      "Epoch 1 - content_rmse: 0.4753 - wording_rmse: 0.7022 - mcrmse: 0.5887\n",
      "Epoch 1 - Save Best Score: 0.5887 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.3649  time: 4261s\n",
      "Epoch 1 - ema_content_rmse: 0.4846 - ema_wording_rmse: 0.7036 - ema_mcrmse: 0.5941\n",
      "Epoch 1 - ema_Save Best Score: 0.5941 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2900/3031] Elapsed 71m 40s (remain 3m 12s) Loss: 0.0225(0.3813) Grad: 3.5940  LR: 0.00000182  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4338  time: 4355s\n",
      "Epoch 1 - content_rmse: 0.5466 - wording_rmse: 0.7542 - mcrmse: 0.6504\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4131  time: 4409s\n",
      "Epoch 1 - ema_content_rmse: 0.5307 - ema_wording_rmse: 0.7380 - ema_mcrmse: 0.6343\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][3000/3031] Elapsed 74m 5s (remain 0m 44s) Loss: 0.2852(0.3766) Grad: 24.1277  LR: 0.00000181  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.5533  time: 4500s\n",
      "Epoch 1 - content_rmse: 0.6347 - wording_rmse: 0.8389 - mcrmse: 0.7368\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4837  time: 4554s\n",
      "Epoch 1 - ema_content_rmse: 0.5712 - ema_wording_rmse: 0.8006 - ema_mcrmse: 0.6859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][3030/3031] Elapsed 76m 5s (remain 0m 0s) Loss: 0.2054(0.3751) Grad: 22.3421  LR: 0.00000181  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4800  time: 4619s\n",
      "Epoch 1 - content_rmse: 0.5566 - wording_rmse: 0.8063 - mcrmse: 0.6815\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 1 avg_val_loss: 0.4488  time: 4674s\n",
      "Epoch 1 - ema_content_rmse: 0.5422 - ema_wording_rmse: 0.7769 - ema_mcrmse: 0.6596\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/3031] Elapsed 0m 0s (remain 28m 27s) Loss: 0.0710(0.0710) Grad: 4.5932  LR: 0.00000181  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4861  time: 55s\n",
      "Epoch 2 - content_rmse: 0.5600 - wording_rmse: 0.8116 - mcrmse: 0.6858\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4502  time: 109s\n",
      "Epoch 2 - ema_content_rmse: 0.5428 - ema_wording_rmse: 0.7783 - ema_mcrmse: 0.6606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][100/3031] Elapsed 2m 25s (remain 70m 23s) Loss: 0.2357(0.2282) Grad: 12.8294  LR: 0.00000180  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4410  time: 200s\n",
      "Epoch 2 - content_rmse: 0.5559 - wording_rmse: 0.7569 - mcrmse: 0.6564\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4171  time: 254s\n",
      "Epoch 2 - ema_content_rmse: 0.5240 - ema_wording_rmse: 0.7481 - ema_mcrmse: 0.6360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][200/3031] Elapsed 4m 50s (remain 68m 11s) Loss: 0.2979(0.1925) Grad: 20.7767  LR: 0.00000178  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4118  time: 345s\n",
      "Epoch 2 - content_rmse: 0.5127 - wording_rmse: 0.7488 - mcrmse: 0.6308\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4384  time: 399s\n",
      "Epoch 2 - ema_content_rmse: 0.5407 - ema_wording_rmse: 0.7645 - ema_mcrmse: 0.6526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][300/3031] Elapsed 7m 15s (remain 65m 52s) Loss: 0.0257(0.1953) Grad: 3.6665  LR: 0.00000177  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4607  time: 490s\n",
      "Epoch 2 - content_rmse: 0.5079 - wording_rmse: 0.8145 - mcrmse: 0.6612\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4673  time: 544s\n",
      "Epoch 2 - ema_content_rmse: 0.5372 - ema_wording_rmse: 0.8038 - ema_mcrmse: 0.6705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][400/3031] Elapsed 9m 40s (remain 63m 28s) Loss: 0.3606(0.1925) Grad: 29.3850  LR: 0.00000176  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3969  time: 635s\n",
      "Epoch 2 - content_rmse: 0.4959 - wording_rmse: 0.7402 - mcrmse: 0.6180\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4687  time: 689s\n",
      "Epoch 2 - ema_content_rmse: 0.5447 - ema_wording_rmse: 0.8005 - ema_mcrmse: 0.6726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][500/3031] Elapsed 12m 5s (remain 61m 4s) Loss: 0.0716(0.1929) Grad: 11.5257  LR: 0.00000174  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3787  time: 780s\n",
      "Epoch 2 - content_rmse: 0.4866 - wording_rmse: 0.7215 - mcrmse: 0.6041\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4272  time: 835s\n",
      "Epoch 2 - ema_content_rmse: 0.5067 - ema_wording_rmse: 0.7731 - ema_mcrmse: 0.6399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][600/3031] Elapsed 14m 30s (remain 58m 40s) Loss: 0.2178(0.1922) Grad: 15.4283  LR: 0.00000173  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.5290  time: 925s\n",
      "Epoch 2 - content_rmse: 0.5780 - wording_rmse: 0.8507 - mcrmse: 0.7144\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4638  time: 980s\n",
      "Epoch 2 - ema_content_rmse: 0.5343 - ema_wording_rmse: 0.8014 - ema_mcrmse: 0.6678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][700/3031] Elapsed 16m 55s (remain 56m 16s) Loss: 0.1229(0.1944) Grad: 14.7926  LR: 0.00000172  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4848  time: 1070s\n",
      "Epoch 2 - content_rmse: 0.6069 - wording_rmse: 0.7755 - mcrmse: 0.6912\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4732  time: 1125s\n",
      "Epoch 2 - ema_content_rmse: 0.5538 - ema_wording_rmse: 0.7998 - ema_mcrmse: 0.6768\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][800/3031] Elapsed 19m 21s (remain 53m 52s) Loss: 0.1718(0.2020) Grad: 18.3699  LR: 0.00000170  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4463  time: 1215s\n",
      "Epoch 2 - content_rmse: 0.5201 - wording_rmse: 0.7888 - mcrmse: 0.6544\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4256  time: 1270s\n",
      "Epoch 2 - ema_content_rmse: 0.5226 - ema_wording_rmse: 0.7603 - ema_mcrmse: 0.6414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][900/3031] Elapsed 21m 46s (remain 51m 27s) Loss: 0.0656(0.1996) Grad: 8.5071  LR: 0.00000169  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3548  time: 1360s\n",
      "Epoch 2 - content_rmse: 0.4757 - wording_rmse: 0.6953 - mcrmse: 0.5855\n",
      "Epoch 2 - Save Best Score: 0.5855 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3765  time: 1418s\n",
      "Epoch 2 - ema_content_rmse: 0.4872 - ema_wording_rmse: 0.7181 - ema_mcrmse: 0.6026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1000/3031] Elapsed 24m 14s (remain 49m 8s) Loss: 0.5422(0.1967) Grad: 21.6110  LR: 0.00000167  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3745  time: 1508s\n",
      "Epoch 2 - content_rmse: 0.4799 - wording_rmse: 0.7201 - mcrmse: 0.6000\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3714  time: 1563s\n",
      "Epoch 2 - ema_content_rmse: 0.4868 - ema_wording_rmse: 0.7112 - ema_mcrmse: 0.5990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1100/3031] Elapsed 26m 39s (remain 46m 43s) Loss: 0.1429(0.1958) Grad: 14.5140  LR: 0.00000166  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4845  time: 1653s\n",
      "Epoch 2 - content_rmse: 0.6113 - wording_rmse: 0.7716 - mcrmse: 0.6914\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4047  time: 1708s\n",
      "Epoch 2 - ema_content_rmse: 0.5166 - ema_wording_rmse: 0.7366 - ema_mcrmse: 0.6266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1200/3031] Elapsed 29m 4s (remain 44m 17s) Loss: 0.0723(0.1982) Grad: 8.8238  LR: 0.00000164  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3752  time: 1798s\n",
      "Epoch 2 - content_rmse: 0.4971 - wording_rmse: 0.7093 - mcrmse: 0.6032\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3945  time: 1853s\n",
      "Epoch 2 - ema_content_rmse: 0.5106 - ema_wording_rmse: 0.7268 - ema_mcrmse: 0.6187\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1300/3031] Elapsed 31m 29s (remain 41m 51s) Loss: 0.0212(0.1999) Grad: 3.2622  LR: 0.00000162  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3783  time: 1943s\n",
      "Epoch 2 - content_rmse: 0.4987 - wording_rmse: 0.7127 - mcrmse: 0.6057\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4031  time: 1998s\n",
      "Epoch 2 - ema_content_rmse: 0.5193 - ema_wording_rmse: 0.7325 - ema_mcrmse: 0.6259\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1400/3031] Elapsed 33m 53s (remain 39m 26s) Loss: 0.0514(0.2019) Grad: 5.3944  LR: 0.00000161  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4303  time: 2088s\n",
      "Epoch 2 - content_rmse: 0.5503 - wording_rmse: 0.7468 - mcrmse: 0.6485\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4052  time: 2142s\n",
      "Epoch 2 - ema_content_rmse: 0.5254 - ema_wording_rmse: 0.7310 - ema_mcrmse: 0.6282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1500/3031] Elapsed 36m 18s (remain 37m 0s) Loss: 0.0210(0.2010) Grad: 3.3229  LR: 0.00000159  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3767  time: 2233s\n",
      "Epoch 2 - content_rmse: 0.5096 - wording_rmse: 0.7026 - mcrmse: 0.6061\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3794  time: 2287s\n",
      "Epoch 2 - ema_content_rmse: 0.5143 - ema_wording_rmse: 0.7031 - ema_mcrmse: 0.6087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1600/3031] Elapsed 38m 43s (remain 34m 35s) Loss: 0.0313(0.1990) Grad: 9.6582  LR: 0.00000157  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3874  time: 2378s\n",
      "Epoch 2 - content_rmse: 0.5219 - wording_rmse: 0.7088 - mcrmse: 0.6154\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3952  time: 2432s\n",
      "Epoch 2 - ema_content_rmse: 0.5197 - ema_wording_rmse: 0.7212 - ema_mcrmse: 0.6205\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1700/3031] Elapsed 41m 8s (remain 32m 9s) Loss: 0.0844(0.1994) Grad: 10.4611  LR: 0.00000156  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3979  time: 2523s\n",
      "Epoch 2 - content_rmse: 0.4820 - wording_rmse: 0.7507 - mcrmse: 0.6163\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3734  time: 2577s\n",
      "Epoch 2 - ema_content_rmse: 0.4857 - ema_wording_rmse: 0.7148 - ema_mcrmse: 0.6003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1800/3031] Elapsed 43m 33s (remain 29m 44s) Loss: 0.0503(0.2003) Grad: 6.4707  LR: 0.00000154  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3719  time: 2667s\n",
      "Epoch 2 - content_rmse: 0.4925 - wording_rmse: 0.7079 - mcrmse: 0.6002\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3790  time: 2722s\n",
      "Epoch 2 - ema_content_rmse: 0.4960 - ema_wording_rmse: 0.7155 - ema_mcrmse: 0.6058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1900/3031] Elapsed 45m 57s (remain 27m 19s) Loss: 0.1096(0.1996) Grad: 7.0797  LR: 0.00000152  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3978  time: 2812s\n",
      "Epoch 2 - content_rmse: 0.5115 - wording_rmse: 0.7307 - mcrmse: 0.6211\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3823  time: 2867s\n",
      "Epoch 2 - ema_content_rmse: 0.4954 - ema_wording_rmse: 0.7205 - ema_mcrmse: 0.6080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2000/3031] Elapsed 48m 22s (remain 24m 54s) Loss: 0.3151(0.1991) Grad: 17.8990  LR: 0.00000150  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4304  time: 2957s\n",
      "Epoch 2 - content_rmse: 0.5301 - wording_rmse: 0.7615 - mcrmse: 0.6458\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4089  time: 3012s\n",
      "Epoch 2 - ema_content_rmse: 0.5093 - ema_wording_rmse: 0.7473 - ema_mcrmse: 0.6283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2100/3031] Elapsed 50m 47s (remain 22m 29s) Loss: 0.1393(0.1982) Grad: 14.6423  LR: 0.00000149  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.5383  time: 3102s\n",
      "Epoch 2 - content_rmse: 0.5989 - wording_rmse: 0.8472 - mcrmse: 0.7231\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4286  time: 3157s\n",
      "Epoch 2 - ema_content_rmse: 0.5177 - ema_wording_rmse: 0.7676 - ema_mcrmse: 0.6427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2200/3031] Elapsed 53m 13s (remain 20m 4s) Loss: 0.1502(0.2006) Grad: 13.2018  LR: 0.00000147  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3889  time: 3248s\n",
      "Epoch 2 - content_rmse: 0.5215 - wording_rmse: 0.7112 - mcrmse: 0.6164\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4257  time: 3302s\n",
      "Epoch 2 - ema_content_rmse: 0.5356 - ema_wording_rmse: 0.7514 - ema_mcrmse: 0.6435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2300/3031] Elapsed 55m 38s (remain 17m 39s) Loss: 0.1646(0.2005) Grad: 7.2556  LR: 0.00000145  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3638  time: 3392s\n",
      "Epoch 2 - content_rmse: 0.5080 - wording_rmse: 0.6852 - mcrmse: 0.5966\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4091  time: 3447s\n",
      "Epoch 2 - ema_content_rmse: 0.5377 - ema_wording_rmse: 0.7274 - ema_mcrmse: 0.6326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2400/3031] Elapsed 58m 3s (remain 15m 13s) Loss: 0.2236(0.2006) Grad: 10.5306  LR: 0.00000143  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3547  time: 3537s\n",
      "Epoch 2 - content_rmse: 0.5014 - wording_rmse: 0.6768 - mcrmse: 0.5891\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3731  time: 3592s\n",
      "Epoch 2 - ema_content_rmse: 0.5145 - ema_wording_rmse: 0.6938 - ema_mcrmse: 0.6042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2500/3031] Elapsed 60m 28s (remain 12m 48s) Loss: 0.2545(0.1999) Grad: 15.8449  LR: 0.00000141  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3505  time: 3682s\n",
      "Epoch 2 - content_rmse: 0.4880 - wording_rmse: 0.6803 - mcrmse: 0.5841\n",
      "Epoch 2 - Save Best Score: 0.5841 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3785  time: 3740s\n",
      "Epoch 2 - ema_content_rmse: 0.5212 - ema_wording_rmse: 0.6966 - ema_mcrmse: 0.6089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2600/3031] Elapsed 62m 55s (remain 10m 24s) Loss: 0.1315(0.1999) Grad: 16.8956  LR: 0.00000139  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4004  time: 3830s\n",
      "Epoch 2 - content_rmse: 0.5224 - wording_rmse: 0.7266 - mcrmse: 0.6245\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3724  time: 3884s\n",
      "Epoch 2 - ema_content_rmse: 0.5152 - ema_wording_rmse: 0.6924 - ema_mcrmse: 0.6038\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2700/3031] Elapsed 65m 20s (remain 7m 59s) Loss: 0.1292(0.2013) Grad: 10.8992  LR: 0.00000137  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3380  time: 3975s\n",
      "Epoch 2 - content_rmse: 0.4852 - wording_rmse: 0.6638 - mcrmse: 0.5745\n",
      "Epoch 2 - Save Best Score: 0.5745 Model\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3621  time: 4032s\n",
      "Epoch 2 - ema_content_rmse: 0.5029 - ema_wording_rmse: 0.6865 - ema_mcrmse: 0.5947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2800/3031] Elapsed 67m 48s (remain 5m 34s) Loss: 0.7080(0.2011) Grad: 19.6612  LR: 0.00000135  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3961  time: 4123s\n",
      "Epoch 2 - content_rmse: 0.5572 - wording_rmse: 0.6941 - mcrmse: 0.6256\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3904  time: 4177s\n",
      "Epoch 2 - ema_content_rmse: 0.5274 - ema_wording_rmse: 0.7090 - ema_mcrmse: 0.6182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2900/3031] Elapsed 70m 13s (remain 3m 8s) Loss: 0.2499(0.2003) Grad: 13.3455  LR: 0.00000133  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3914  time: 4268s\n",
      "Epoch 2 - content_rmse: 0.5187 - wording_rmse: 0.7168 - mcrmse: 0.6177\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3677  time: 4322s\n",
      "Epoch 2 - ema_content_rmse: 0.5083 - ema_wording_rmse: 0.6907 - ema_mcrmse: 0.5995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][3000/3031] Elapsed 72m 38s (remain 0m 43s) Loss: 0.2247(0.2004) Grad: 12.6176  LR: 0.00000131  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3563  time: 4413s\n",
      "Epoch 2 - content_rmse: 0.4968 - wording_rmse: 0.6825 - mcrmse: 0.5896\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3672  time: 4467s\n",
      "Epoch 2 - ema_content_rmse: 0.5118 - ema_wording_rmse: 0.6874 - ema_mcrmse: 0.5996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][3030/3031] Elapsed 74m 38s (remain 0m 0s) Loss: 0.3397(0.2012) Grad: 29.7375  LR: 0.00000131  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.4364  time: 4533s\n",
      "Epoch 2 - content_rmse: 0.5876 - wording_rmse: 0.7263 - mcrmse: 0.6570\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 2 avg_val_loss: 0.3674  time: 4587s\n",
      "Epoch 2 - ema_content_rmse: 0.5128 - ema_wording_rmse: 0.6869 - ema_mcrmse: 0.5999\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/3031] Elapsed 0m 0s (remain 27m 16s) Loss: 0.0299(0.0299) Grad: 3.3193  LR: 0.00000131  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.4436  time: 55s\n",
      "Epoch 3 - content_rmse: 0.5946 - wording_rmse: 0.7305 - mcrmse: 0.6626\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.3681  time: 109s\n",
      "Epoch 3 - ema_content_rmse: 0.5136 - ema_wording_rmse: 0.6873 - ema_mcrmse: 0.6004\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][100/3031] Elapsed 2m 25s (remain 70m 18s) Loss: 0.1109(0.1312) Grad: 9.0678  LR: 0.00000129  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.4112  time: 200s\n",
      "Epoch 3 - content_rmse: 0.5418 - wording_rmse: 0.7272 - mcrmse: 0.6345\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.3806  time: 254s\n",
      "Epoch 3 - ema_content_rmse: 0.5358 - ema_wording_rmse: 0.6885 - ema_mcrmse: 0.6122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][200/3031] Elapsed 4m 50s (remain 68m 10s) Loss: 0.0579(0.1420) Grad: 8.9715  LR: 0.00000127  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.3944  time: 345s\n",
      "Epoch 3 - content_rmse: 0.5258 - wording_rmse: 0.7158 - mcrmse: 0.6208\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.4132  time: 399s\n",
      "Epoch 3 - ema_content_rmse: 0.5500 - ema_wording_rmse: 0.7238 - ema_mcrmse: 0.6369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][300/3031] Elapsed 7m 15s (remain 65m 47s) Loss: 0.0270(0.1516) Grad: 5.2431  LR: 0.00000125  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.4027  time: 489s\n",
      "Epoch 3 - content_rmse: 0.5284 - wording_rmse: 0.7253 - mcrmse: 0.6269\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.4576  time: 544s\n",
      "Epoch 3 - ema_content_rmse: 0.5736 - ema_wording_rmse: 0.7656 - ema_mcrmse: 0.6696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][400/3031] Elapsed 9m 39s (remain 63m 23s) Loss: 0.0696(0.1529) Grad: 5.5251  LR: 0.00000123  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.4034  time: 634s\n",
      "Epoch 3 - content_rmse: 0.5694 - wording_rmse: 0.6947 - mcrmse: 0.6320\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.4267  time: 688s\n",
      "Epoch 3 - ema_content_rmse: 0.5676 - ema_wording_rmse: 0.7289 - ema_mcrmse: 0.6483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][500/3031] Elapsed 12m 4s (remain 61m 0s) Loss: 0.0210(0.1543) Grad: 4.6201  LR: 0.00000121  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.4282  time: 779s\n",
      "Epoch 3 - content_rmse: 0.5868 - wording_rmse: 0.7156 - mcrmse: 0.6512\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.4085  time: 833s\n",
      "Epoch 3 - ema_content_rmse: 0.5492 - ema_wording_rmse: 0.7179 - ema_mcrmse: 0.6336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][600/3031] Elapsed 14m 29s (remain 58m 35s) Loss: 0.1817(0.1590) Grad: 12.9492  LR: 0.00000119  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.3901  time: 924s\n",
      "Epoch 3 - content_rmse: 0.5221 - wording_rmse: 0.7124 - mcrmse: 0.6173\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.3946  time: 978s\n",
      "Epoch 3 - ema_content_rmse: 0.5441 - ema_wording_rmse: 0.7022 - ema_mcrmse: 0.6232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][700/3031] Elapsed 16m 54s (remain 56m 11s) Loss: 0.2057(0.1580) Grad: 14.4415  LR: 0.00000117  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.3630  time: 1069s\n",
      "Epoch 3 - content_rmse: 0.4994 - wording_rmse: 0.6904 - mcrmse: 0.5949\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.3945  time: 1123s\n",
      "Epoch 3 - ema_content_rmse: 0.5339 - ema_wording_rmse: 0.7099 - ema_mcrmse: 0.6219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][800/3031] Elapsed 19m 19s (remain 53m 47s) Loss: 0.0583(0.1581) Grad: 6.9701  LR: 0.00000115  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.3763  time: 1214s\n",
      "Epoch 3 - content_rmse: 0.5479 - wording_rmse: 0.6725 - mcrmse: 0.6102\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.3839  time: 1268s\n",
      "Epoch 3 - ema_content_rmse: 0.5337 - ema_wording_rmse: 0.6950 - ema_mcrmse: 0.6143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][900/3031] Elapsed 21m 44s (remain 51m 22s) Loss: 0.3079(0.1591) Grad: 9.6411  LR: 0.00000113  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.4461  time: 1358s\n",
      "Epoch 3 - content_rmse: 0.6064 - wording_rmse: 0.7243 - mcrmse: 0.6653\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "Epoch 3 avg_val_loss: 0.3837  time: 1413s\n",
      "Epoch 3 - ema_content_rmse: 0.5359 - ema_wording_rmse: 0.6929 - ema_mcrmse: 0.6144\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 219\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m    216\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m--> 219\u001b[0m     avg_loss, best_score, best_score_ema \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_score_ema\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mema_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    223\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[0;32mIn[18], line 39\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, optimizer, epoch, scheduler, device, valid_loader, start_time, best_score, best_score_ema, ema_hook, wandb, fold)\u001b[0m\n\u001b[1;32m     37\u001b[0m grad_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     38\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 39\u001b[0m \u001b[43mema_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mafter_train_iter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m global_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     41\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "Cell \u001b[0;32mIn[13], line 84\u001b[0m, in \u001b[0;36mEMAHook.after_train_iter\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mafter_train_iter\u001b[39m(\u001b[38;5;28mself\u001b[39m, model):\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mema_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 41\u001b[0m, in \u001b[0;36mModelEMA.update\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m v\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mis_floating_point:\n\u001b[1;32m     40\u001b[0m     v \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m d\n\u001b[0;32m---> 41\u001b[0m     v \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m d) \u001b[38;5;241m*\u001b[39m msd[k]\u001b[38;5;241m.\u001b[39mdetach()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BV3Rp1bbXUQ-"
   },
   "outputs": [],
   "source": [
    "## total_complex = []\n",
    "# for fold in range(4):\n",
    "#     va_data = train_df[train_df['fold'] == fold]\n",
    "#     preds = torch.load('/content/drive/MyDrive/deb_simple/microsoft_deberta-v3-large_best{}.pth'.format(fold))['predictions']\n",
    "#     va_data['preds'] = preds\n",
    "#     va_data = va_data[['id', 'preds', 'score']]\n",
    "#     print(compute_metrics(va_data['preds'].values.reshape(-1,1), va_data['score'].values))\n",
    "#     total_complex.append(va_data)\n",
    "# total_complex = pd.concat(total_complex)\n",
    "# compute_metrics(total_complex['preds'].values.reshape(-1,1), total_complex['score'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11_KW3qSx1IK",
    "outputId": "e5423c46-d702-49da-d29b-c5d125a665e4"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /root/.kaggle\n",
    "# !cp /content/drive/MyDrive/kaggle/kaggle.json /root/.kaggle/\n",
    "# !chmod 600 /root/.kaggle/kaggle.json\n",
    "# !kaggle datasets init -p /content/drive/MyDrive/elc_mean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cN6F3A_hKthE",
    "outputId": "0358b1da-6195-44ba-8ab5-2eacb5268fff"
   },
   "outputs": [],
   "source": [
    "#!kaggle datasets create -p /content/drive/MyDrive/elc_mean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGinNBYJFw3O"
   },
   "outputs": [],
   "source": [
    "# deberta v3 large\n",
    "# 1.5 0.8228\n",
    "# 2 0.8197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVfKappB86jX",
    "tags": []
   },
   "source": [
    "# 1.5  8137\n",
    "#2 8175\n",
    "#2.5 8181\n",
    "#3 8181\n",
    "#3.5 8175\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
