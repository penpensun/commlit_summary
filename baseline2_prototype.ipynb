{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AmhfxdCR86jB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedGroupKFold\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForWholeWordMask\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n",
    "from transformers.modeling_outputs  import BaseModelOutput,SequenceClassifierOutput\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "# imports the torch_xla package\n",
    "import wandb\n",
    "from torch.nn.parameter import Parameter\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1dMoHaCUn29I",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Model training')\n",
    "    # params of training\n",
    "    parser.add_argument(\n",
    "        \"--fold\", dest=\"fold\", help=\"Train fold\", default=None, type=int)\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        dest='batch_size',\n",
    "        help='Mini batch size of one gpu or cpu',\n",
    "        type=int,\n",
    "        default=None)\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6G2r5GS86jE"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Bojtddae86jG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    pretraining = False\n",
    "    load_pretrained = False\n",
    "    input_path = './input/'\n",
    "    input_type = '2'\n",
    "    model_path = 'microsoft/deberta-v3-base' #  nghuyong/ernie-2.0-large-en studio-ousia/luke-large\n",
    "    model_type = 'base'\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5  # 1.5\n",
    "    num_warmup_steps = 0\n",
    "    max_input_length = 512\n",
    "    max_position_embeddings = 512\n",
    "    folds = [0]\n",
    "    epochs = 5  # 5\n",
    "    # layer - wise larning rate \n",
    "    discriminative_learning_rate = False\n",
    "    discriminative_learning_rate_num_groups = 1\n",
    "    discriminative_learning_rate_decay_rate = 0.99\n",
    "    # reinint layer\n",
    "    reinit_layers = 0\n",
    "    \n",
    "    encoder_lr = 50e-6\n",
    "    head_lr = 10e-4\n",
    "    min_lr = 1e-7\n",
    "    eps = 1e-7\n",
    "    betas = (0.9, 0.999)\n",
    "    weight_decay = 0\n",
    "    dropout = 0\n",
    "    num_fold = 5\n",
    "    batch_size = 16\n",
    "    seed = 1006\n",
    "    OUTPUT_DIR = './pretrain/'\n",
    "    num_workers = 2\n",
    "    device='cuda'\n",
    "    print_freq = 100\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FaoPHcM86jH"
   },
   "source": [
    "## logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RWkPxbhq86jI",
    "outputId": "1f97e316-be01-408d-c79b-1b128aaff9c9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===============lr_5e-05===============\n",
      "===============seed_1006===============\n",
      "===============total_epochs_5===============\n",
      "===============num_warmup_steps_0===============\n"
     ]
    }
   ],
   "source": [
    "def get_logger(filename=CFG.OUTPUT_DIR+ 'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    if not os.path.exists(CFG.OUTPUT_DIR):\n",
    "        os.makedirs(CFG.OUTPUT_DIR)\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "LOGGER.info('===============lr_{}==============='.format(CFG.encoder_lr))\n",
    "LOGGER.info('===============seed_{}==============='.format(CFG.seed))\n",
    "LOGGER.info('===============total_epochs_{}==============='.format(CFG.epochs))\n",
    "LOGGER.info('===============num_warmup_steps_{}==============='.format(CFG.num_warmup_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSqTonVY86jJ"
   },
   "source": [
    "# Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ACTTNzFL86jK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf = pd.read_csv(f\"{CFG.input_path}/prompts_train.csv\")\n",
    "sdf = pd.read_csv(f\"{CFG.input_path}/summaries_train.csv\")\n",
    "\n",
    "df = pdf.merge(sdf, on=\"prompt_id\")\n",
    "\n",
    "# 4 prompt ids, 4 folds\n",
    "id2fold = {\n",
    "    \"814d6b\": 0,\n",
    "    \"39c16e\": 1,\n",
    "    \"3b9047\": 2,\n",
    "    \"ebad26\": 3,\n",
    "}\n",
    "\n",
    "df[\"fold\"] = df[\"prompt_id\"].map(id2fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "FY5ZdDSh86jL",
    "outputId": "f25ff859-20f1-4a74-f363-eb0dee6c3ba1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>student_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00791789cc1f</td>\n",
       "      <td>1 element of an ideal tragedy is that it shoul...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>0086ef22de8f</td>\n",
       "      <td>The three elements of an ideal tragedy are:  H...</td>\n",
       "      <td>-0.970237</td>\n",
       "      <td>-0.417058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>0094589c7a22</td>\n",
       "      <td>Aristotle states that an ideal tragedy should ...</td>\n",
       "      <td>-0.387791</td>\n",
       "      <td>-0.584181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00cd5736026a</td>\n",
       "      <td>One element of an Ideal tragedy is having a co...</td>\n",
       "      <td>0.088882</td>\n",
       "      <td>-0.594710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00d98b8ff756</td>\n",
       "      <td>The 3 ideal of tragedy is how complex you need...</td>\n",
       "      <td>-0.687288</td>\n",
       "      <td>-0.460886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff37545b2805</td>\n",
       "      <td>In paragraph two, they would use pickle meat a...</td>\n",
       "      <td>1.520355</td>\n",
       "      <td>-0.292990</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff4ed38ef099</td>\n",
       "      <td>in the first paragraph  it says \"either can it...</td>\n",
       "      <td>-1.204574</td>\n",
       "      <td>-1.169784</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff53b94f7ce0</td>\n",
       "      <td>They would have piles of filthy meat on the fl...</td>\n",
       "      <td>0.328739</td>\n",
       "      <td>-1.053294</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff7c7e70df07</td>\n",
       "      <td>They used all sorts of chemical concoctions to...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>fffbccfd8a08</td>\n",
       "      <td>The meat would smell sour but the would \"rub i...</td>\n",
       "      <td>1.771596</td>\n",
       "      <td>0.547742</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7165 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt_id                                    prompt_question  \\\n",
       "0       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "1       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "2       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "3       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "4       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "...        ...                                                ...   \n",
       "7160    ebad26  Summarize the various ways the factory would u...   \n",
       "7161    ebad26  Summarize the various ways the factory would u...   \n",
       "7162    ebad26  Summarize the various ways the factory would u...   \n",
       "7163    ebad26  Summarize the various ways the factory would u...   \n",
       "7164    ebad26  Summarize the various ways the factory would u...   \n",
       "\n",
       "                 prompt_title  \\\n",
       "0                  On Tragedy   \n",
       "1                  On Tragedy   \n",
       "2                  On Tragedy   \n",
       "3                  On Tragedy   \n",
       "4                  On Tragedy   \n",
       "...                       ...   \n",
       "7160  Excerpt from The Jungle   \n",
       "7161  Excerpt from The Jungle   \n",
       "7162  Excerpt from The Jungle   \n",
       "7163  Excerpt from The Jungle   \n",
       "7164  Excerpt from The Jungle   \n",
       "\n",
       "                                            prompt_text    student_id  \\\n",
       "0     Chapter 13 \\r\\nAs the sequel to what has alrea...  00791789cc1f   \n",
       "1     Chapter 13 \\r\\nAs the sequel to what has alrea...  0086ef22de8f   \n",
       "2     Chapter 13 \\r\\nAs the sequel to what has alrea...  0094589c7a22   \n",
       "3     Chapter 13 \\r\\nAs the sequel to what has alrea...  00cd5736026a   \n",
       "4     Chapter 13 \\r\\nAs the sequel to what has alrea...  00d98b8ff756   \n",
       "...                                                 ...           ...   \n",
       "7160  With one member trimming beef in a cannery, an...  ff37545b2805   \n",
       "7161  With one member trimming beef in a cannery, an...  ff4ed38ef099   \n",
       "7162  With one member trimming beef in a cannery, an...  ff53b94f7ce0   \n",
       "7163  With one member trimming beef in a cannery, an...  ff7c7e70df07   \n",
       "7164  With one member trimming beef in a cannery, an...  fffbccfd8a08   \n",
       "\n",
       "                                                   text   content   wording  \\\n",
       "0     1 element of an ideal tragedy is that it shoul... -0.210614 -0.471415   \n",
       "1     The three elements of an ideal tragedy are:  H... -0.970237 -0.417058   \n",
       "2     Aristotle states that an ideal tragedy should ... -0.387791 -0.584181   \n",
       "3     One element of an Ideal tragedy is having a co...  0.088882 -0.594710   \n",
       "4     The 3 ideal of tragedy is how complex you need... -0.687288 -0.460886   \n",
       "...                                                 ...       ...       ...   \n",
       "7160  In paragraph two, they would use pickle meat a...  1.520355 -0.292990   \n",
       "7161  in the first paragraph  it says \"either can it... -1.204574 -1.169784   \n",
       "7162  They would have piles of filthy meat on the fl...  0.328739 -1.053294   \n",
       "7163  They used all sorts of chemical concoctions to...  0.205683  0.380538   \n",
       "7164  The meat would smell sour but the would \"rub i...  1.771596  0.547742   \n",
       "\n",
       "      fold  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "...    ...  \n",
       "7160     3  \n",
       "7161     3  \n",
       "7162     3  \n",
       "7163     3  \n",
       "7164     3  \n",
       "\n",
       "[7165 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2bILjGET86jN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vpC_oJw86jO"
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3p6AtTj_86jO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSHcmlTt86jP"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X-XxwPIR86jP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_lm_datacollator = DataCollatorForWholeWordMask(tokenizer)\n",
    "def data_collator(batch):\n",
    "    input_ids = [{'input_ids':i[0]} for i in batch]\n",
    "    token_type_ids = [i[1] for i in batch]\n",
    "    attention_mask = [i[2] for i in batch]\n",
    "    labels = [i[3] for i in batch]\n",
    "    masked_input = mask_lm_datacollator(input_ids)['input_ids']\n",
    "    return masked_input,\\\n",
    "               torch.stack(token_type_ids),\\\n",
    "               torch.stack(attention_mask),\\\n",
    "               torch.stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.prompt_title = df['prompt_title'].values.astype(str)\n",
    "        self.prompt_text = df['prompt_text'].values.astype(str)\n",
    "        self.prompt_question = df['prompt_question'].values.astype(str)\n",
    "        self.text = df['text'].values.astype(str)\n",
    "        self.content = df['content'].values\n",
    "        self.wording = df['wording'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.prompt_title)\n",
    "    \n",
    "    def tokenize(self, example):\n",
    "        sep = self.tokenizer.sep_token\n",
    "        if  CFG.input_type == '1':\n",
    "            prompt = sep.join([example[\"prompt_title\"], example[\"prompt_text\"], example[\"prompt_question\"]])\n",
    "        else:\n",
    "            prompt = example[\"prompt_question\"] \n",
    "        \n",
    "        labels = [float(example[\"content\"]), float(example[\"wording\"])]\n",
    "\n",
    "        tokenized = tokenizer(\n",
    "            example[\"text\"],\n",
    "            prompt,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=CFG.max_input_length,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        example = {\n",
    "                    \"prompt_title\":self.prompt_title[item],\n",
    "                    \"prompt_text\":self.prompt_text[item],\n",
    "                    \"prompt_question\":self.prompt_question[item],\n",
    "                    \"text\":self.text[item],\n",
    "                    \"content\":self.content[item],\n",
    "                    \"wording\":self.wording[item],\n",
    "                  }\n",
    "        \n",
    "        out = self.tokenize(example)\n",
    "       \n",
    "        return {\n",
    "                'input_ids': torch.as_tensor(out['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids': torch.as_tensor(out['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask': torch.as_tensor(out['attention_mask'], dtype=torch.long),\n",
    "                'labels': torch.as_tensor(out['labels'], dtype=torch.float),\n",
    "        }\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP7TFS-c86jQ"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0A7qv3qn86jQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_params(module_lst):\n",
    "    for module in module_lst:\n",
    "        for param in module.parameters():\n",
    "            if param.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "    return\n",
    "\n",
    "class Custom_Bert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\"output_hidden_states\":True})\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "\n",
    "        dim = config.hidden_size\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        n_weights = 24\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(dim,1)\n",
    "        )\n",
    "        init_params([self.cls,self.attention])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                )\n",
    "\n",
    "        cls_outputs = torch.stack(\n",
    "            [self.dropout(layer) for layer in base_output['hidden_states'][-24:]], dim=0\n",
    "        )\n",
    "        cls_output = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * cls_outputs).sum(0)\n",
    "\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [torch.sum(self.attention(self.high_dropout(cls_output)) * cls_output, dim=1) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        output = self.cls(logits)\n",
    "        if labels is None:\n",
    "            return output\n",
    "\n",
    "        else:\n",
    "            return (nn.MSELoss()(torch.squeeze(output,1),labels), output)\n",
    "\n",
    "\n",
    "class Custom_Bert_Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "        dim = config.hidden_size\n",
    "        self.dropout = nn.Dropout(p=0)\n",
    "        self.cls = nn.Linear(dim,2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids\n",
    "                               )\n",
    "        output = base_output.last_hidden_state\n",
    "        output = self.cls(torch.mean(output, dim=1))\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=nn.MSELoss()(output,labels),\n",
    "            logits=output, \n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "\n",
    "class GeMText(nn.Module):\n",
    "    def __init__(self, dim = 1, p=3, eps=1e-6):\n",
    "        super(GeMText, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.feat_mult = 1\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n",
    "        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n",
    "        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n",
    "        ret = ret.pow(1 / self.p)\n",
    "        return ret    \n",
    "\n",
    "class Custom_Bert_Pool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        self.config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=self.config)\n",
    "        self.pool = GeMText()\n",
    "        self.cls = nn.Linear(self.config.hidden_size,2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids\n",
    "                               )\n",
    "        output = base_output.last_hidden_state\n",
    "        output = self.cls(self.pool(output, attention_mask))\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=nn.SmoothL1Loss()(output,labels),\n",
    "            logits=output, \n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "class Custom_Bert_Mean(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.output_hidden_states=True\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "        dim = config.hidden_size\n",
    "        self.dropout = nn.Dropout(p=0)\n",
    "        self.cls = nn.Linear(dim,1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,labels=None):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                            )\n",
    "\n",
    "\n",
    "        output = base_output.hidden_states[-1]\n",
    "        output = self.cls(self.dropout(torch.mean(output, dim=1)))\n",
    "        if labels is None:\n",
    "            return output\n",
    "\n",
    "        else:\n",
    "            return (nn.MSELoss()(torch.squeeze(output,1),labels), output)\n",
    "\n",
    "class Custom_Bert_M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\"output_hidden_states\":True})\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "\n",
    "        dim = config.hidden_size\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        n_weights = 24\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.cls_0 = nn.Sequential(\n",
    "            nn.Linear(dim,1)\n",
    "        )\n",
    "\n",
    "        self.cls_1 = nn.Linear(dim,5)\n",
    "        init_params([self.cls_0,self.cls_1,self.attention])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                             )\n",
    "\n",
    "        cls_outputs = torch.stack(\n",
    "            [self.dropout(layer) for layer in base_output['hidden_states'][-24:]], dim=0\n",
    "        )\n",
    "        cls_output = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * cls_outputs).sum(0)\n",
    "\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [torch.sum(self.attention(self.high_dropout(cls_output)) * cls_output, dim=1) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        output_0 = self.cls_0(logits)\n",
    "        output_1 = self.cls_1(logits)\n",
    "        if labels is None:\n",
    "            return output_0\n",
    "\n",
    "        else:\n",
    "            regression_loss = nn.MSELoss()(torch.squeeze(output_0,1),labels)\n",
    "            labels = labels.double()\n",
    "            cls_labels = torch.where(labels==1.,4.0,labels)\n",
    "            cls_labels = torch.where(cls_labels==0.25,1.0,cls_labels)\n",
    "            cls_labels = torch.where(cls_labels==0.5,2.0,cls_labels)\n",
    "            cls_labels = torch.where(cls_labels==0.75,3.0,cls_labels)\n",
    "            cls_labels = cls_labels.long()\n",
    "            cls_loss = nn.CrossEntropyLoss()(output_1, cls_labels)\n",
    "            return ( 0.8 * regression_loss + 0.2 * cls_loss, output_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    if CFG.model_type == 'base':\n",
    "        model_config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        model_config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "\n",
    "        #print(model_config)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            CFG.model_path, config=model_config\n",
    "        )\n",
    "    if CFG.model_type == 'simple':\n",
    "        model = Custom_Bert_Simple()\n",
    "    if CFG.model_type == 'pool':\n",
    "        model = Custom_Bert_Pool()\n",
    "        if CFG.reinit_layers > 0:\n",
    "            print(\"==\"*40)\n",
    "            print(f\"Reinitialize the last {CFG.reinit_layers} layer(s).\")\n",
    "            for layer in model.base.encoder.layer[-CFG.reinit_layers:]:\n",
    "                print(\"===\")\n",
    "                layer.apply(model._init_weights)\n",
    "            print(\"==\"*40)\n",
    "        if CFG.load_pretrained:\n",
    "            model.load_state_dict(torch.load('./pretrained/microsoft_deberta-v3-base_best_ema.pth')['model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op1OHevU86jR"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "class ModelEMA:\n",
    "    \"\"\"Model Exponential Moving Average from https://github.com/rwightman/\n",
    "    pytorch-image-models Keep a moving average of everything in the model\n",
    "    state_dict (parameters and buffers).\n",
    "\n",
    "    This is intended to allow functionality like\n",
    "    https://www.tensorflow.org/api_docs/python/tf/train/\n",
    "    ExponentialMovingAverage\n",
    "    A smoothed version of the weights is necessary for some training\n",
    "    schemes to perform well.\n",
    "    This class is sensitive where it is initialized in the sequence\n",
    "    of model init, GPU assignment and distributed training wrappers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay=0.9999, updates=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (nn.Module): model to apply EMA.\n",
    "            decay (float): ema decay reate.\n",
    "            updates (int): counter of EMA updates.\n",
    "        \"\"\"\n",
    "        # Create EMA(FP32)\n",
    "        self.ema_model = deepcopy(model).eval()\n",
    "        self.ema = self.ema_model\n",
    "        self.updates = updates\n",
    "        # decay exponential ramp (to help early epochs)\n",
    "        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        # Update EMA parameters\n",
    "        with torch.no_grad():\n",
    "            self.updates += 1\n",
    "            d = self.decay(self.updates)\n",
    "            msd =  model.state_dict()# model state_dict\n",
    "            for k, v in self.ema.state_dict().items():\n",
    "                if v.dtype.is_floating_point:\n",
    "                    v *= d\n",
    "                    v += (1.0 - d) * msd[k].detach()\n",
    "\n",
    "class EMAHook:\n",
    "    \"\"\"EMAHook used in BEVDepth.\n",
    "\n",
    "    Modified from https://github.com/Megvii-Base\n",
    "    Detection/BEVDepth/blob/main/callbacks/ema.py.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, init_updates=0, decay=0.9990, resume=None, logger=None):\n",
    "        super().__init__()\n",
    "        self.init_updates = init_updates\n",
    "        self.resume = resume\n",
    "        self.decay = decay\n",
    "        self.ema_model = self.before_run(model)\n",
    "        self.logger = logger\n",
    "\n",
    "    def before_run(self, model):\n",
    "        from torch.nn.modules.batchnorm import SyncBatchNorm\n",
    "\n",
    "        bn_model_list = list()\n",
    "        bn_model_dist_group_list = list()\n",
    "        for model_ref in model.modules():\n",
    "            if isinstance(model_ref, SyncBatchNorm):\n",
    "                bn_model_list.append(model_ref)\n",
    "                bn_model_dist_group_list.append(model_ref.process_group)\n",
    "                model_ref.process_group = None\n",
    "        ema_model = ModelEMA(model, self.decay)\n",
    "\n",
    "        for bn_model, dist_group in zip(bn_model_list,\n",
    "                                        bn_model_dist_group_list):\n",
    "            bn_model.process_group = dist_group\n",
    "        ema_model.updates = self.init_updates\n",
    "\n",
    "        if self.resume is not None:\n",
    "            self.logger.info(f'resume ema checkpoint from {self.resume}')\n",
    "            cpt = torch.load(self.resume, map_location='cpu')\n",
    "            load_state_dict(ema_model.ema, cpt['state_dict'])\n",
    "            ema_model.updates = cpt['updates']\n",
    "\n",
    "        return ema_model\n",
    "\n",
    "    def after_train_iter(self, model):\n",
    "        self.ema_model.update(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "O__vnlBV86jR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qYJKjn8H86jR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.discriminative_learning_rate_num_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_llr_params(model, type='s'):\n",
    "    \"\"\"\n",
    "    Setup the optimizer.\n",
    "    We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
    "    Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
    "\n",
    "    MODIFIED VERSION:\n",
    "    * added support for differential learning rates per layer\n",
    "\n",
    "    reference: https://github.com/huggingface/transformers/blob/05fa1a7ac17bb7aa07b9e0c1e138ecb31a28bbfe/src/transformers/trainer.py#L804\n",
    "    \"\"\"\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
    "    ### ADDED\n",
    "    if CFG.discriminative_learning_rate:\n",
    "\n",
    "        num_layers = model.config.num_hidden_layers\n",
    "\n",
    "        learning_rate_powers = range(0, num_layers, num_layers//CFG.discriminative_learning_rate_num_groups)\n",
    "        layer_wise_learning_rates = [\n",
    "            pow(CFG.discriminative_learning_rate_decay_rate, power) * CFG.encoder_lr \n",
    "            for power in learning_rate_powers \n",
    "            for _ in range(num_layers//CFG.discriminative_learning_rate_num_groups)\n",
    "          ]\n",
    "        layer_wise_learning_rates = layer_wise_learning_rates[::-1]\n",
    "        print('Layer-wise learning rates:', layer_wise_learning_rates)\n",
    "\n",
    "        # group embedding paramters from the transformer encoder\n",
    "        embedding_layer = model.base.embeddings\n",
    "        optimizer_grouped_parameters = [\n",
    "          {\n",
    "              \"params\": [p for n, p in embedding_layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "              \"lr\": pow(CFG.discriminative_learning_rate_decay_rate, num_layers) * CFG.encoder_lr ,\n",
    "              \"weight_decay\": CFG.weight_decay,\n",
    "          },\n",
    "          {\n",
    "              \"params\": [p for n, p in embedding_layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "              \"lr\": pow(CFG.discriminative_learning_rate_decay_rate, num_layers) * CFG.encoder_lr ,\n",
    "              \"weight_decay\": 0.0,\n",
    "          },\n",
    "        ]\n",
    "\n",
    "        # group encoding paramters from the transformer encoder\n",
    "        encoding_layers = [layer for layer in model.base.encoder.layer]\n",
    "        for i, layer in enumerate(encoding_layers):\n",
    "            optimizer_grouped_parameters += [\n",
    "                {\n",
    "                    \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                    \"lr\": layer_wise_learning_rates[i],\n",
    "                    \"weight_decay\": CFG.weight_decay,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                    \"lr\": layer_wise_learning_rates[i],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "            ]    \n",
    "        print(f\"Detected unattached modules in model.encoder: {[n for n, p in model.base.encoder.named_parameters() if not n.startswith('layer')]}\")\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.base.encoder.named_parameters() if not n.startswith('layer') and not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": layer_wise_learning_rates[-1],\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.base.encoder.named_parameters() if not n.startswith('layer') and any(nd in n for nd in no_decay)],\n",
    "                \"lr\": layer_wise_learning_rates[-1],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # group paramters from the task specific head\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if 'base' not in n and not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.head_lr,\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if 'base' not in n and any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.head_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    ### END ADDED\n",
    "    else:\n",
    "        # group paramters for the entire network\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.encoder_lr,\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.encoder_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3HIHxVsj86jS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    start = end = time.time()\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        for key, value in batch.items():\n",
    "            batch[key] = value.to(device)\n",
    "        batch_size = batch['labels'].size(0)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**batch)\n",
    "        label = batch['labels']\n",
    "        loss, logits = model_output.loss, model_output.logits\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(logits.to('cpu').numpy())\n",
    "        labels.append(label.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, epoch, scheduler, device, valid_loader, start_time, best_score, best_score_ema,ema_hook,wandb, fold):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        for key, value in batch.items():\n",
    "            batch[key] = value.to(device)\n",
    "        batch_size = batch['labels'].size(0)\n",
    "        loss = model(**batch).loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        optimizer.step()\n",
    "        ema_hook.after_train_iter(model)\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        \n",
    "        wandb.log({\n",
    "                'train loss': loss.item(),\n",
    "                'step': global_step,\n",
    "                'epoch': epoch\n",
    "            })\n",
    "        \n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "            \n",
    "            # eval\n",
    "            avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, model, CFG.device)\n",
    "\n",
    "            # scoring\n",
    "            score = compute_mcrmse((predictions, valid_labels))\n",
    "\n",
    "\n",
    "            content_rmse, wording_rmse, mcrmse = list(score.values())\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch + 1} avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - content_rmse: {content_rmse:.4f} - wording_rmse: {wording_rmse:.4f} - mcrmse: {mcrmse:.4f}')\n",
    "            \n",
    "            \n",
    "            if best_score > score['mcrmse']:\n",
    "                if best_score != float('inf'):\n",
    "                    os.remove(CFG.OUTPUT_DIR + \"{}_best{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score))\n",
    "                best_score = score['mcrmse']\n",
    "                best_predictions = predictions\n",
    "                LOGGER.info(f'Epoch {epoch + 1} - Save Best Score: {best_score:.4f} Model')\n",
    "                torch.save({'model': model.state_dict(),\n",
    "                            'predictions': predictions},\n",
    "                           CFG.OUTPUT_DIR + \"{}_best{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score))\n",
    "            \n",
    "            \n",
    "            avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, ema_hook.ema_model.ema, CFG.device)\n",
    "            # ema scoring\n",
    "            ema_score = compute_mcrmse((predictions, valid_labels))\n",
    "\n",
    "\n",
    "            content_rmse, wording_rmse, mcrmse = list(ema_score.values())\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch + 1} avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - ema_content_rmse: {content_rmse:.4f} - ema_wording_rmse: {wording_rmse:.4f} - ema_mcrmse: {mcrmse:.4f}')\n",
    "            \n",
    "            \n",
    "            if best_score_ema > ema_score['mcrmse']:\n",
    "                if best_score_ema != float('inf'):\n",
    "                    os.remove(CFG.OUTPUT_DIR + \"{}_best_ema{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score_ema))\n",
    "                best_score_ema = ema_score['mcrmse']\n",
    "                best_predictions = predictions\n",
    "                LOGGER.info(f'Epoch {epoch + 1} - ema_Save Best Score: {best_score_ema:.4f} Model')\n",
    "                torch.save({'model': ema_hook.ema_model.ema.state_dict(),\n",
    "                            'predictions': predictions},\n",
    "                           CFG.OUTPUT_DIR + \"{}_best_ema{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold,best_score_ema))\n",
    "            \n",
    "            wandb.log({\n",
    "            'learning rate': optimizer.param_groups[0]['lr'],\n",
    "            'validation mcrmse': score['mcrmse'],\n",
    "            'validation ema mcrmse': ema_score['mcrmse'],\n",
    "            'step': global_step,\n",
    "            'epoch': epoch,\n",
    "        })\n",
    "            \n",
    "            model.train()\n",
    "    return losses.avg, best_score, best_score_ema\n",
    "\n",
    "\n",
    "\n",
    "def train_loop():\n",
    "    LOGGER.info(f\"========== training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    wandb.init(project='commit_summary')\n",
    "    wandb.config = dict(epochs=CFG.epochs, \n",
    "                            batch_size=CFG.batch_size, \n",
    "                            learning_rate=CFG.encoder_lr,\n",
    "                            save_checkpoint=True,\n",
    "                            )\n",
    "    for fold in CFG.folds:\n",
    "        \n",
    "        if CFG.pretraining:\n",
    "            tr_data = pd.read_csv('tmp_pessudo.csv')\n",
    "            tr_data['prompt_title'] = ''\n",
    "            tr_data = tr_data[-(tr_data['prompt_question'].isin(pdf['prompt_question'].tolist()))]\n",
    "            va_data = df #df[df['fold']==fold].reset_index(drop=True)\n",
    "        else:\n",
    "            tr_data = df[df['fold']!=fold].reset_index(drop=True)\n",
    "            va_data = df[df['fold']==fold].reset_index(drop=True)\n",
    "        train_dataset = TrainDataset(tr_data, tokenizer)\n",
    "        valid_dataset = TrainDataset(va_data, tokenizer)\n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                                  batch_size=CFG.batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "        valid_loader = DataLoader(valid_dataset,\n",
    "                                  batch_size=CFG.batch_size * 2,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "        # ====================================================\n",
    "        # model & optimizer\n",
    "        # ====================================================\n",
    "        model = build_model()\n",
    "        #model = AutoModelForSequenceClassification.from_pretrained(CFG.model_path, num_labels=1)\n",
    "        model.to(CFG.device)\n",
    "        # for param in model.base.parameters():\n",
    "        #         param.requires_grad = False\n",
    "        ema_hook = EMAHook(model, init_updates=3000, logger=LOGGER)\n",
    "        def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "            param_optimizer = list(model.named_parameters())\n",
    "            no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_parameters = [\n",
    "                {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                 'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "                {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                 'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            ]\n",
    "            return optimizer_parameters\n",
    "\n",
    "        optimizer_parameters = get_optimizer_llr_params(model)\n",
    "        optimizer = AdamW(optimizer_parameters, eps=CFG.eps, betas=CFG.betas)\n",
    "\n",
    "\n",
    "        \n",
    "        # ====================================================\n",
    "        # scheduler\n",
    "        # ====================================================\n",
    "        def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "            cfg.num_warmup_steps = cfg.num_warmup_steps * num_train_steps\n",
    "            if cfg.scheduler == 'linear':\n",
    "                scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "                )\n",
    "            elif cfg.scheduler == 'cosine':\n",
    "                scheduler = get_cosine_schedule_with_warmup(\n",
    "                    optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps,\n",
    "                    num_cycles=cfg.num_cycles\n",
    "                )\n",
    "            return scheduler\n",
    "\n",
    "        num_train_steps = int(len(train_dataset) / CFG.batch_size * CFG.epochs)\n",
    "        scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "        # ====================================================\n",
    "        # loop\n",
    "        # ====================================================\n",
    "        # criterion = torch.nn.CrossEntropyLoss(ignore_index=- 1)\n",
    "\n",
    "        # criterion = LabelSmoothingLoss()\n",
    "        best_score = float('inf')\n",
    "        best_score_ema = float('inf')\n",
    "        for epoch in range(CFG.epochs):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # train\n",
    "            avg_loss, best_score, best_score_ema = train_fn(train_loader, model, optimizer, epoch, scheduler, CFG.device, valid_loader, start_time, best_score, best_score_ema ,ema_hook, wandb,fold)\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        del scheduler, optimizer, model\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbgHecjCyz5D",
    "outputId": "09f6fba5-dd11-4adc-a85c-8805c6d6c90d",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== training ==========\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeng_sun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3e7eb215d4941c6b63d7c158983a12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01667054104618728, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/workspace/commonlit/wandb/run-20230905_035123-mvemvhpj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peng_sun/commit_summary/runs/mvemvhpj' target=\"_blank\">absurd-thunder-4</a></strong> to <a href='https://wandb.ai/peng_sun/commit_summary' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peng_sun/commit_summary' target=\"_blank\">https://wandb.ai/peng_sun/commit_summary</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peng_sun/commit_summary/runs/mvemvhpj' target=\"_blank\">https://wandb.ai/peng_sun/commit_summary/runs/mvemvhpj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/378] Elapsed 0m 1s (remain 11m 52s) Loss: 1.1001(1.1001) Grad: 4.7941  LR: 0.00005000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 1.3363  time: 18s\n",
      "Epoch 1 - content_rmse: 1.1309 - wording_rmse: 1.1805 - mcrmse: 1.1557\n",
      "Epoch 1 - Save Best Score: 1.1557 Model\n",
      "Epoch 1 avg_val_loss: 1.3115  time: 35s\n",
      "Epoch 1 - ema_content_rmse: 1.1279 - ema_wording_rmse: 1.1623 - ema_mcrmse: 1.1451\n",
      "Epoch 1 - ema_Save Best Score: 1.1451 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][100/378] Elapsed 1m 41s (remain 4m 37s) Loss: 0.3013(0.4935) Grad: 2.5886  LR: 0.00004965  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.6427  time: 117s\n",
      "Epoch 1 - content_rmse: 0.6377 - wording_rmse: 0.9374 - mcrmse: 0.7875\n",
      "Epoch 1 - Save Best Score: 0.7875 Model\n",
      "Epoch 1 avg_val_loss: 0.6070  time: 135s\n",
      "Epoch 1 - ema_content_rmse: 0.6107 - ema_wording_rmse: 0.9171 - ema_mcrmse: 0.7639\n",
      "Epoch 1 - ema_Save Best Score: 0.7639 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][200/378] Elapsed 3m 20s (remain 2m 56s) Loss: 0.2319(0.4088) Grad: 3.6238  LR: 0.00004862  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.3897  time: 217s\n",
      "Epoch 1 - content_rmse: 0.5040 - wording_rmse: 0.7248 - mcrmse: 0.6144\n",
      "Epoch 1 - Save Best Score: 0.6144 Model\n",
      "Epoch 1 avg_val_loss: 0.4500  time: 234s\n",
      "Epoch 1 - ema_content_rmse: 0.5168 - ema_wording_rmse: 0.7955 - ema_mcrmse: 0.6562\n",
      "Epoch 1 - ema_Save Best Score: 0.6562 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][300/378] Elapsed 5m 0s (remain 1m 16s) Loss: 0.2304(0.3654) Grad: 5.0561  LR: 0.00004695  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.8976  time: 316s\n",
      "Epoch 1 - content_rmse: 0.7046 - wording_rmse: 1.1396 - mcrmse: 0.9221\n",
      "Epoch 1 avg_val_loss: 0.6338  time: 332s\n",
      "Epoch 1 - ema_content_rmse: 0.6301 - ema_wording_rmse: 0.9331 - ema_mcrmse: 0.7816\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][377/378] Elapsed 6m 22s (remain 0m 0s) Loss: 0.1648(0.3440) Grad: 1.8462  LR: 0.00004524  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.4179  time: 398s\n",
      "Epoch 1 - content_rmse: 0.5623 - wording_rmse: 0.7208 - mcrmse: 0.6416\n",
      "Epoch 1 avg_val_loss: 0.4884  time: 414s\n",
      "Epoch 1 - ema_content_rmse: 0.5808 - ema_wording_rmse: 0.7997 - ema_mcrmse: 0.6903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/378] Elapsed 0m 0s (remain 4m 58s) Loss: 0.2787(0.2787) Grad: 5.0218  LR: 0.00004522  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.4177  time: 17s\n",
      "Epoch 2 - content_rmse: 0.5644 - wording_rmse: 0.7190 - mcrmse: 0.6417\n",
      "Epoch 2 avg_val_loss: 0.4715  time: 33s\n",
      "Epoch 2 - ema_content_rmse: 0.5772 - ema_wording_rmse: 0.7809 - ema_mcrmse: 0.6791\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][100/378] Elapsed 1m 37s (remain 4m 27s) Loss: 0.1234(0.2167) Grad: 1.5733  LR: 0.00004252  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.4941  time: 113s\n",
      "Epoch 2 - content_rmse: 0.5553 - wording_rmse: 0.8245 - mcrmse: 0.6899\n",
      "Epoch 2 avg_val_loss: 0.4508  time: 129s\n",
      "Epoch 2 - ema_content_rmse: 0.5595 - ema_wording_rmse: 0.7672 - ema_mcrmse: 0.6633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][200/378] Elapsed 3m 13s (remain 2m 50s) Loss: 0.1970(0.2086) Grad: 4.1330  LR: 0.00003933  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.4062  time: 210s\n",
      "Epoch 2 - content_rmse: 0.5626 - wording_rmse: 0.7042 - mcrmse: 0.6334\n",
      "Epoch 2 avg_val_loss: 0.3908  time: 226s\n",
      "Epoch 2 - ema_content_rmse: 0.5422 - ema_wording_rmse: 0.6983 - ema_mcrmse: 0.6203\n",
      "Epoch 2 - ema_Save Best Score: 0.6203 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][300/378] Elapsed 4m 52s (remain 1m 14s) Loss: 0.1320(0.2035) Grad: 1.7506  LR: 0.00003575  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.4899  time: 308s\n",
      "Epoch 2 - content_rmse: 0.6252 - wording_rmse: 0.7674 - mcrmse: 0.6963\n",
      "Epoch 2 avg_val_loss: 0.4693  time: 324s\n",
      "Epoch 2 - ema_content_rmse: 0.5840 - ema_wording_rmse: 0.7731 - ema_mcrmse: 0.6785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][377/378] Elapsed 6m 13s (remain 0m 0s) Loss: 0.1515(0.1999) Grad: 2.5307  LR: 0.00003279  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.4924  time: 390s\n",
      "Epoch 2 - content_rmse: 0.6071 - wording_rmse: 0.7850 - mcrmse: 0.6961\n",
      "Epoch 2 avg_val_loss: 0.4611  time: 406s\n",
      "Epoch 2 - ema_content_rmse: 0.6273 - ema_wording_rmse: 0.7272 - ema_mcrmse: 0.6772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/378] Elapsed 0m 0s (remain 5m 2s) Loss: 0.1047(0.1047) Grad: 1.7008  LR: 0.00003275  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.5056  time: 17s\n",
      "Epoch 3 - content_rmse: 0.6024 - wording_rmse: 0.8052 - mcrmse: 0.7038\n",
      "Epoch 3 avg_val_loss: 0.4630  time: 33s\n",
      "Epoch 3 - ema_content_rmse: 0.6218 - ema_wording_rmse: 0.7344 - ema_mcrmse: 0.6781\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][100/378] Elapsed 1m 37s (remain 4m 27s) Loss: 0.1250(0.1452) Grad: 3.7623  LR: 0.00002872  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.6681  time: 113s\n",
      "Epoch 3 - content_rmse: 0.8404 - wording_rmse: 0.7937 - mcrmse: 0.8171\n",
      "Epoch 3 avg_val_loss: 0.5491  time: 129s\n",
      "Epoch 3 - ema_content_rmse: 0.7420 - ema_wording_rmse: 0.7401 - ema_mcrmse: 0.7410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][200/378] Elapsed 3m 13s (remain 2m 50s) Loss: 0.2219(0.1405) Grad: 3.8738  LR: 0.00002459  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.4481  time: 210s\n",
      "Epoch 3 - content_rmse: 0.6592 - wording_rmse: 0.6795 - mcrmse: 0.6694\n",
      "Epoch 3 avg_val_loss: 0.4715  time: 226s\n",
      "Epoch 3 - ema_content_rmse: 0.6650 - ema_wording_rmse: 0.7077 - ema_mcrmse: 0.6864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][300/378] Elapsed 4m 50s (remain 1m 14s) Loss: 0.1461(0.1389) Grad: 3.8237  LR: 0.00002046  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.5657  time: 307s\n",
      "Epoch 3 - content_rmse: 0.7392 - wording_rmse: 0.7648 - mcrmse: 0.7520\n",
      "Epoch 3 avg_val_loss: 0.4683  time: 323s\n",
      "Epoch 3 - ema_content_rmse: 0.6527 - ema_wording_rmse: 0.7145 - ema_mcrmse: 0.6836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][377/378] Elapsed 6m 12s (remain 0m 0s) Loss: 0.1350(0.1372) Grad: 1.4851  LR: 0.00001737  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.4531  time: 388s\n",
      "Epoch 3 - content_rmse: 0.6371 - wording_rmse: 0.7073 - mcrmse: 0.6722\n",
      "Epoch 3 avg_val_loss: 0.4484  time: 404s\n",
      "Epoch 3 - ema_content_rmse: 0.6337 - ema_wording_rmse: 0.7038 - ema_mcrmse: 0.6687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/378] Elapsed 0m 0s (remain 5m 5s) Loss: 0.0691(0.0691) Grad: 1.0979  LR: 0.00001733  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.4639  time: 17s\n",
      "Epoch 4 - content_rmse: 0.6477 - wording_rmse: 0.7130 - mcrmse: 0.6804\n",
      "Epoch 4 avg_val_loss: 0.4501  time: 33s\n",
      "Epoch 4 - ema_content_rmse: 0.6353 - ema_wording_rmse: 0.7047 - ema_mcrmse: 0.6700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][100/378] Elapsed 1m 37s (remain 4m 27s) Loss: 0.1037(0.0813) Grad: 1.5270  LR: 0.00001351  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-43:\n",
      "Traceback (most recent call last):\n",
      "  File \"/root/miniconda3/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/root/miniconda3/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n",
      "    do_one_step()\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n",
      "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/root/miniconda3/lib/python3.8/multiprocessing/queues.py\", line 116, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/root/miniconda3/lib/python3.8/site-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/root/miniconda3/lib/python3.8/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/root/miniconda3/lib/python3.8/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/root/miniconda3/lib/python3.8/multiprocessing/connection.py\", line 508, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/root/miniconda3/lib/python3.8/multiprocessing/connection.py\", line 756, in answer_challenge\n",
      "    connection.send_bytes(digest)\n",
      "  File \"/root/miniconda3/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/root/miniconda3/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/root/miniconda3/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 214\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m    211\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m     avg_loss, best_score, best_score_ema \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_score_ema\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mema_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    218\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[0;32mIn[18], line 63\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, optimizer, epoch, scheduler, device, valid_loader, start_time, best_score, best_score_ema, ema_hook, wandb, fold)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: [\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m][\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     52\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mElapsed \u001b[39m\u001b[38;5;132;01m{remain:s}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     53\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{loss.val:.4f}\u001b[39;00m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{loss.avg:.4f}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m               grad_norm\u001b[38;5;241m=\u001b[39mgrad_norm,\n\u001b[1;32m     60\u001b[0m               lr\u001b[38;5;241m=\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mget_lr()[\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# eval\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m avg_val_loss, predictions, valid_labels \u001b[38;5;241m=\u001b[39m \u001b[43mvalid_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# scoring\u001b[39;00m\n\u001b[1;32m     66\u001b[0m score \u001b[38;5;241m=\u001b[39m compute_mcrmse((predictions, valid_labels))\n",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m, in \u001b[0;36mvalid_fn\u001b[0;34m(valid_loader, model, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mloss, model_output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m---> 15\u001b[0m losses\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, batch_size)\n\u001b[1;32m     16\u001b[0m preds\u001b[38;5;241m.\u001b[39mappend(logits\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     17\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(label\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BV3Rp1bbXUQ-"
   },
   "outputs": [],
   "source": [
    "## total_complex = []\n",
    "# for fold in range(4):\n",
    "#     va_data = train_df[train_df['fold'] == fold]\n",
    "#     preds = torch.load('/content/drive/MyDrive/deb_simple/microsoft_deberta-v3-large_best{}.pth'.format(fold))['predictions']\n",
    "#     va_data['preds'] = preds\n",
    "#     va_data = va_data[['id', 'preds', 'score']]\n",
    "#     print(compute_metrics(va_data['preds'].values.reshape(-1,1), va_data['score'].values))\n",
    "#     total_complex.append(va_data)\n",
    "# total_complex = pd.concat(total_complex)\n",
    "# compute_metrics(total_complex['preds'].values.reshape(-1,1), total_complex['score'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11_KW3qSx1IK",
    "outputId": "e5423c46-d702-49da-d29b-c5d125a665e4"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /root/.kaggle\n",
    "# !cp /content/drive/MyDrive/kaggle/kaggle.json /root/.kaggle/\n",
    "# !chmod 600 /root/.kaggle/kaggle.json\n",
    "# !kaggle datasets init -p /content/drive/MyDrive/elc_mean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cN6F3A_hKthE",
    "outputId": "0358b1da-6195-44ba-8ab5-2eacb5268fff"
   },
   "outputs": [],
   "source": [
    "#!kaggle datasets create -p /content/drive/MyDrive/elc_mean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BGinNBYJFw3O"
   },
   "outputs": [],
   "source": [
    "# deberta v3 large\n",
    "# 1.5 0.8228\n",
    "# 2 0.8197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVfKappB86jX",
    "tags": []
   },
   "source": [
    "# 1.5  8137\n",
    "#2 8175\n",
    "#2.5 8181\n",
    "#3 8181\n",
    "#3.5 8175\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
