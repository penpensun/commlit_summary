{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dca6380",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd;\n",
    "pdf_test = pd.read_csv('./input/prompts_test.csv')\n",
    "sdf_test = pd.read_csv('./input/summaries_test.csv')\n",
    "df_test = pdf_test.merge(sdf_test, on ='prompt_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8737b366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    set_seed,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    HfArgumentParser,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from datasets import Dataset, disable_progress_bar\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.disable(logging.ERROR)\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ['WANDB_PROJECT'] = 'kaggle-commonlit-eval-student-summaries'\n",
    "\n",
    "disable_progress_bar()\n",
    "\n",
    "\n",
    "def tokenize(example, add_prompt_question, add_prompt_text):\n",
    "    \"\"\"\n",
    "    To see how long it would be with prompt question, prompt text, and text.\n",
    "    \"\"\"\n",
    "\n",
    "    cols = []\n",
    "\n",
    "    if add_prompt_question:\n",
    "        cols.append(\"prompt_question\")\n",
    "    if add_prompt_text:\n",
    "        cols.append(\"prompt_text\")\n",
    "\n",
    "    cols.append(\"text\")\n",
    "\n",
    "    return tok(\n",
    "        \" \".join([example[c] for c in cols]),\n",
    "        padding=False,\n",
    "        truncation=False\n",
    "    )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=\"microsoft/deberta-v3-base\",\n",
    "        metadata={\"help\": \"Model name or path\"},\n",
    "    )\n",
    "\n",
    "    data_dir: Optional[str] = field(\n",
    "        default=\"/kaggle/input/commonlit-evaluate-student-summaries\",\n",
    "        metadata={\"help\": \"Data directory\"},\n",
    "    )\n",
    "\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=1600,\n",
    "        metadata={\"help\": \"Max sequence length\"},\n",
    "    )\n",
    "\n",
    "    add_prompt_question: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Add prompt question into input\"},\n",
    "    )\n",
    "\n",
    "    add_prompt_text: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Add prompt text into input\"},\n",
    "    )\n",
    "\n",
    "    fold: Optional[int] = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"Fold\"},\n",
    "    )\n",
    "\n",
    "    num_proc: Optional[int] = field(\n",
    "        default=4,\n",
    "        metadata={\"help\": \"Number of processes\"},\n",
    "    )\n",
    "\n",
    "    dropout: Optional[float] = field(\n",
    "        default=0.,\n",
    "        metadata={\"help\": \"Amount of dropout to apply\"},\n",
    "    )\n",
    "    max_position_embeddings: Optional[int] = field(\n",
    "        default=1600,\n",
    "        metadata={\"help\": \"Amount of dropout to apply\"},\n",
    "    )\n",
    "\n",
    "\n",
    "def tokenize(example, tokenizer, config):\n",
    "    sep = tokenizer.sep_token\n",
    "\n",
    "    # if config.add_prompt_question:\n",
    "    #     text = sep.join(\n",
    "    #         [example[\"prompt_question\"], example[\"prompt_text\"], example[\"text\"]]\n",
    "    #     )\n",
    "    # elif config.add_prompt_text:\n",
    "    #     text = sep.join([example[\"prompt_text\"], example[\"text\"]])\n",
    "    # else:\n",
    "    #     text = example[\"text\"]\n",
    "    prompt = sep.join([example[\"prompt_title\"], example[\"prompt_text\"], example[\"prompt_question\"]])\n",
    "    labels = [example[\"content\"], example[\"wording\"]]\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        example[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        max_length=config.max_seq_length,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        **tokenized,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "def tokenize_inf(example, tokenizer, config):\n",
    "    sep = tokenizer.sep_token;\n",
    "    prompt = sep.join([example[\"prompt_title\"], example[\"prompt_text\"], example[\"prompt_question\"]])\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        example[\"text\"],\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        max_length=config.max_seq_length,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        **tokenized\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8150bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_or_path = './output_fold3_seed42_1908'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "model_config = AutoConfig.from_pretrained(model_name_or_path)\n",
    "\n",
    "model_config.update({\n",
    "    \"hidden_dropout_prob\": 0,\n",
    "    \"attention_probs_dropout_prob\": 0,\n",
    "    \"num_labels\": 2,\n",
    "    \"problem_type\": \"regression\",\n",
    "    \"max_position_embeddings\":1600,\n",
    "})\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    pad_to_multiple_of=16,\n",
    ")\n",
    "# Do not use pretrained model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name_or_path, config=model_config\n",
    ")\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "432ae8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt_id', 'prompt_question', 'prompt_title', 'prompt_text', 'student_id', 'text'],\n",
       "    num_rows: 4\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74538329",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = Dataset.from_pandas(df_test)\n",
    "tokenized_test_ds = test_ds.map(tokenize_inf, batched=False, num_proc=4, fn_kwargs={\"tokenizer\": tokenizer, \"config\": config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06634ce3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "test_args = TrainingArguments(\n",
    "    output_dir = './',\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = 1,   \n",
    "    dataloader_drop_last = False,\n",
    "    eval_accumulation_steps=1,\n",
    ")\n",
    "\n",
    "# init trainer\n",
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args = test_args,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "\n",
    "test_results = trainer.predict(tokenized_test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f2cdc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['student_id'] = df_test['student_id']\n",
    "submission_df['content'] = test_results[0][:, 0]\n",
    "submission_df['wording'] = test_results[0][:, 1]\n",
    "submission_df.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dfdda1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
