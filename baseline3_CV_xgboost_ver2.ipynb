{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e60c2986",
   "metadata": {},
   "source": [
    "### Log\n",
    "* xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41344d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedGroupKFold\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import logging\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForWholeWordMask\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n",
    "from transformers.modeling_outputs  import BaseModelOutput,SequenceClassifierOutput\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from datasets import load_metric, disable_progress_bar\n",
    "import datasets\n",
    "# imports the torch_xla package\n",
    "import wandb\n",
    "from torch.nn.parameter import Parameter\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "from tqdm import tqdm\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import xgboost\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.ERROR);\n",
    "os.environ['TOKENIZER_PARALLELISM'] = 'false'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "disable_progress_bar()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66193e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spellchecker\n",
    "spellchecker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a90655b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23ff38",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95a371e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    pretraining = False\n",
    "    load_pretrained = False\n",
    "    input_path = './input/'\n",
    "    input_type = '2'\n",
    "    model_path = 'microsoft/deberta-v3-large' #  nghuyong/ernie-2.0-large-en studio-ousia/luke-large\n",
    "    model_type = 'custom'\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5  # 1.5\n",
    "    num_warmup_steps = 0\n",
    "    max_input_length = 1600\n",
    "    max_position_embeddings = 1600\n",
    "    n_folds = 4\n",
    "    folds = [1]\n",
    "    epochs = 4  # 5\n",
    "    # layer - wise larning rate \n",
    "    discriminative_learning_rate = False\n",
    "    discriminative_learning_rate_num_groups = 1\n",
    "    discriminative_learning_rate_decay_rate = 0.99\n",
    "    # reinint layer\n",
    "    reinit_layers = 0\n",
    "    \n",
    "#     encoder_lr = 5e-6\n",
    "#     head_lr = 5e-6\n",
    "    encoder_lr = 20e-6\n",
    "    head_lr = 10e-5\n",
    "    \n",
    "    min_lr = 1e-7\n",
    "    eps = 1e-7\n",
    "    betas = (0.9, 0.999)\n",
    "    weight_decay = 0\n",
    "    dropout = 0\n",
    "    num_fold = 5\n",
    "    batch_size = 8\n",
    "    seed = 42\n",
    "    OUTPUT_DIR = './pretrain/'\n",
    "    num_workers = 2\n",
    "    device='cuda'\n",
    "    print_freq = 100\n",
    "    run_deberta_model = True;\n",
    "\n",
    "    \n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=\"microsoft/deberta-v3-base\",\n",
    "        metadata={\"help\": \"Model name or path\"},\n",
    "    )\n",
    "\n",
    "    data_dir: Optional[str] = field(\n",
    "        default=\"/kaggle/input/commonlit-evaluate-student-summaries\",\n",
    "        metadata={\"help\": \"Data directory\"},\n",
    "    )\n",
    "\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=CFG.max_input_length,\n",
    "        metadata={\"help\": \"Max sequence length\"},\n",
    "    )\n",
    "\n",
    "    add_prompt_question: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Add prompt question into input\"},\n",
    "    )\n",
    "\n",
    "    add_prompt_text: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Add prompt text into input\"},\n",
    "    )\n",
    "\n",
    "    fold: Optional[int] = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"Fold\"},\n",
    "    )\n",
    "\n",
    "    num_proc: Optional[int] = field(\n",
    "        default=4,\n",
    "        metadata={\"help\": \"Number of processes\"},\n",
    "    )\n",
    "\n",
    "    dropout: Optional[float] = field(\n",
    "        default=0.,\n",
    "        metadata={\"help\": \"Amount of dropout to apply\"},\n",
    "    )\n",
    "    max_position_embeddings: Optional[int] = field(\n",
    "        default=CFG.max_input_length,\n",
    "        metadata={\"help\": \"Amount of dropout to apply\"},\n",
    "    )\n",
    "\n",
    "\n",
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab9c06b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append('/root/workspace/commonlit/nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fbd085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(CFG.model_path);\n",
    "        self.STOP_WORDS = set(stopwords.words('english'));\n",
    "        \n",
    "        self.spacy_ner_model = spacy.load('en_core_web_sm', )\n",
    "        self.speller = SpellChecker()\n",
    "        \n",
    "    def count_text_length(self, df: pd.DataFrame, col:str) -> pd.Series:\n",
    "        \"\"\" text length \"\"\"\n",
    "        tokenizer=self.tokenizer\n",
    "        return df[col].progress_apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "    def word_overlap_count(self, row):\n",
    "        \"\"\" intersection(prompt_text, text) \"\"\"        \n",
    "        def check_is_stop_word(word):\n",
    "            return word in self.STOP_WORDS\n",
    "        \n",
    "        prompt_words = row['prompt_tokens']\n",
    "        summary_words = row['summary_tokens']\n",
    "        if self.STOP_WORDS:\n",
    "            prompt_words = list(filter(check_is_stop_word, prompt_words))\n",
    "            summary_words = list(filter(check_is_stop_word, summary_words))\n",
    "        return len(set(prompt_words).intersection(set(summary_words)))\n",
    "            \n",
    "    def ngrams(self, token, n):\n",
    "        # Use the zip function to help us generate n-grams\n",
    "        # Concatentate the tokens into ngrams and return\n",
    "        ngrams = zip(*[token[i:] for i in range(n)])\n",
    "        return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "    def ngram_co_occurrence(self, row, n: int):\n",
    "        # Tokenize the original text and summary into words\n",
    "        original_tokens = row['prompt_tokens']\n",
    "        summary_tokens = row['summary_tokens']\n",
    "\n",
    "        # Generate n-grams for the original text and summary\n",
    "        original_ngrams = set(self.ngrams(original_tokens, n))\n",
    "        summary_ngrams = set(self.ngrams(summary_tokens, n))\n",
    "\n",
    "        # Calculate the number of common n-grams\n",
    "        common_ngrams = original_ngrams.intersection(summary_ngrams)\n",
    "\n",
    "        # # Optionally, you can get the frequency of common n-grams for a more nuanced analysis\n",
    "        # original_ngram_freq = Counter(ngrams(original_words, n))\n",
    "        # summary_ngram_freq = Counter(ngrams(summary_words, n))\n",
    "        # common_ngram_freq = {ngram: min(original_ngram_freq[ngram], summary_ngram_freq[ngram]) for ngram in common_ngrams}\n",
    "\n",
    "        return len(common_ngrams)\n",
    "    \n",
    "    def ner_overlap_count(self, row, mode:str):\n",
    "        model = self.spacy_ner_model\n",
    "        def clean_ners(ner_list):\n",
    "            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n",
    "        prompt = model(row['prompt_text'])\n",
    "        summary = model(row['text'])\n",
    "\n",
    "        if \"spacy\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n",
    "        elif \"stanza\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.type) for token in summary.ents])\n",
    "        else:\n",
    "            raise Exception(\"Model not supported\")\n",
    "\n",
    "        prompt_ner = clean_ners(prompt_ner)\n",
    "        summary_ner = clean_ners(summary_ner)\n",
    "\n",
    "        intersecting_ners = prompt_ner.intersection(summary_ner)\n",
    "        \n",
    "        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            return ner_dict\n",
    "        elif mode == \"test\":\n",
    "            return {key: ner_dict.get(key) for key in self.ner_keys}\n",
    "\n",
    "    \n",
    "    def quotes_count(self, row):\n",
    "        summary = row['text']\n",
    "        text = row['prompt_text']\n",
    "        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n",
    "        if len(quotes_from_summary)>0:\n",
    "            return [quote in text for quote in quotes_from_summary].count(True)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def spelling(self, text):\n",
    "        \n",
    "        wordlist=text.split()\n",
    "        amount_miss = len(list(self.speller.unknown(wordlist)))\n",
    "\n",
    "        return amount_miss\n",
    "    \n",
    "    def run(self, \n",
    "            prompts: pd.DataFrame,\n",
    "            summaries:pd.DataFrame,\n",
    "            mode:str\n",
    "        ) -> pd.DataFrame:\n",
    "        \n",
    "        # before merge preprocess\n",
    "        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n",
    "            lambda x: len(self.tokenizer.encode(x))\n",
    "        )\n",
    "        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n",
    "            lambda x: self.tokenizer.convert_ids_to_tokens(\n",
    "                self.tokenizer.encode(x), \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n",
    "            lambda x: len(self.tokenizer.encode(x))\n",
    "        )\n",
    "        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n",
    "            lambda x: self.tokenizer.convert_ids_to_tokens(\n",
    "                self.tokenizer.encode(x), \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "        )\n",
    "        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n",
    "\n",
    "        # merge prompts and summaries\n",
    "        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n",
    "\n",
    "        # after merge preprocess\n",
    "        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n",
    "        \n",
    "        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n",
    "        input_df['bigram_overlap_count'] = input_df.progress_apply(\n",
    "            self.ngram_co_occurrence,args=(2,), axis=1 \n",
    "        )\n",
    "        input_df['trigram_overlap_count'] = input_df.progress_apply(\n",
    "            self.ngram_co_occurrence, args=(3,), axis=1\n",
    "        )\n",
    "        \n",
    "        # Crate dataframe with count of each category NERs overlap for all the summaries\n",
    "        # Because it spends too much time for this feature, I don't use this time.\n",
    "#         ners_count_df  = input_df.progress_apply(\n",
    "#             lambda row: pd.Series(self.ner_overlap_count(row, mode=mode), dtype='float64'), axis=1\n",
    "#         ).fillna(0)\n",
    "#         self.ner_keys = ners_count_df.columns\n",
    "#         ners_count_df['sum'] = ners_count_df.sum(axis=1)\n",
    "#         ners_count_df.columns = ['NER_' + col for col in ners_count_df.columns]\n",
    "#         # join ner count dataframe with train dataframe\n",
    "#         input_df = pd.concat([input_df, ners_count_df], axis=1)\n",
    "        \n",
    "        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n",
    "        \n",
    "        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n",
    "    \n",
    "preprocessor = Preprocessor()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2185acce",
   "metadata": {},
   "source": [
    "### Read data and Create Folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0939091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7165/7165 [00:01<00:00, 5819.07it/s]\n",
      "100%|██████████| 7165/7165 [00:01<00:00, 6197.45it/s]\n",
      "100%|██████████| 7165/7165 [00:02<00:00, 2927.61it/s]\n",
      "100%|██████████| 7165/7165 [00:02<00:00, 2543.15it/s]\n",
      "100%|██████████| 7165/7165 [00:00<00:00, 63584.35it/s]\n"
     ]
    }
   ],
   "source": [
    "pdf = pd.read_csv(f\"{CFG.input_path}/prompts_train.csv\")\n",
    "sdf = pd.read_csv(f\"{CFG.input_path}/summaries_train.csv\")\n",
    "\n",
    "#df = pdf.merge(sdf, on=\"prompt_id\")\n",
    "\n",
    "#train = preprocessor.run(pdf, sdf, mode=\"train\")\n",
    "train = preprocessor.run(pdf, sdf, mode='test')\n",
    "\n",
    "\n",
    "# 4 prompt ids, 4 folds\n",
    "id2fold = {\n",
    "    \"814d6b\": 0,\n",
    "    \"39c16e\": 1,\n",
    "    \"3b9047\": 2,\n",
    "    \"ebad26\": 3,\n",
    "}\n",
    "\n",
    "train[\"fold\"] = train[\"prompt_id\"].map(id2fold)\n",
    "#test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n",
    "\n",
    "#test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976532ab",
   "metadata": {},
   "source": [
    "## Deberta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8af51ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_inf(example, tokenizer, config):\n",
    "    sep = tokenizer.sep_token;\n",
    "    prompt = sep.join([example[\"prompt_title\"], example[\"prompt_text\"], example[\"prompt_question\"]])\n",
    "    tokenized = tokenizer(\n",
    "        example[\"text\"],\n",
    "        prompt,\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=config.max_seq_length,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        **tokenized\n",
    "    }\n",
    "\n",
    "model_paths = [\n",
    "    '/root/autodl-tmp/output_fold0_seed42_2609',\n",
    "    '/root/autodl-tmp/output_fold1_seed42_2609',\n",
    "    '/root/autodl-tmp/output_fold2_seed42_2609',\n",
    "    '/root/autodl-tmp/output_fold3_seed42_2609'\n",
    "]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_paths[0]);\n",
    "model_config = AutoConfig.from_pretrained(model_paths[0]);\n",
    "model_config.update({\n",
    "    'hidden_dropout_prob': 0,\n",
    "    'attention_probs_dropout_prob': 0,\n",
    "    'num_labels':2,\n",
    "    'problem_type': 'regression',\n",
    "    'max_position_embeddings': 1600,\n",
    "});\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer = tokenizer,\n",
    "    pad_to_multiple_of=16,\n",
    ")\n",
    "\n",
    "# Do not use pretrained model\n",
    "deberta_models = []\n",
    "for model_path in model_paths:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, config = model_config);\n",
    "    deberta_models.append(model)\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df2f27",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca43e0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3a3ea602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run deberta model for fold 2\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 85\u001b[0m\n\u001b[1;32m     77\u001b[0m y_eval_cv \u001b[38;5;241m=\u001b[39m train[train[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfold\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m fold][target]\n\u001b[1;32m     79\u001b[0m xgboost_model \u001b[38;5;241m=\u001b[39m XGBRegressor(\n\u001b[1;32m     80\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, \n\u001b[1;32m     81\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m7\u001b[39m, \n\u001b[1;32m     82\u001b[0m     eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, \n\u001b[1;32m     83\u001b[0m     subsample\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m, \n\u001b[1;32m     84\u001b[0m     colsample_bytree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m xgboost_model\u001b[38;5;241m.\u001b[39mfit(X_train_cv\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mdrop_columns)\u001b[38;5;241m.\u001b[39mvalues,\u001b[43my_train_cv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)) )\n\u001b[1;32m     86\u001b[0m xgboost_pair[target] \u001b[38;5;241m=\u001b[39m xgboost_model\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m## save model\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/pandas/core/generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5983\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5984\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5985\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5986\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5987\u001b[0m ):\n\u001b[1;32m   5988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5989\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments \n",
    "\n",
    "\n",
    "targets = [\"content\", \"wording\"]\n",
    "drop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \n",
    "                \"prompt_question\", \"prompt_title\", \n",
    "                \"prompt_text\"\n",
    "               ] + targets\n",
    "\n",
    "lgb_models = []\n",
    "\n",
    "for fold in range(2, CFG.n_folds):\n",
    "    \n",
    "    X_train_cv = train[train[\"fold\"] != fold]\n",
    "    X_eval_cv = train[train[\"fold\"] == fold]\n",
    "    \n",
    "    ## Create dataset for deberta\n",
    "    X_train_ds = datasets.Dataset.from_pandas(X_train_cv);\n",
    "    tokenized_X_train_ds = X_train_ds.map(tokenize_inf, \n",
    "                                          batched = False, \n",
    "                                          num_proc = 4,\n",
    "                                          fn_kwargs= {'tokenizer': tokenizer, 'config': config}\n",
    "                                         );\n",
    "    X_eval_ds = datasets.Dataset.from_pandas(X_eval_cv);\n",
    "    tokenized_X_eval_ds = X_eval_ds.map(tokenize_inf,\n",
    "                                       batched = False,\n",
    "                                       num_proc = 4,\n",
    "                                       fn_kwargs = {'tokenizer': tokenizer, 'config': config}\n",
    "                                       );\n",
    "    \n",
    "    \n",
    "    ## Run deberta model\n",
    "    if CFG.run_deberta_model:\n",
    "        deberta = deberta_models[fold]\n",
    "\n",
    "        infer_args = TrainingArguments(\n",
    "            output_dir = './',\n",
    "            do_train = False,\n",
    "            do_predict = True,\n",
    "            per_device_eval_batch_size = 16,   \n",
    "            dataloader_drop_last = False,\n",
    "            eval_accumulation_steps=1,\n",
    "        );\n",
    "\n",
    "        # Init deberta predictor\n",
    "        trainer = Trainer(\n",
    "            model = deberta,\n",
    "            args = infer_args,\n",
    "            data_collator = data_collator,\n",
    "            tokenizer = tokenizer\n",
    "        )\n",
    "        print(f'Run deberta model for fold {fold}')\n",
    "        deberta_res_train = trainer.predict(tokenized_X_train_ds)[0]\n",
    "        deberta_res_eval = trainer.predict(tokenized_X_eval_ds)[0]\n",
    "\n",
    "        X_train_cv['pred_content_score'] = deberta_res_train[:,0]\n",
    "        X_train_cv['pred_wording_score'] = deberta_res_train[:,1]\n",
    "    \n",
    "        X_eval_cv['pred_content_score'] = deberta_res_eval[:, 0]\n",
    "        X_eval_cv['pred_wording_score'] = deberta_res_eval[:, 1]\n",
    "        \n",
    "        ## Output the files\n",
    "        X_train_cv.to_csv(f'./boost_input/X_train_cv_{fold}.csv', index = False);\n",
    "        X_eval_cv.to_csv(f'./boost_input/X_eval_cv_{fold}.csv', index = False);\n",
    "    else:\n",
    "        X_train_cv = pd.read_csv(f'./boost_input/X_train_cv_{fold}.csv')\n",
    "        X_eval_cv = pd.read_csv(f'./boost_input/X_eval_cv_{fold}.csv');\n",
    "    \n",
    "    \n",
    "    ## Train xgboost\n",
    "    evaluation_results = {}\n",
    "    eval_labels = {}\n",
    "    eval_preds = {}\n",
    "    xgboost_pair = {}\n",
    "    for target in targets:\n",
    "        y_train_cv = train[train[\"fold\"] != fold][target]\n",
    "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "        \n",
    "        xgboost_model = XGBRegressor(\n",
    "            n_estimators=10000, \n",
    "            max_depth=64, \n",
    "            eta=0.1, \n",
    "            subsample=0.9, \n",
    "            colsample_bytree=0.8)\n",
    "        xgboost_model.fit(\n",
    "            X_train_cv.drop(columns=drop_columns).values,\n",
    "            y_train_cv.values.reshape((-1, 1)) \n",
    "        )\n",
    "        \n",
    "        xgboost_pair[target] = xgboost_model\n",
    "        ## save model\n",
    "        xgboost_model.save_model(f'./boost_input/xgboost_{target}_{fold}.txt')\n",
    "        eval_preds[target] = xgboost_model.predict(X_eval_cv.drop(columns=drop_columns).values)\n",
    "        eval_labels[target] = y_eval_cv\n",
    "    ## Get compute_mcrmse\n",
    "    preds_arr = np.concatenate([\n",
    "        eval_preds['content'].reshape(-1, 1),\n",
    "        eval_preds['wording'].reshape(-1, 1)],\n",
    "        axis = 1\n",
    "    )\n",
    "    preds_arr_deberta = np.concatenate([\n",
    "        X_eval_cv[:, 'pred_content_score'].reshape(-1, 1),\n",
    "        X_eval_cv[:, 'pred_wording_score'].reshape(-1, 1)\n",
    "    ], axis = 1)\n",
    "    \n",
    "    labels_arr = np.concatenate([\n",
    "        train.loc[train[\"fold\"] == fold, 'content'].values.reshape(-1, 1),\n",
    "        train.loc[train['fold'] == fold, 'wording'].values.reshape(-1, 1)],\n",
    "        axis = 1\n",
    "    )\n",
    "    \n",
    "    print(f'deberta compute_mcrmse: ')\n",
    "    print(compute_mcrmse([preds_arr_deberta, labels_arr]))\n",
    "    print(f'Print compute_mcrmse for fold {fold}')\n",
    "    print(compute_mcrmse([preds_arr, labels_arr]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "027b802f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2009, 1)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_eval_cv.loc[:, 'pred_content_score'].values.reshape(-1, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "248558ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary_length</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>length_ratio</th>\n",
       "      <th>word_overlap_count</th>\n",
       "      <th>bigram_overlap_count</th>\n",
       "      <th>trigram_overlap_count</th>\n",
       "      <th>quotes_count</th>\n",
       "      <th>pred_content_score</th>\n",
       "      <th>pred_wording_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69</td>\n",
       "      <td>5</td>\n",
       "      <td>671</td>\n",
       "      <td>0.102832</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.011516</td>\n",
       "      <td>0.786554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56</td>\n",
       "      <td>2</td>\n",
       "      <td>1137</td>\n",
       "      <td>0.049252</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.353386</td>\n",
       "      <td>0.307327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>253</td>\n",
       "      <td>29</td>\n",
       "      <td>671</td>\n",
       "      <td>0.377049</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>2.434699</td>\n",
       "      <td>2.672835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   summary_length  splling_err_num  prompt_length  length_ratio  \\\n",
       "0              69                5            671      0.102832   \n",
       "1              56                2           1137      0.049252   \n",
       "4             253               29            671      0.377049   \n",
       "\n",
       "   word_overlap_count  bigram_overlap_count  trigram_overlap_count  \\\n",
       "0                   0                     5                      0   \n",
       "1                   0                    22                     10   \n",
       "4                   1                    27                      5   \n",
       "\n",
       "   quotes_count  pred_content_score  pred_wording_score  \n",
       "0             0            0.011516            0.786554  \n",
       "1             0           -0.353386            0.307327  \n",
       "4             4            2.434699            2.672835  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv.drop(columns=drop_columns).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "64b2a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "del xgboost_model, eval_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ba542d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deberta compute_mcrmse: \n",
      "{'content_rmse': 0.43378799291648845, 'wording_rmse': 0.5737806059579986, 'mcrmse': 0.5037842994372436}\n",
      "Print compute_mcrmse for fold 2\n",
      "{'content_rmse': 0.46537228830234983, 'wording_rmse': 0.6152083574774982, 'mcrmse': 0.5402903228899241}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_labels = {}\n",
    "eval_preds = {}\n",
    "xgboost_pair = {}\n",
    "for target in targets:\n",
    "    y_train_cv = train[train[\"fold\"] != fold][target]\n",
    "    y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "\n",
    "    xgboost_model = XGBRegressor(\n",
    "            n_estimators=1000, \n",
    "            max_depth=7, \n",
    "            eta=0.001, \n",
    "            subsample=0.7, \n",
    "            colsample_bytree=0.8)\n",
    "    xgboost_model.fit(\n",
    "        X_train_cv.drop(columns=drop_columns).values,\n",
    "        y_train_cv.values.reshape((-1, 1)) \n",
    "    )\n",
    "\n",
    "    xgboost_pair[target] = xgboost_model\n",
    "    ## save model\n",
    "    xgboost_model.save_model(f'./boost_input/xgboost_{target}_{fold}.txt')\n",
    "    eval_preds[target] = xgboost_model.predict(X_eval_cv.drop(columns=drop_columns).values)\n",
    "    eval_labels[target] = y_eval_cv\n",
    "## Get compute_mcrmse\n",
    "preds_arr = np.concatenate([\n",
    "    eval_preds['content'].reshape(-1, 1),\n",
    "    eval_preds['wording'].reshape(-1, 1)],\n",
    "    axis = 1\n",
    ")\n",
    "preds_arr_deberta = np.concatenate([\n",
    "    X_eval_cv.loc[:, 'pred_content_score'].values.reshape(-1, 1),\n",
    "    X_eval_cv.loc[:, 'pred_wording_score'].values.reshape(-1, 1)\n",
    "], axis = 1)\n",
    "\n",
    "labels_arr = np.concatenate([\n",
    "    train.loc[train[\"fold\"] == fold, 'content'].values.reshape(-1, 1),\n",
    "    train.loc[train['fold'] == fold, 'wording'].values.reshape(-1, 1)],\n",
    "    axis = 1\n",
    ")\n",
    "\n",
    "print(f'deberta compute_mcrmse: ')\n",
    "print(compute_mcrmse([preds_arr_deberta, labels_arr]))\n",
    "print(f'Print compute_mcrmse for fold {fold}')\n",
    "print(compute_mcrmse([preds_arr, labels_arr]))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac49be35",
   "metadata": {},
   "source": [
    "## Obsolete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1fb10e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_labels_dict = {}\n",
    "for target in targets:\n",
    "    evaluation_results[target] = lgb_model_pair[target].predict(X_eval_cv.drop(columns=drop_columns))\n",
    "    eval_labels_dict[target] = train[train[\"fold\"] == fold][target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63a9cdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_preds = np.concatenate([\n",
    "    evaluation_results['content'].reshape(-1, 1),\n",
    "    evaluation_results['wording'].reshape(-1, 1)],\n",
    "    axis = 1)\n",
    "eval_labels = np.concatenate([\n",
    "    train.loc[train[\"fold\"] == fold, 'content'].values.reshape(-1, 1),\n",
    "    train.loc[train['fold'] == fold, 'wording'].values.reshape(-1, 1)],\n",
    "    axis = 1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e76eada9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content_rmse': 0.41287069842384666,\n",
       " 'wording_rmse': 0.5018779304633396,\n",
       " 'mcrmse': 0.4573743144435931}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_mcrmse([eval_preds, eval_labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4bc39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_ds = datasets.Dataset.from_pandas(train)\n",
    "# tokenized_train_ds = train_ds.map(tokenize_inf, batched = False, num_proc = 4, fn_kwargs= {'tokenizer': tokenizer, 'config': config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cd13115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "infer_args = TrainingArguments(\n",
    "    output_dir = './',\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = 16,   \n",
    "    dataloader_drop_last = False,\n",
    "    eval_accumulation_steps=1,\n",
    ")\n",
    "\n",
    "# init trainer\n",
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args = infer_args,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "\n",
    "deberta_res = []\n",
    "for model in models:\n",
    "    deberta_res.append(trainer.predict(tokenized_train_ds)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64a431a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deberta_res_mean = np.mean(np.array(deberta_res), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9c65081",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['pred_content_score'] = deberta_res_mean[:,0]\n",
    "train['pred_wording_score'] = deberta_res_mean[:,1]\n",
    "#train.to_csv('./train_2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d470ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_csv('./train_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0c3aaa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content_rmse': 0.3729688675080504,\n",
       " 'wording_rmse': 0.4903757307947375,\n",
       " 'mcrmse': 0.43167229915139393}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }\n",
    "\n",
    "\n",
    "mcrmse = compute_mcrmse((train.loc[:, ['pred_content_score', 'pred_wording_score']].values, \n",
    "               train.loc[:, ['content', 'wording']].values))\n",
    "mcrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0afd2d20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred_content_score</th>\n",
       "      <th>pred_wording_score</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2839</th>\n",
       "      <td>-0.305602</td>\n",
       "      <td>0.348631</td>\n",
       "      <td>-0.093814</td>\n",
       "      <td>0.503833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1768</th>\n",
       "      <td>-0.425709</td>\n",
       "      <td>-0.498831</td>\n",
       "      <td>-0.627647</td>\n",
       "      <td>-0.125597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4296</th>\n",
       "      <td>-0.335351</td>\n",
       "      <td>-0.187537</td>\n",
       "      <td>-0.296443</td>\n",
       "      <td>-1.133453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5288</th>\n",
       "      <td>1.678848</td>\n",
       "      <td>1.327419</td>\n",
       "      <td>0.686362</td>\n",
       "      <td>0.704364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5314</th>\n",
       "      <td>-0.706576</td>\n",
       "      <td>-0.307890</td>\n",
       "      <td>-0.776512</td>\n",
       "      <td>-0.383759</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      pred_content_score  pred_wording_score   content   wording\n",
       "2839           -0.305602            0.348631 -0.093814  0.503833\n",
       "1768           -0.425709           -0.498831 -0.627647 -0.125597\n",
       "4296           -0.335351           -0.187537 -0.296443 -1.133453\n",
       "5288            1.678848            1.327419  0.686362  0.704364\n",
       "5314           -0.706576           -0.307890 -0.776512 -0.383759"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.loc[:, ['pred_content_score', 'pred_wording_score', 'content', 'wording']].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b099a2",
   "metadata": {},
   "source": [
    "### Lightgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e54e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"content\", \"wording\"]\n",
    "\n",
    "drop_columns = [\"fold\", \"student_id\", \"prompt_id\", \"text\", \n",
    "                \"prompt_question\", \"prompt_title\", \n",
    "                \"prompt_text\"\n",
    "               ] + targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e5c9ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1347\n",
      "[LightGBM] [Info] Number of data points in the train set: 5169, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.013356\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttrain's rmse: 0.413015\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000403 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1347\n",
      "[LightGBM] [Info] Number of data points in the train set: 5169, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 0.028040\n",
      "Training until validation scores don't improve for 30 rounds\n",
      "Early stopping, best iteration is:\n",
      "[38]\ttrain's rmse: 0.503381\n"
     ]
    }
   ],
   "source": [
    "lgb_model_dict = {}\n",
    "\n",
    "for target in targets:\n",
    "    lgb_models = []\n",
    "    \n",
    "    #for fold in range(CFG.n_folds):\n",
    "    for fold in range(3,4):\n",
    "\n",
    "        X_train_cv = train[train[\"fold\"] != fold].drop(columns=drop_columns)\n",
    "        y_train_cv = train[train[\"fold\"] != fold][target]\n",
    "\n",
    "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "\n",
    "        dtrain = lgb.Dataset(X_train_cv, label=y_train_cv)\n",
    "        dval = lgb.Dataset(X_eval_cv, label=y_eval_cv)\n",
    "\n",
    "        params = {\n",
    "                  'boosting_type': 'gbdt',\n",
    "                  'random_state': 42,\n",
    "                  'objective': 'regression',\n",
    "                  'metric': 'rmse',\n",
    "                  'learning_rate': 0.05,\n",
    "                  }\n",
    "\n",
    "        evaluation_results = {}\n",
    "        lgb_model = lgb.train(params,\n",
    "                          num_boost_round=10000,\n",
    "                            #categorical_feature = categorical_features,\n",
    "                          valid_names=['train', 'valid'],\n",
    "                          train_set=dtrain,\n",
    "                          valid_sets=dval,\n",
    "                          callbacks=[\n",
    "                              lgb.early_stopping(stopping_rounds=30, verbose=True),\n",
    "                               lgb.log_evaluation(100),\n",
    "                              lgb.callback.record_evaluation(evaluation_results)\n",
    "                            ],\n",
    "                          )\n",
    "        lgb_model.save_model(f'./lgbt_{target}_{fold}.txt')\n",
    "        lgb_models.append(lgb_model)\n",
    "    \n",
    "    lgb_model_dict[target] = lgb_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f7a5a7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lgb_model_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mlgb_model_dict\u001b[49m[target]\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./lgbt_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lgb_model_dict' is not defined"
     ]
    }
   ],
   "source": [
    "for target in targets:\n",
    "    lgb_model_dict[target].save_model(f'./lgbt_{target}_{idx}.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3940b285",
   "metadata": {},
   "source": [
    "## CV score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "73972797",
   "metadata": {},
   "outputs": [],
   "source": [
    "### test load\n",
    "model_targets = ['content', 'wording']\n",
    "\n",
    "lgb_model_dict = {}\n",
    "for model_target in model_targets:\n",
    "    lgb_model_dict[model_target] = [];\n",
    "    for idx in range(4):\n",
    "        lgb_model_dict[model_target].append(lgb.Booster(model_file = f'./lgbt_{model_target}_{idx}.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2138ad65",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_content = lgb_model_dict['content'][0].predict(train[train['fold'] == 3].drop(columns = drop_columns))\n",
    "wording_content = lgb_model_dict['wording'][0].predict(train[train['fold'] == 3].drop(columns = drop_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2eb380d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content_rmse': 0.4130148003572234,\n",
       " 'wording_rmse': 0.5033809768388605,\n",
       " 'mcrmse': 0.45819788859804195}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_mcrmse(\n",
    "    (np.concatenate([preds_content.reshape(-1, 1), wording_content.reshape(-1, 1)], axis = 1),\n",
    "    train.loc[train['fold'] == 3, ['content', 'wording']])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd2d43d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content_rmse : 0.37573650633040434\n",
      "wording_rmse : 0.4850980756412646\n",
      "mcrmse : 0.4304172909858345\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "# cv\n",
    "rmses = []\n",
    "\n",
    "for target in targets:\n",
    "    lgb_models = lgb_model_dict[target]\n",
    "\n",
    "    preds = []\n",
    "    trues = []\n",
    "    \n",
    "    for fold, lgb_model in enumerate(lgb_models):\n",
    "        X_eval_cv = train[train[\"fold\"] == fold].drop(columns=drop_columns)\n",
    "        y_eval_cv = train[train[\"fold\"] == fold][target]\n",
    "\n",
    "        pred = lgb_model.predict(X_eval_cv)\n",
    "\n",
    "        trues.extend(y_eval_cv)\n",
    "        preds.extend(pred)\n",
    "        \n",
    "    rmse = np.sqrt(mean_squared_error(trues, preds))\n",
    "    print(f\"{target}_rmse : {rmse}\")\n",
    "    rmses = rmses + [rmse]\n",
    "\n",
    "print(f\"mcrmse : {sum(rmses) / len(rmses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b408f1e2",
   "metadata": {},
   "source": [
    "### Check mcrmse on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "562212ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model_content_0 = lgb_model_dict['content'][0]\n",
    "lgb_model_wording_0 = lgb_model_dict['wording'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "646f5939",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.09165974,  0.89087325],\n",
       "       [-0.7616748 , -0.50564852],\n",
       "       [ 2.68807138,  2.58113256],\n",
       "       ...,\n",
       "       [-1.07972116, -0.60669181],\n",
       "       [-0.09735215,  0.38939329],\n",
       "       [ 1.06160919,  0.55252552]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_content_res = lgb_model_content_0.predict(train.drop(columns = drop_columns)).reshape(-1,1)\n",
    "train_wording_res = lgb_model_wording_0.predict(train.drop(columns = drop_columns)).reshape(-1,1)\n",
    "preds = np.concatenate([train_content_res, train_wording_res], axis = 1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c18b991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content_rmse': 0.32244564104233536,\n",
       " 'wording_rmse': 0.42940207314241746,\n",
       " 'mcrmse': 0.3759238570923764}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = train.loc[:, ['content', 'wording']]\n",
    "compute_mcrmse([preds, labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f949d3",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec7bd1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "for target in targets:\n",
    "    idx = 0;\n",
    "    for lgb_model in lgb_model_dict[target]:\n",
    "        lgb_model.save_model(f'./lgbt_{target}_{idx}.txt')\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75f28639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<lightgbm.basic.Booster at 0x7fce9aa76190>,\n",
       " <lightgbm.basic.Booster at 0x7fce9ab0f100>,\n",
       " <lightgbm.basic.Booster at 0x7fce93e18250>,\n",
       " <lightgbm.basic.Booster at 0x7fce93e18d90>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_model_dict[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1372adda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
