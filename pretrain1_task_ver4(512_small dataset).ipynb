{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d62c666",
   "metadata": {},
   "source": [
    "### Pretrain task\n",
    "\n",
    "**MLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ca14e",
   "metadata": {},
   "source": [
    "#### Read Feedback Prize - Evaluating Student Writing data\n",
    "\n",
    "#### Log\n",
    "\n",
    "* 09.01: do not join the text together, use short text to do MLM task, and reduce max_seq from 2048 to 1024, do not use Feedback Prize dataset\n",
    "* 09.10: do not join the text together, reduce max_len from 1024 to 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c1eec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "data_files = os.listdir('./input/pretrain/train')\n",
    "train_text = []\n",
    "for data_file in data_files:\n",
    "    with open('./input/pretrain/train/'+ data_file, 'r') as reader:\n",
    "        # join all text in a file together\n",
    "        #train_text += [' '.join(re.split(r'\\n+', reader.read()))]\n",
    "        # do not join the split text segments, use shorter texts for pretraining\n",
    "        train_text +=re.split(r'\\n+', reader.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7971ae5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86707"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efceda2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A very appropriate setting for the Facial Action Coding System would be online schooling and home schooling because both of those are mainly based on the one student taking that course or class. If the camera on the computer sent a signal to the online schooler or home schooler, it would be easy to make a change in the lesson to try and get the student more engaged. Having a student pay more attention and be more engaged in a lesson could actually help raise test scores because if someone was doing something that was actually fun and they could actually retain the knowledge, it might be effective.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "train_text[random.randint(0,len(train_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c79c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text[random.randint(0,len(train_text))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f62e99",
   "metadata": {},
   "source": [
    "#### Read Feedback Prize - English Language Learning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0a376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pretrain_df = pd.read_csv('./input/pretrain/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2929b142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Line 1', 'Line 3', 'Line 4']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(bool, 'Line 1\\n\\nLine 3\\rLine 4\\r\\n'.splitlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2534a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21661\n"
     ]
    }
   ],
   "source": [
    "train_lines = []\n",
    "\n",
    "for text in pretrain_df.full_text:\n",
    "    ## join all text in a file together\n",
    "    train_lines += list(filter(bool, text.splitlines()));\n",
    "    \n",
    "    ## do not join the split text segments, use shorter texts for pretraining\n",
    "    #train_lines += [' '.join(re.split(r'\\n+', text))]\n",
    "\n",
    "# print(len(train_lines))    \n",
    "# train_lines += train_text\n",
    "print(len(train_lines))\n",
    "train_lines = pd.DataFrame(train_lines, columns = ['train_lines'])\n",
    "\n",
    "train_lines.to_csv('./input/pretrain/train_MLM_ver3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55dae640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"But this happen also when the students have a good communication and they are friendly. However sometimes can be difficult even the sudents don't try to study with the group or only due the part of oneself.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines.iloc[random.randint(0,len(train_lines))].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96e255a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74f2b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42;\n",
    "    model_name = 'microsoft/deberta-v3-large'\n",
    "    epochs = 3;\n",
    "    batch_size = 8;\n",
    "    lr = 1e-6;\n",
    "    weight_decay = 1e-6\n",
    "    max_len = 512 # use max length as 2048 #01.09, use max_length as 1024\n",
    "    mask_prob = 0.15;\n",
    "    n_accumulate = 4\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da2f6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import os\n",
    "def set_seed(seed = CFG.seed):\n",
    "    np.random.seed(seed);\n",
    "    torch.manual_seed(seed);\n",
    "    torch.cuda.manual_seed(seed);\n",
    "    torch.backends.cudnn.deterministic = True;\n",
    "    torch.backends.cudnn.benchmark = True;\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f66fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/deberta-v3-large/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f017927f310>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 25048fde-ec1d-4be3-99ed-7c9a165fbdc5)')' thrown while requesting HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1423: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/deberta-v3-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f01721a9850>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 90c6929c-00ea-4ff5-9d7e-b22d4c1c80b2)')' thrown while requesting HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name);\n",
    "model = AutoModelWithLMHead.from_pretrained(CFG.model_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00a45f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     1,      2, 128000,      0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = tokenizer.encode_plus('[CLS] [SEP] [MASK] [PAD]',\n",
    "                                      add_special_tokens = False,\n",
    "                                      return_tensors='pt')\n",
    "special_tokens = torch.flatten(special_tokens['input_ids'])\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f9f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaskedLabels(input_ids):\n",
    "    rand = torch.rand(input_ids.shape);\n",
    "    mask_arr = (rand < CFG.mask_prob);\n",
    "    \n",
    "    for special_token in special_tokens:\n",
    "        token = special_token.item();\n",
    "        mask_arr *= (input_ids != token);\n",
    "    selection = torch.flatten(mask_arr[0].nonzero()).tolist()\n",
    "    input_ids[selection] = 128000\n",
    "    \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "341cc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset:\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data;\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data);\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        \n",
    "        tokenized_data = self.tokenizer.encode_plus(\n",
    "                            text,\n",
    "                            max_length = CFG.max_len,\n",
    "                            truncation = True,\n",
    "                            padding = 'max_length',\n",
    "                            add_special_tokens = True,\n",
    "                            return_tensors = 'pt'\n",
    "                        )\n",
    "        input_ids = torch.flatten(tokenized_data.input_ids);\n",
    "        attention_mask = torch.flatten(tokenized_data.input_ids);\n",
    "        labels = getMaskedLabels(input_ids)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10c9bd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21386"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lines.train_lines.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99f6d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MLMDataset(train_lines.train_lines.unique(), tokenizer)\n",
    "dataloader = DataLoader(train_data, batch_size = CFG.batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22f25b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21386, 2674)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f08abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = CFG.lr, weight_decay = CFG.weight_decay);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d79718e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, device):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    for batch_num, batch in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        batch_loss = loss / CFG.n_accumulate\n",
    "        batch_losses.append(batch_loss.item())\n",
    "    \n",
    "        loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "        loop.set_postfix(loss=batch_loss.item())\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        if batch_num % CFG.n_accumulate == 0 or batch_num == len(dataloader):\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "    return np.mean(batch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca03af20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Checkpointing: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2674/2674 [1:19:23<00:00,  1.78s/it, loss=0.871]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.5585583276327815\n",
      "New Best Loss inf -> 1.5586, Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2674/2674 [1:19:22<00:00,  1.78s/it, loss=0.75] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7744891889610661\n",
      "New Best Loss 1.5586 -> 0.7745, Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2674/2674 [1:19:22<00:00,  1.78s/it, loss=0.454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5360053671257836\n",
      "New Best Loss 0.7745 -> 0.5360, Saving Model\n"
     ]
    }
   ],
   "source": [
    "device = CFG.device\n",
    "model.to(device)\n",
    "history = []\n",
    "best_loss = np.inf\n",
    "prev_loss = np.inf\n",
    "model.gradient_checkpointing_enable()\n",
    "print(f\"Gradient Checkpointing: {model.is_gradient_checkpointing}\")\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    loss = train_loop(model, device)\n",
    "    history.append(loss)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    if loss < best_loss:\n",
    "        print(\"New Best Loss {:.4f} -> {:.4f}, Saving Model\".format(prev_loss, loss))\n",
    "        # torch.save(model.state_dict(), \"./deberta_mlm.pt\")\n",
    "        model.save_pretrained('./input/pretrain/pretrained_model_1009/')\n",
    "        best_loss = loss\n",
    "    prev_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Checkpointing: True\n",
    "Epoch 1: 100%|██████████| 4848/4848 [9:03:47<00:00,  6.73s/it, loss=0.753]  \n",
    "Loss: 1.3033109624615007\n",
    "New Best Loss inf -> 1.3033, Saving Model\n",
    "Epoch 2: 100%|██████████| 4848/4848 [9:03:44<00:00,  6.73s/it, loss=0.404]  \n",
    "Loss: 0.6164927809336299\n",
    "New Best Loss 1.3033 -> 0.6165, Saving Model\n",
    "Epoch 3: 100%|██████████| 4848/4848 [9:03:37<00:00,  6.73s/it, loss=0.359]  \n",
    "Loss: 0.46205416228314833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695f6b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0816e13",
   "metadata": {},
   "source": [
    "### Try to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d699139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoConfig\n",
    "# config = AutoConfig.from_pretrained('./input/pretrain/pretrained_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa751b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ./input/pretrain/pretrained_model/ and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForSequenceClassification\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('./input/pretrain/pretrained_model/', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff30cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
