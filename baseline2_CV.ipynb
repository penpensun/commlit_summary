{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AmhfxdCR86jB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedGroupKFold\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForWholeWordMask\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n",
    "from transformers.modeling_outputs  import BaseModelOutput,SequenceClassifierOutput\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "# imports the torch_xla package\n",
    "import wandb\n",
    "from torch.nn.parameter import Parameter\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1dMoHaCUn29I",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Model training')\n",
    "    # params of training\n",
    "    parser.add_argument(\n",
    "        \"--fold\", dest=\"fold\", help=\"Train fold\", default=None, type=int)\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        dest='batch_size',\n",
    "        help='Mini batch size of one gpu or cpu',\n",
    "        type=int,\n",
    "        default=None)\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6G2r5GS86jE"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Bojtddae86jG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    pretraining = False\n",
    "    load_pretrained = False\n",
    "    input_path = './input/'\n",
    "    input_type = '2'\n",
    "    model_path = 'microsoft/deberta-v3-large' #  nghuyong/ernie-2.0-large-en studio-ousia/luke-large\n",
    "    model_type = 'pool'\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5  # 1.5\n",
    "    num_warmup_steps = 0\n",
    "    max_input_length = 1024\n",
    "    max_position_embeddings = 1024\n",
    "    folds = [3]\n",
    "    epochs = 4  # 5\n",
    "    # layer - wise larning rate \n",
    "    discriminative_learning_rate = False\n",
    "    discriminative_learning_rate_num_groups = 1\n",
    "    discriminative_learning_rate_decay_rate = 0.99\n",
    "    # reinint layer\n",
    "    reinit_layers = 0\n",
    "    \n",
    "    encoder_lr = 5e-6\n",
    "    head_lr = 5e-6\n",
    "    min_lr = 1e-7\n",
    "    eps = 1e-7\n",
    "    betas = (0.9, 0.999)\n",
    "    weight_decay = 0\n",
    "    dropout = 0\n",
    "    num_fold = 5\n",
    "    batch_size = 4\n",
    "    seed = 42\n",
    "    OUTPUT_DIR = './pretrain/'\n",
    "    num_workers = 2\n",
    "    device='cuda'\n",
    "    print_freq = 100\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FaoPHcM86jH"
   },
   "source": [
    "## logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RWkPxbhq86jI",
    "outputId": "1f97e316-be01-408d-c79b-1b128aaff9c9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===============lr_5e-06===============\n",
      "===============seed_42===============\n",
      "===============total_epochs_4===============\n",
      "===============num_warmup_steps_0===============\n"
     ]
    }
   ],
   "source": [
    "def get_logger(filename=CFG.OUTPUT_DIR+ 'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    if not os.path.exists(CFG.OUTPUT_DIR):\n",
    "        os.makedirs(CFG.OUTPUT_DIR)\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "LOGGER.info('===============lr_{}==============='.format(CFG.encoder_lr))\n",
    "LOGGER.info('===============seed_{}==============='.format(CFG.seed))\n",
    "LOGGER.info('===============total_epochs_{}==============='.format(CFG.epochs))\n",
    "LOGGER.info('===============num_warmup_steps_{}==============='.format(CFG.num_warmup_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSqTonVY86jJ"
   },
   "source": [
    "# Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ACTTNzFL86jK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf = pd.read_csv(f\"{CFG.input_path}/prompts_train.csv\")\n",
    "sdf = pd.read_csv(f\"{CFG.input_path}/summaries_train.csv\")\n",
    "\n",
    "df = pdf.merge(sdf, on=\"prompt_id\")\n",
    "\n",
    "# 4 prompt ids, 4 folds\n",
    "id2fold = {\n",
    "    \"814d6b\": 0,\n",
    "    \"39c16e\": 1,\n",
    "    \"3b9047\": 2,\n",
    "    \"ebad26\": 3,\n",
    "}\n",
    "\n",
    "df[\"fold\"] = df[\"prompt_id\"].map(id2fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "FY5ZdDSh86jL",
    "outputId": "f25ff859-20f1-4a74-f363-eb0dee6c3ba1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>student_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00791789cc1f</td>\n",
       "      <td>1 element of an ideal tragedy is that it shoul...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>0086ef22de8f</td>\n",
       "      <td>The three elements of an ideal tragedy are:  H...</td>\n",
       "      <td>-0.970237</td>\n",
       "      <td>-0.417058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>0094589c7a22</td>\n",
       "      <td>Aristotle states that an ideal tragedy should ...</td>\n",
       "      <td>-0.387791</td>\n",
       "      <td>-0.584181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00cd5736026a</td>\n",
       "      <td>One element of an Ideal tragedy is having a co...</td>\n",
       "      <td>0.088882</td>\n",
       "      <td>-0.594710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00d98b8ff756</td>\n",
       "      <td>The 3 ideal of tragedy is how complex you need...</td>\n",
       "      <td>-0.687288</td>\n",
       "      <td>-0.460886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff37545b2805</td>\n",
       "      <td>In paragraph two, they would use pickle meat a...</td>\n",
       "      <td>1.520355</td>\n",
       "      <td>-0.292990</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff4ed38ef099</td>\n",
       "      <td>in the first paragraph  it says \"either can it...</td>\n",
       "      <td>-1.204574</td>\n",
       "      <td>-1.169784</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff53b94f7ce0</td>\n",
       "      <td>They would have piles of filthy meat on the fl...</td>\n",
       "      <td>0.328739</td>\n",
       "      <td>-1.053294</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff7c7e70df07</td>\n",
       "      <td>They used all sorts of chemical concoctions to...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>fffbccfd8a08</td>\n",
       "      <td>The meat would smell sour but the would \"rub i...</td>\n",
       "      <td>1.771596</td>\n",
       "      <td>0.547742</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7165 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt_id                                    prompt_question  \\\n",
       "0       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "1       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "2       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "3       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "4       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "...        ...                                                ...   \n",
       "7160    ebad26  Summarize the various ways the factory would u...   \n",
       "7161    ebad26  Summarize the various ways the factory would u...   \n",
       "7162    ebad26  Summarize the various ways the factory would u...   \n",
       "7163    ebad26  Summarize the various ways the factory would u...   \n",
       "7164    ebad26  Summarize the various ways the factory would u...   \n",
       "\n",
       "                 prompt_title  \\\n",
       "0                  On Tragedy   \n",
       "1                  On Tragedy   \n",
       "2                  On Tragedy   \n",
       "3                  On Tragedy   \n",
       "4                  On Tragedy   \n",
       "...                       ...   \n",
       "7160  Excerpt from The Jungle   \n",
       "7161  Excerpt from The Jungle   \n",
       "7162  Excerpt from The Jungle   \n",
       "7163  Excerpt from The Jungle   \n",
       "7164  Excerpt from The Jungle   \n",
       "\n",
       "                                            prompt_text    student_id  \\\n",
       "0     Chapter 13 \\r\\nAs the sequel to what has alrea...  00791789cc1f   \n",
       "1     Chapter 13 \\r\\nAs the sequel to what has alrea...  0086ef22de8f   \n",
       "2     Chapter 13 \\r\\nAs the sequel to what has alrea...  0094589c7a22   \n",
       "3     Chapter 13 \\r\\nAs the sequel to what has alrea...  00cd5736026a   \n",
       "4     Chapter 13 \\r\\nAs the sequel to what has alrea...  00d98b8ff756   \n",
       "...                                                 ...           ...   \n",
       "7160  With one member trimming beef in a cannery, an...  ff37545b2805   \n",
       "7161  With one member trimming beef in a cannery, an...  ff4ed38ef099   \n",
       "7162  With one member trimming beef in a cannery, an...  ff53b94f7ce0   \n",
       "7163  With one member trimming beef in a cannery, an...  ff7c7e70df07   \n",
       "7164  With one member trimming beef in a cannery, an...  fffbccfd8a08   \n",
       "\n",
       "                                                   text   content   wording  \\\n",
       "0     1 element of an ideal tragedy is that it shoul... -0.210614 -0.471415   \n",
       "1     The three elements of an ideal tragedy are:  H... -0.970237 -0.417058   \n",
       "2     Aristotle states that an ideal tragedy should ... -0.387791 -0.584181   \n",
       "3     One element of an Ideal tragedy is having a co...  0.088882 -0.594710   \n",
       "4     The 3 ideal of tragedy is how complex you need... -0.687288 -0.460886   \n",
       "...                                                 ...       ...       ...   \n",
       "7160  In paragraph two, they would use pickle meat a...  1.520355 -0.292990   \n",
       "7161  in the first paragraph  it says \"either can it... -1.204574 -1.169784   \n",
       "7162  They would have piles of filthy meat on the fl...  0.328739 -1.053294   \n",
       "7163  They used all sorts of chemical concoctions to...  0.205683  0.380538   \n",
       "7164  The meat would smell sour but the would \"rub i...  1.771596  0.547742   \n",
       "\n",
       "      fold  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "...    ...  \n",
       "7160     3  \n",
       "7161     3  \n",
       "7162     3  \n",
       "7163     3  \n",
       "7164     3  \n",
       "\n",
       "[7165 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2bILjGET86jN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vpC_oJw86jO"
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3p6AtTj_86jO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSHcmlTt86jP"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X-XxwPIR86jP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_lm_datacollator = DataCollatorForWholeWordMask(tokenizer)\n",
    "def data_collator(batch):\n",
    "    input_ids = [{'input_ids':i[0]} for i in batch]\n",
    "    token_type_ids = [i[1] for i in batch]\n",
    "    attention_mask = [i[2] for i in batch]\n",
    "    labels = [i[3] for i in batch]\n",
    "    masked_input = mask_lm_datacollator(input_ids)['input_ids']\n",
    "    return masked_input,\\\n",
    "               torch.stack(token_type_ids),\\\n",
    "               torch.stack(attention_mask),\\\n",
    "               torch.stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.prompt_title = df['prompt_title'].values.astype(str)\n",
    "        self.prompt_text = df['prompt_text'].values.astype(str)\n",
    "        self.prompt_question = df['prompt_question'].values.astype(str)\n",
    "        self.text = df['text'].values.astype(str)\n",
    "        self.content = df['content'].values\n",
    "        self.wording = df['wording'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.prompt_title)\n",
    "    \n",
    "    def tokenize(self, example):\n",
    "        sep = self.tokenizer.sep_token\n",
    "        if  CFG.input_type == '1':\n",
    "            prompt = sep.join([example[\"prompt_title\"], example[\"prompt_text\"], example[\"prompt_question\"]])\n",
    "        else:\n",
    "            prompt = example[\"prompt_question\"] \n",
    "        \n",
    "        labels = [float(example[\"content\"]), float(example[\"wording\"])]\n",
    "\n",
    "        tokenized = tokenizer(\n",
    "            example[\"text\"],\n",
    "            prompt,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=CFG.max_input_length,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        example = {\n",
    "                    \"prompt_title\":self.prompt_title[item],\n",
    "                    \"prompt_text\":self.prompt_text[item],\n",
    "                    \"prompt_question\":self.prompt_question[item],\n",
    "                    \"text\":self.text[item],\n",
    "                    \"content\":self.content[item],\n",
    "                    \"wording\":self.wording[item],\n",
    "                  }\n",
    "        \n",
    "        out = self.tokenize(example)\n",
    "       \n",
    "        return {\n",
    "                'input_ids': torch.as_tensor(out['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids': torch.as_tensor(out['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask': torch.as_tensor(out['attention_mask'], dtype=torch.long),\n",
    "                'labels': torch.as_tensor(out['labels'], dtype=torch.float),\n",
    "        }\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP7TFS-c86jQ"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0A7qv3qn86jQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_params(module_lst):\n",
    "    for module in module_lst:\n",
    "        for param in module.parameters():\n",
    "            if param.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "    return\n",
    "\n",
    "class Custom_Bert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\"output_hidden_states\":True})\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "\n",
    "        dim = config.hidden_size\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        n_weights = 24\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(dim,1)\n",
    "        )\n",
    "        init_params([self.cls,self.attention])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                )\n",
    "\n",
    "        cls_outputs = torch.stack(\n",
    "            [self.dropout(layer) for layer in base_output['hidden_states'][-24:]], dim=0\n",
    "        )\n",
    "        cls_output = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * cls_outputs).sum(0)\n",
    "\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [torch.sum(self.attention(self.high_dropout(cls_output)) * cls_output, dim=1) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        output = self.cls(logits)\n",
    "        if labels is None:\n",
    "            return output\n",
    "\n",
    "        else:\n",
    "            return (nn.MSELoss()(torch.squeeze(output,1),labels), output)\n",
    "\n",
    "\n",
    "class Custom_Bert_Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "        dim = config.hidden_size\n",
    "        self.dropout = nn.Dropout(p=0)\n",
    "        self.cls = nn.Linear(dim,2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids\n",
    "                               )\n",
    "        output = base_output.last_hidden_state\n",
    "        output = self.cls(torch.mean(output, dim=1))\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=nn.MSELoss()(output,labels),\n",
    "            logits=output, \n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "\n",
    "class GeMText(nn.Module):\n",
    "    def __init__(self, dim = 1, p=3, eps=1e-6):\n",
    "        super(GeMText, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.feat_mult = 1\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n",
    "        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n",
    "        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n",
    "        ret = ret.pow(1 / self.p)\n",
    "        return ret    \n",
    "\n",
    "class Custom_Bert_Pool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        self.config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "        #self.base = AutoModel.from_pretrained(CFG.model_path, config=self.config)\n",
    "        print('load pretrained model ...');\n",
    "        self.base = AutoModel.from_pretrained('./input/pretrain/pretrained_model', config = self.config)\n",
    "        \n",
    "        self.pool = GeMText()\n",
    "        self.cls = nn.Linear(self.config.hidden_size,2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids\n",
    "                               )\n",
    "        output = base_output.last_hidden_state\n",
    "        output = self.cls(self.pool(output, attention_mask))\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=nn.SmoothL1Loss()(output,labels),\n",
    "            logits=output, \n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "class Custom_Bert_Mean(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.output_hidden_states=True\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "        dim = config.hidden_size\n",
    "        self.dropout = nn.Dropout(p=0)\n",
    "        self.cls = nn.Linear(dim,1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,labels=None):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                            )\n",
    "\n",
    "\n",
    "        output = base_output.hidden_states[-1]\n",
    "        output = self.cls(self.dropout(torch.mean(output, dim=1)))\n",
    "        if labels is None:\n",
    "            return output\n",
    "\n",
    "        else:\n",
    "            return (nn.MSELoss()(torch.squeeze(output,1),labels), output)\n",
    "\n",
    "class Custom_Bert_M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\"output_hidden_states\":True})\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "\n",
    "        dim = config.hidden_size\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        n_weights = 24\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.cls_0 = nn.Sequential(\n",
    "            nn.Linear(dim,1)\n",
    "        )\n",
    "\n",
    "        self.cls_1 = nn.Linear(dim,5)\n",
    "        init_params([self.cls_0,self.cls_1,self.attention])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                             )\n",
    "\n",
    "        cls_outputs = torch.stack(\n",
    "            [self.dropout(layer) for layer in base_output['hidden_states'][-24:]], dim=0\n",
    "        )\n",
    "        cls_output = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * cls_outputs).sum(0)\n",
    "\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [torch.sum(self.attention(self.high_dropout(cls_output)) * cls_output, dim=1) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        output_0 = self.cls_0(logits)\n",
    "        output_1 = self.cls_1(logits)\n",
    "        if labels is None:\n",
    "            return output_0\n",
    "\n",
    "        else:\n",
    "            regression_loss = nn.MSELoss()(torch.squeeze(output_0,1),labels)\n",
    "            labels = labels.double()\n",
    "            cls_labels = torch.where(labels==1.,4.0,labels)\n",
    "            cls_labels = torch.where(cls_labels==0.25,1.0,cls_labels)\n",
    "            cls_labels = torch.where(cls_labels==0.5,2.0,cls_labels)\n",
    "            cls_labels = torch.where(cls_labels==0.75,3.0,cls_labels)\n",
    "            cls_labels = cls_labels.long()\n",
    "            cls_loss = nn.CrossEntropyLoss()(output_1, cls_labels)\n",
    "            return ( 0.8 * regression_loss + 0.2 * cls_loss, output_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    if CFG.model_type == 'base':\n",
    "        model_config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        model_config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "\n",
    "        #print(model_config)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            CFG.model_path, config=model_config\n",
    "        )\n",
    "    if CFG.model_type == 'simple':\n",
    "        model = Custom_Bert_Simple()\n",
    "    if CFG.model_type == 'pool':\n",
    "        model = Custom_Bert_Pool()\n",
    "        if CFG.reinit_layers > 0:\n",
    "            print(\"==\"*40)\n",
    "            print(f\"Reinitialize the last {CFG.reinit_layers} layer(s).\")\n",
    "            for layer in model.base.encoder.layer[-CFG.reinit_layers:]:\n",
    "                print(\"===\")\n",
    "                layer.apply(model._init_weights)\n",
    "            print(\"==\"*40)\n",
    "        if CFG.load_pretrained:\n",
    "            model.load_state_dict(torch.load('./pretrained/microsoft_deberta-v3-base_best_ema.pth')['model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op1OHevU86jR"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "class ModelEMA:\n",
    "    \"\"\"Model Exponential Moving Average from https://github.com/rwightman/\n",
    "    pytorch-image-models Keep a moving average of everything in the model\n",
    "    state_dict (parameters and buffers).\n",
    "\n",
    "    This is intended to allow functionality like\n",
    "    https://www.tensorflow.org/api_docs/python/tf/train/\n",
    "    ExponentialMovingAverage\n",
    "    A smoothed version of the weights is necessary for some training\n",
    "    schemes to perform well.\n",
    "    This class is sensitive where it is initialized in the sequence\n",
    "    of model init, GPU assignment and distributed training wrappers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay=0.9999, updates=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (nn.Module): model to apply EMA.\n",
    "            decay (float): ema decay reate.\n",
    "            updates (int): counter of EMA updates.\n",
    "        \"\"\"\n",
    "        # Create EMA(FP32)\n",
    "        self.ema_model = deepcopy(model).eval()\n",
    "        self.ema = self.ema_model\n",
    "        self.updates = updates\n",
    "        # decay exponential ramp (to help early epochs)\n",
    "        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        # Update EMA parameters\n",
    "        with torch.no_grad():\n",
    "            self.updates += 1\n",
    "            d = self.decay(self.updates)\n",
    "            msd =  model.state_dict()# model state_dict\n",
    "            for k, v in self.ema.state_dict().items():\n",
    "                if v.dtype.is_floating_point:\n",
    "                    v *= d\n",
    "                    v += (1.0 - d) * msd[k].detach()\n",
    "\n",
    "class EMAHook:\n",
    "    \"\"\"EMAHook used in BEVDepth.\n",
    "\n",
    "    Modified from https://github.com/Megvii-Base\n",
    "    Detection/BEVDepth/blob/main/callbacks/ema.py.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, init_updates=0, decay=0.9990, resume=None, logger=None):\n",
    "        super().__init__()\n",
    "        self.init_updates = init_updates\n",
    "        self.resume = resume\n",
    "        self.decay = decay\n",
    "        self.ema_model = self.before_run(model)\n",
    "        self.logger = logger\n",
    "\n",
    "    def before_run(self, model):\n",
    "        from torch.nn.modules.batchnorm import SyncBatchNorm\n",
    "\n",
    "        bn_model_list = list()\n",
    "        bn_model_dist_group_list = list()\n",
    "        for model_ref in model.modules():\n",
    "            if isinstance(model_ref, SyncBatchNorm):\n",
    "                bn_model_list.append(model_ref)\n",
    "                bn_model_dist_group_list.append(model_ref.process_group)\n",
    "                model_ref.process_group = None\n",
    "        ema_model = ModelEMA(model, self.decay)\n",
    "\n",
    "        for bn_model, dist_group in zip(bn_model_list,\n",
    "                                        bn_model_dist_group_list):\n",
    "            bn_model.process_group = dist_group\n",
    "        ema_model.updates = self.init_updates\n",
    "\n",
    "        if self.resume is not None:\n",
    "            self.logger.info(f'resume ema checkpoint from {self.resume}')\n",
    "            cpt = torch.load(self.resume, map_location='cpu')\n",
    "            load_state_dict(ema_model.ema, cpt['state_dict'])\n",
    "            ema_model.updates = cpt['updates']\n",
    "\n",
    "        return ema_model\n",
    "\n",
    "    def after_train_iter(self, model):\n",
    "        self.ema_model.update(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "O__vnlBV86jR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qYJKjn8H86jR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.discriminative_learning_rate_num_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_llr_params(model, type='s'):\n",
    "    \"\"\"\n",
    "    Setup the optimizer.\n",
    "    We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
    "    Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
    "\n",
    "    MODIFIED VERSION:\n",
    "    * added support for differential learning rates per layer\n",
    "\n",
    "    reference: https://github.com/huggingface/transformers/blob/05fa1a7ac17bb7aa07b9e0c1e138ecb31a28bbfe/src/transformers/trainer.py#L804\n",
    "    \"\"\"\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
    "    ### ADDED\n",
    "    if CFG.discriminative_learning_rate:\n",
    "\n",
    "        num_layers = model.config.num_hidden_layers\n",
    "\n",
    "        learning_rate_powers = range(0, num_layers, num_layers//CFG.discriminative_learning_rate_num_groups)\n",
    "        layer_wise_learning_rates = [\n",
    "            pow(CFG.discriminative_learning_rate_decay_rate, power) * CFG.encoder_lr \n",
    "            for power in learning_rate_powers \n",
    "            for _ in range(num_layers//CFG.discriminative_learning_rate_num_groups)\n",
    "          ]\n",
    "        layer_wise_learning_rates = layer_wise_learning_rates[::-1]\n",
    "        print('Layer-wise learning rates:', layer_wise_learning_rates)\n",
    "\n",
    "        # group embedding paramters from the transformer encoder\n",
    "        embedding_layer = model.base.embeddings\n",
    "        optimizer_grouped_parameters = [\n",
    "          {\n",
    "              \"params\": [p for n, p in embedding_layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "              \"lr\": pow(CFG.discriminative_learning_rate_decay_rate, num_layers) * CFG.encoder_lr ,\n",
    "              \"weight_decay\": CFG.weight_decay,\n",
    "          },\n",
    "          {\n",
    "              \"params\": [p for n, p in embedding_layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "              \"lr\": pow(CFG.discriminative_learning_rate_decay_rate, num_layers) * CFG.encoder_lr ,\n",
    "              \"weight_decay\": 0.0,\n",
    "          },\n",
    "        ]\n",
    "\n",
    "        # group encoding paramters from the transformer encoder\n",
    "        encoding_layers = [layer for layer in model.base.encoder.layer]\n",
    "        for i, layer in enumerate(encoding_layers):\n",
    "            optimizer_grouped_parameters += [\n",
    "                {\n",
    "                    \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                    \"lr\": layer_wise_learning_rates[i],\n",
    "                    \"weight_decay\": CFG.weight_decay,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                    \"lr\": layer_wise_learning_rates[i],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "            ]    \n",
    "        print(f\"Detected unattached modules in model.encoder: {[n for n, p in model.base.encoder.named_parameters() if not n.startswith('layer')]}\")\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.base.encoder.named_parameters() if not n.startswith('layer') and not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": layer_wise_learning_rates[-1],\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.base.encoder.named_parameters() if not n.startswith('layer') and any(nd in n for nd in no_decay)],\n",
    "                \"lr\": layer_wise_learning_rates[-1],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # group paramters from the task specific head\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if 'base' not in n and not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.head_lr,\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if 'base' not in n and any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.head_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    ### END ADDED\n",
    "    else:\n",
    "        # group paramters for the entire network\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.encoder_lr,\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.encoder_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3HIHxVsj86jS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    start = end = time.time()\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        for key, value in batch.items():\n",
    "            batch[key] = value.to(device)\n",
    "        batch_size = batch['labels'].size(0)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**batch)\n",
    "        label = batch['labels']\n",
    "        loss, logits = model_output.loss, model_output.logits\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(logits.to('cpu').numpy())\n",
    "        labels.append(label.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, epoch, scheduler, device, valid_loader, start_time, best_score, best_score_ema,ema_hook,wandb, fold):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        for key, value in batch.items():\n",
    "            batch[key] = value.to(device)\n",
    "        batch_size = batch['labels'].size(0)\n",
    "        loss = model(**batch).loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        optimizer.step()\n",
    "        ema_hook.after_train_iter(model)\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        \n",
    "        wandb.log({\n",
    "                'train loss': loss.item(),\n",
    "                'step': global_step,\n",
    "                'epoch': epoch,\n",
    "                'fold': fold,\n",
    "                'batch_size':CFG.batch_size\n",
    "            })\n",
    "        \n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "            \n",
    "            # eval\n",
    "            avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, model, CFG.device)\n",
    "\n",
    "            # scoring\n",
    "            score = compute_mcrmse((predictions, valid_labels))\n",
    "\n",
    "\n",
    "            content_rmse, wording_rmse, mcrmse = list(score.values())\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch + 1} avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - content_rmse: {content_rmse:.4f} - wording_rmse: {wording_rmse:.4f} - mcrmse: {mcrmse:.4f}')\n",
    "            \n",
    "            \n",
    "            if best_score > score['mcrmse']:\n",
    "                if best_score != float('inf'):\n",
    "                    os.remove(CFG.OUTPUT_DIR + \"{}_best{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score))\n",
    "                best_score = score['mcrmse']\n",
    "                best_predictions = predictions\n",
    "                LOGGER.info(f'Epoch {epoch + 1} - Save Best Score: {best_score:.4f} Model')\n",
    "                torch.save({'model': model.state_dict(),\n",
    "                            'predictions': predictions},\n",
    "                           CFG.OUTPUT_DIR + \"{}_best{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score))\n",
    "            \n",
    "            \n",
    "            avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, ema_hook.ema_model.ema, CFG.device)\n",
    "            # ema scoring\n",
    "            ema_score = compute_mcrmse((predictions, valid_labels))\n",
    "\n",
    "\n",
    "            content_rmse, wording_rmse, mcrmse = list(ema_score.values())\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch + 1} avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - ema_content_rmse: {content_rmse:.4f} - ema_wording_rmse: {wording_rmse:.4f} - ema_mcrmse: {mcrmse:.4f}')\n",
    "            \n",
    "            \n",
    "            if best_score_ema > ema_score['mcrmse']:\n",
    "                if best_score_ema != float('inf'):\n",
    "                    os.remove(CFG.OUTPUT_DIR + \"{}_best_ema{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score_ema))\n",
    "                best_score_ema = ema_score['mcrmse']\n",
    "                best_predictions = predictions\n",
    "                LOGGER.info(f'Epoch {epoch + 1} - ema_Save Best Score: {best_score_ema:.4f} Model')\n",
    "                torch.save({'model': ema_hook.ema_model.ema.state_dict(),\n",
    "                            'predictions': predictions},\n",
    "                           CFG.OUTPUT_DIR + \"{}_best_ema{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold,best_score_ema))\n",
    "            \n",
    "            wandb.log({\n",
    "            'learning rate': optimizer.param_groups[0]['lr'],\n",
    "            'validation mcrmse': score['mcrmse'],\n",
    "            'validation ema mcrmse': ema_score['mcrmse'],\n",
    "            'step': global_step,\n",
    "            'epoch': epoch,\n",
    "        })\n",
    "            \n",
    "            model.train()\n",
    "    return losses.avg, best_score, best_score_ema\n",
    "\n",
    "\n",
    "\n",
    "def train_loop():\n",
    "    LOGGER.info(f\"========== training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    wandb.init(project='kaggle-commonlit-eval-student-summaries-0509')\n",
    "    wandb.config = dict(epochs=CFG.epochs, \n",
    "                            batch_size=CFG.batch_size, \n",
    "                            learning_rate=CFG.encoder_lr,\n",
    "                            save_checkpoint=True,\n",
    "                            )\n",
    "    for fold in CFG.folds:\n",
    "        \n",
    "        if CFG.pretraining:\n",
    "            tr_data = pd.read_csv('tmp_pessudo.csv')\n",
    "            tr_data['prompt_title'] = ''\n",
    "            tr_data = tr_data[-(tr_data['prompt_question'].isin(pdf['prompt_question'].tolist()))]\n",
    "            va_data = df #df[df['fold']==fold].reset_index(drop=True)\n",
    "        else:\n",
    "            tr_data = df[df['fold']!=fold].reset_index(drop=True)\n",
    "            va_data = df[df['fold']==fold].reset_index(drop=True)\n",
    "        train_dataset = TrainDataset(tr_data, tokenizer)\n",
    "        valid_dataset = TrainDataset(va_data, tokenizer)\n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                                  batch_size=CFG.batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "        valid_loader = DataLoader(valid_dataset,\n",
    "                                  batch_size=CFG.batch_size * 2,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "        # ====================================================\n",
    "        # model & optimizer\n",
    "        # ====================================================\n",
    "        model = build_model()\n",
    "        #model = AutoModelForSequenceClassification.from_pretrained(CFG.model_path, num_labels=1)\n",
    "        model.to(CFG.device)\n",
    "        # for param in model.base.parameters():\n",
    "        #         param.requires_grad = False\n",
    "        ema_hook = EMAHook(model, init_updates=3000, logger=LOGGER)\n",
    "        def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "            param_optimizer = list(model.named_parameters())\n",
    "            no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_parameters = [\n",
    "                {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                 'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "                {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                 'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            ]\n",
    "            return optimizer_parameters\n",
    "\n",
    "        optimizer_parameters = get_optimizer_llr_params(model)\n",
    "        optimizer = AdamW(optimizer_parameters, eps=CFG.eps, betas=CFG.betas)\n",
    "\n",
    "\n",
    "        \n",
    "        # ====================================================\n",
    "        # scheduler\n",
    "        # ====================================================\n",
    "        def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "            cfg.num_warmup_steps = cfg.num_warmup_steps * num_train_steps\n",
    "            if cfg.scheduler == 'linear':\n",
    "                scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "                )\n",
    "            elif cfg.scheduler == 'cosine':\n",
    "                scheduler = get_cosine_schedule_with_warmup(\n",
    "                    optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps,\n",
    "                    num_cycles=cfg.num_cycles\n",
    "                )\n",
    "            return scheduler\n",
    "\n",
    "        num_train_steps = int(len(train_dataset) / CFG.batch_size * CFG.epochs)\n",
    "        scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "        # ====================================================\n",
    "        # loop\n",
    "        # ====================================================\n",
    "        # criterion = torch.nn.CrossEntropyLoss(ignore_index=- 1)\n",
    "\n",
    "        # criterion = LabelSmoothingLoss()\n",
    "        best_score = float('inf')\n",
    "        best_score_ema = float('inf')\n",
    "        for epoch in range(CFG.epochs):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # train\n",
    "            avg_loss, best_score, best_score_ema = train_fn(train_loader, model, optimizer, epoch, scheduler, CFG.device, valid_loader, start_time, best_score, best_score_ema ,ema_hook, wandb,fold)\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        del scheduler, optimizer, model\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbgHecjCyz5D",
    "outputId": "09f6fba5-dd11-4adc-a85c-8805c6d6c90d",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== training ==========\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeng_sun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae1b6f853dc4213b3e9fcded92175f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669552416230242, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/workspace/commonlit/wandb/run-20230907_115323-3lrmwmzu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-0509/runs/3lrmwmzu' target=\"_blank\">noble-shadow-5</a></strong> to <a href='https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-0509' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-0509' target=\"_blank\">https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-0509</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-0509/runs/3lrmwmzu' target=\"_blank\">https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-0509/runs/3lrmwmzu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained model ...\n",
      "Epoch: [1][0/1292] Elapsed 0m 2s (remain 45m 52s) Loss: 0.6433(0.6433) Grad: 13.3274  LR: 0.00000500  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.4907  time: 257s\n",
      "Epoch 1 - content_rmse: 1.1861 - wording_rmse: 0.9419 - mcrmse: 1.0640\n",
      "Epoch 1 - Save Best Score: 1.0640 Model\n",
      "Epoch 1 avg_val_loss: 0.5089  time: 516s\n",
      "Epoch 1 - ema_content_rmse: 1.2172 - ema_wording_rmse: 0.9561 - ema_mcrmse: 1.0866\n",
      "Epoch 1 - ema_Save Best Score: 1.0866 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][100/1292] Elapsed 10m 51s (remain 128m 3s) Loss: 0.2233(0.3319) Grad: 6.6434  LR: 0.00000500  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2557  time: 907s\n",
      "Epoch 1 - content_rmse: 0.6035 - wording_rmse: 0.8529 - mcrmse: 0.7282\n",
      "Epoch 1 - Save Best Score: 0.7282 Model\n",
      "Epoch 1 avg_val_loss: 0.2356  time: 1166s\n",
      "Epoch 1 - ema_content_rmse: 0.5582 - ema_wording_rmse: 0.8301 - ema_mcrmse: 0.6941\n",
      "Epoch 1 - ema_Save Best Score: 0.6941 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][200/1292] Elapsed 21m 42s (remain 117m 47s) Loss: 0.0782(0.2471) Grad: 6.3141  LR: 0.00000498  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1714  time: 1557s\n",
      "Epoch 1 - content_rmse: 0.5184 - wording_rmse: 0.6712 - mcrmse: 0.5948\n",
      "Epoch 1 - Save Best Score: 0.5948 Model\n",
      "Epoch 1 avg_val_loss: 0.1719  time: 1816s\n",
      "Epoch 1 - ema_content_rmse: 0.5167 - ema_wording_rmse: 0.6735 - ema_mcrmse: 0.5951\n",
      "Epoch 1 - ema_Save Best Score: 0.5951 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][300/1292] Elapsed 32m 31s (remain 107m 5s) Loss: 0.2948(0.2180) Grad: 12.1504  LR: 0.00000496  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1514  time: 2207s\n",
      "Epoch 1 - content_rmse: 0.4948 - wording_rmse: 0.6176 - mcrmse: 0.5562\n",
      "Epoch 1 - Save Best Score: 0.5562 Model\n",
      "Epoch 1 avg_val_loss: 0.1576  time: 2465s\n",
      "Epoch 1 - ema_content_rmse: 0.4860 - ema_wording_rmse: 0.6458 - ema_mcrmse: 0.5659\n",
      "Epoch 1 - ema_Save Best Score: 0.5659 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][400/1292] Elapsed 43m 21s (remain 96m 19s) Loss: 0.1158(0.2023) Grad: 7.9900  LR: 0.00000493  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1694  time: 2856s\n",
      "Epoch 1 - content_rmse: 0.4886 - wording_rmse: 0.6866 - mcrmse: 0.5876\n",
      "Epoch 1 avg_val_loss: 0.1659  time: 3112s\n",
      "Epoch 1 - ema_content_rmse: 0.4914 - ema_wording_rmse: 0.6713 - ema_mcrmse: 0.5813\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][500/1292] Elapsed 54m 4s (remain 85m 23s) Loss: 0.1519(0.1924) Grad: 3.6263  LR: 0.00000488  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1230  time: 3500s\n",
      "Epoch 1 - content_rmse: 0.4518 - wording_rmse: 0.5492 - mcrmse: 0.5005\n",
      "Epoch 1 - Save Best Score: 0.5005 Model\n",
      "Epoch 1 avg_val_loss: 0.1235  time: 3759s\n",
      "Epoch 1 - ema_content_rmse: 0.4528 - ema_wording_rmse: 0.5508 - ema_mcrmse: 0.5018\n",
      "Epoch 1 - ema_Save Best Score: 0.5018 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][600/1292] Elapsed 64m 54s (remain 74m 38s) Loss: 0.2190(0.1857) Grad: 5.2220  LR: 0.00000484  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1140  time: 4150s\n",
      "Epoch 1 - content_rmse: 0.4362 - wording_rmse: 0.5242 - mcrmse: 0.4802\n",
      "Epoch 1 - Save Best Score: 0.4802 Model\n",
      "Epoch 1 avg_val_loss: 0.1143  time: 4408s\n",
      "Epoch 1 - ema_content_rmse: 0.4347 - ema_wording_rmse: 0.5263 - ema_mcrmse: 0.4805\n",
      "Epoch 1 - ema_Save Best Score: 0.4805 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][700/1292] Elapsed 75m 44s (remain 63m 51s) Loss: 0.0571(0.1792) Grad: 5.6324  LR: 0.00000478  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1258  time: 4800s\n",
      "Epoch 1 - content_rmse: 0.4549 - wording_rmse: 0.5531 - mcrmse: 0.5040\n",
      "Epoch 1 avg_val_loss: 0.1159  time: 5055s\n",
      "Epoch 1 - ema_content_rmse: 0.4281 - ema_wording_rmse: 0.5377 - ema_mcrmse: 0.4829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][800/1292] Elapsed 86m 28s (remain 53m 0s) Loss: 0.0448(0.1723) Grad: 1.7217  LR: 0.00000471  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1133  time: 5444s\n",
      "Epoch 1 - content_rmse: 0.4265 - wording_rmse: 0.5295 - mcrmse: 0.4780\n",
      "Epoch 1 - Save Best Score: 0.4780 Model\n",
      "Epoch 1 avg_val_loss: 0.1221  time: 5703s\n",
      "Epoch 1 - ema_content_rmse: 0.4316 - ema_wording_rmse: 0.5598 - ema_mcrmse: 0.4957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][900/1292] Elapsed 97m 16s (remain 42m 12s) Loss: 0.1616(0.1669) Grad: 3.6976  LR: 0.00000463  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1399  time: 6092s\n",
      "Epoch 1 - content_rmse: 0.4773 - wording_rmse: 0.5887 - mcrmse: 0.5330\n",
      "Epoch 1 avg_val_loss: 0.1407  time: 6348s\n",
      "Epoch 1 - ema_content_rmse: 0.4679 - ema_wording_rmse: 0.6021 - ema_mcrmse: 0.5350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1000/1292] Elapsed 108m 1s (remain 31m 24s) Loss: 0.1448(0.1624) Grad: 5.8580  LR: 0.00000455  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1306  time: 6737s\n",
      "Epoch 1 - content_rmse: 0.4677 - wording_rmse: 0.5587 - mcrmse: 0.5132\n",
      "Epoch 1 avg_val_loss: 0.1233  time: 6992s\n",
      "Epoch 1 - ema_content_rmse: 0.4500 - ema_wording_rmse: 0.5463 - ema_mcrmse: 0.4981\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1100/1292] Elapsed 118m 45s (remain 20m 36s) Loss: 0.0856(0.1586) Grad: 5.4312  LR: 0.00000446  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1320  time: 7381s\n",
      "Epoch 1 - content_rmse: 0.4336 - wording_rmse: 0.5928 - mcrmse: 0.5132\n",
      "Epoch 1 avg_val_loss: 0.1356  time: 7637s\n",
      "Epoch 1 - ema_content_rmse: 0.4424 - ema_wording_rmse: 0.5977 - ema_mcrmse: 0.5200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1200/1292] Elapsed 129m 30s (remain 9m 48s) Loss: 0.2404(0.1558) Grad: 4.5654  LR: 0.00000436  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1194  time: 8026s\n",
      "Epoch 1 - content_rmse: 0.4446 - wording_rmse: 0.5477 - mcrmse: 0.4961\n",
      "Epoch 1 avg_val_loss: 0.1200  time: 8281s\n",
      "Epoch 1 - ema_content_rmse: 0.4422 - ema_wording_rmse: 0.5497 - ema_mcrmse: 0.4960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1291/1292] Elapsed 140m 2s (remain 0m 0s) Loss: 0.2700(0.1535) Grad: 12.2806  LR: 0.00000427  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1285  time: 8658s\n",
      "Epoch 1 - content_rmse: 0.4769 - wording_rmse: 0.5471 - mcrmse: 0.5120\n",
      "Epoch 1 avg_val_loss: 0.1345  time: 8914s\n",
      "Epoch 1 - ema_content_rmse: 0.4618 - ema_wording_rmse: 0.5853 - ema_mcrmse: 0.5235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/1292] Elapsed 0m 1s (remain 33m 18s) Loss: 0.2382(0.2382) Grad: 6.5344  LR: 0.00000427  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1260  time: 257s\n",
      "Epoch 2 - content_rmse: 0.4788 - wording_rmse: 0.5346 - mcrmse: 0.5067\n",
      "Epoch 2 avg_val_loss: 0.1331  time: 512s\n",
      "Epoch 2 - ema_content_rmse: 0.4635 - ema_wording_rmse: 0.5779 - ema_mcrmse: 0.5207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][100/1292] Elapsed 10m 46s (remain 126m 59s) Loss: 0.0385(0.0951) Grad: 3.3556  LR: 0.00000416  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1384  time: 901s\n",
      "Epoch 2 - content_rmse: 0.4411 - wording_rmse: 0.6116 - mcrmse: 0.5264\n",
      "Epoch 2 avg_val_loss: 0.1366  time: 1157s\n",
      "Epoch 2 - ema_content_rmse: 0.4361 - ema_wording_rmse: 0.6086 - ema_mcrmse: 0.5224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][200/1292] Elapsed 21m 30s (remain 116m 42s) Loss: 0.0760(0.0920) Grad: 3.8933  LR: 0.00000404  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1204  time: 1545s\n",
      "Epoch 2 - content_rmse: 0.4665 - wording_rmse: 0.5219 - mcrmse: 0.4942\n",
      "Epoch 2 avg_val_loss: 0.1142  time: 1801s\n",
      "Epoch 2 - ema_content_rmse: 0.4427 - ema_wording_rmse: 0.5184 - ema_mcrmse: 0.4805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][300/1292] Elapsed 32m 14s (remain 106m 7s) Loss: 0.0284(0.0923) Grad: 1.8707  LR: 0.00000392  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1149  time: 2189s\n",
      "Epoch 2 - content_rmse: 0.4307 - wording_rmse: 0.5308 - mcrmse: 0.4807\n",
      "Epoch 2 avg_val_loss: 0.1160  time: 2445s\n",
      "Epoch 2 - ema_content_rmse: 0.4371 - ema_wording_rmse: 0.5295 - ema_mcrmse: 0.4833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][400/1292] Elapsed 42m 58s (remain 95m 29s) Loss: 0.1031(0.0952) Grad: 3.3970  LR: 0.00000379  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1217  time: 2834s\n",
      "Epoch 2 - content_rmse: 0.4560 - wording_rmse: 0.5384 - mcrmse: 0.4972\n",
      "Epoch 2 avg_val_loss: 0.1148  time: 3089s\n",
      "Epoch 2 - ema_content_rmse: 0.4326 - ema_wording_rmse: 0.5294 - ema_mcrmse: 0.4810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][500/1292] Elapsed 53m 42s (remain 84m 47s) Loss: 0.0490(0.0940) Grad: 3.9690  LR: 0.00000366  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1088  time: 3478s\n",
      "Epoch 2 - content_rmse: 0.4259 - wording_rmse: 0.5118 - mcrmse: 0.4689\n",
      "Epoch 2 - Save Best Score: 0.4689 Model\n",
      "Epoch 2 avg_val_loss: 0.1071  time: 3737s\n",
      "Epoch 2 - ema_content_rmse: 0.4173 - ema_wording_rmse: 0.5117 - ema_mcrmse: 0.4645\n",
      "Epoch 2 - ema_Save Best Score: 0.4645 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][600/1292] Elapsed 64m 32s (remain 74m 12s) Loss: 0.0581(0.0956) Grad: 2.9866  LR: 0.00000352  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1063  time: 4128s\n",
      "Epoch 2 - content_rmse: 0.4143 - wording_rmse: 0.5103 - mcrmse: 0.4623\n",
      "Epoch 2 - Save Best Score: 0.4623 Model\n",
      "Epoch 2 avg_val_loss: 0.1163  time: 4386s\n",
      "Epoch 2 - ema_content_rmse: 0.4207 - ema_wording_rmse: 0.5446 - ema_mcrmse: 0.4827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][700/1292] Elapsed 75m 19s (remain 63m 30s) Loss: 0.0388(0.0933) Grad: 5.1436  LR: 0.00000338  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1131  time: 4775s\n",
      "Epoch 2 - content_rmse: 0.4187 - wording_rmse: 0.5331 - mcrmse: 0.4759\n",
      "Epoch 2 avg_val_loss: 0.1196  time: 5031s\n",
      "Epoch 2 - ema_content_rmse: 0.4215 - ema_wording_rmse: 0.5557 - ema_mcrmse: 0.4886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][800/1292] Elapsed 86m 4s (remain 52m 45s) Loss: 0.1588(0.0930) Grad: 5.0957  LR: 0.00000324  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1156  time: 5420s\n",
      "Epoch 2 - content_rmse: 0.4239 - wording_rmse: 0.5411 - mcrmse: 0.4825\n",
      "Epoch 2 avg_val_loss: 0.1137  time: 5675s\n",
      "Epoch 2 - ema_content_rmse: 0.4258 - ema_wording_rmse: 0.5330 - ema_mcrmse: 0.4794\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][900/1292] Elapsed 96m 48s (remain 42m 0s) Loss: 0.1037(0.0934) Grad: 5.7582  LR: 0.00000309  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1121  time: 6064s\n",
      "Epoch 2 - content_rmse: 0.4293 - wording_rmse: 0.5200 - mcrmse: 0.4746\n",
      "Epoch 2 avg_val_loss: 0.1077  time: 6319s\n",
      "Epoch 2 - ema_content_rmse: 0.4212 - ema_wording_rmse: 0.5087 - ema_mcrmse: 0.4650\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1000/1292] Elapsed 107m 32s (remain 31m 15s) Loss: 0.0342(0.0927) Grad: 3.7011  LR: 0.00000294  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1105  time: 6708s\n",
      "Epoch 2 - content_rmse: 0.4142 - wording_rmse: 0.5259 - mcrmse: 0.4701\n",
      "Epoch 2 avg_val_loss: 0.1123  time: 6964s\n",
      "Epoch 2 - ema_content_rmse: 0.4167 - ema_wording_rmse: 0.5317 - ema_mcrmse: 0.4742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1100/1292] Elapsed 118m 17s (remain 20m 31s) Loss: 0.0458(0.0933) Grad: 2.4769  LR: 0.00000279  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1223  time: 7353s\n",
      "Epoch 2 - content_rmse: 0.4319 - wording_rmse: 0.5570 - mcrmse: 0.4945\n",
      "Epoch 2 avg_val_loss: 0.1112  time: 7608s\n",
      "Epoch 2 - ema_content_rmse: 0.4210 - ema_wording_rmse: 0.5235 - ema_mcrmse: 0.4722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1200/1292] Elapsed 129m 1s (remain 9m 46s) Loss: 0.0585(0.0926) Grad: 3.8886  LR: 0.00000264  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1188  time: 7997s\n",
      "Epoch 2 - content_rmse: 0.4232 - wording_rmse: 0.5511 - mcrmse: 0.4872\n",
      "Epoch 2 avg_val_loss: 0.1152  time: 8252s\n",
      "Epoch 2 - ema_content_rmse: 0.4254 - ema_wording_rmse: 0.5355 - ema_mcrmse: 0.4804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1291/1292] Elapsed 139m 33s (remain 0m 0s) Loss: 0.0317(0.0924) Grad: 2.9553  LR: 0.00000250  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1145  time: 8629s\n",
      "Epoch 2 - content_rmse: 0.4320 - wording_rmse: 0.5296 - mcrmse: 0.4808\n",
      "Epoch 2 avg_val_loss: 0.1145  time: 8884s\n",
      "Epoch 2 - ema_content_rmse: 0.4169 - ema_wording_rmse: 0.5414 - ema_mcrmse: 0.4792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/1292] Elapsed 0m 2s (remain 44m 6s) Loss: 0.0560(0.0560) Grad: 4.1548  LR: 0.00000250  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1148  time: 258s\n",
      "Epoch 3 - content_rmse: 0.4348 - wording_rmse: 0.5284 - mcrmse: 0.4816\n",
      "Epoch 3 avg_val_loss: 0.1142  time: 513s\n",
      "Epoch 3 - ema_content_rmse: 0.4172 - ema_wording_rmse: 0.5401 - ema_mcrmse: 0.4787\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][100/1292] Elapsed 10m 46s (remain 127m 2s) Loss: 0.0921(0.0685) Grad: 5.0340  LR: 0.00000235  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1144  time: 902s\n",
      "Epoch 3 - content_rmse: 0.4146 - wording_rmse: 0.5408 - mcrmse: 0.4777\n",
      "Epoch 3 avg_val_loss: 0.1127  time: 1157s\n",
      "Epoch 3 - ema_content_rmse: 0.4146 - ema_wording_rmse: 0.5343 - ema_mcrmse: 0.4745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][200/1292] Elapsed 21m 30s (remain 116m 44s) Loss: 0.0582(0.0671) Grad: 1.6158  LR: 0.00000220  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1097  time: 1546s\n",
      "Epoch 3 - content_rmse: 0.4143 - wording_rmse: 0.5226 - mcrmse: 0.4684\n",
      "Epoch 3 avg_val_loss: 0.1080  time: 1801s\n",
      "Epoch 3 - ema_content_rmse: 0.4154 - ema_wording_rmse: 0.5153 - ema_mcrmse: 0.4654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][300/1292] Elapsed 32m 14s (remain 106m 8s) Loss: 0.0287(0.0660) Grad: 2.5671  LR: 0.00000205  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1048  time: 2190s\n",
      "Epoch 3 - content_rmse: 0.4255 - wording_rmse: 0.4953 - mcrmse: 0.4604\n",
      "Epoch 3 - Save Best Score: 0.4604 Model\n",
      "Epoch 3 avg_val_loss: 0.1061  time: 2449s\n",
      "Epoch 3 - ema_content_rmse: 0.4184 - ema_wording_rmse: 0.5055 - ema_mcrmse: 0.4619\n",
      "Epoch 3 - ema_Save Best Score: 0.4619 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][400/1292] Elapsed 43m 4s (remain 95m 43s) Loss: 0.0333(0.0642) Grad: 2.5433  LR: 0.00000190  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1160  time: 2840s\n",
      "Epoch 3 - content_rmse: 0.4188 - wording_rmse: 0.5446 - mcrmse: 0.4817\n",
      "Epoch 3 avg_val_loss: 0.1131  time: 3095s\n",
      "Epoch 3 - ema_content_rmse: 0.4205 - ema_wording_rmse: 0.5320 - ema_mcrmse: 0.4762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][500/1292] Elapsed 53m 48s (remain 84m 57s) Loss: 0.0805(0.0624) Grad: 6.3382  LR: 0.00000175  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1128  time: 3484s\n",
      "Epoch 3 - content_rmse: 0.4230 - wording_rmse: 0.5307 - mcrmse: 0.4768\n",
      "Epoch 3 avg_val_loss: 0.1089  time: 3740s\n",
      "Epoch 3 - ema_content_rmse: 0.4130 - ema_wording_rmse: 0.5222 - ema_mcrmse: 0.4676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][600/1292] Elapsed 64m 33s (remain 74m 13s) Loss: 0.0351(0.0613) Grad: 3.3041  LR: 0.00000161  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1081  time: 4129s\n",
      "Epoch 3 - content_rmse: 0.4169 - wording_rmse: 0.5157 - mcrmse: 0.4663\n",
      "Epoch 3 avg_val_loss: 0.1065  time: 4384s\n",
      "Epoch 3 - ema_content_rmse: 0.4161 - ema_wording_rmse: 0.5098 - ema_mcrmse: 0.4630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][700/1292] Elapsed 75m 17s (remain 63m 28s) Loss: 0.0422(0.0615) Grad: 2.4590  LR: 0.00000147  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1080  time: 4773s\n",
      "Epoch 3 - content_rmse: 0.4191 - wording_rmse: 0.5128 - mcrmse: 0.4660\n",
      "Epoch 3 avg_val_loss: 0.1037  time: 5028s\n",
      "Epoch 3 - ema_content_rmse: 0.4162 - ema_wording_rmse: 0.4982 - ema_mcrmse: 0.4572\n",
      "Epoch 3 - ema_Save Best Score: 0.4572 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][800/1292] Elapsed 86m 4s (remain 52m 45s) Loss: 0.0255(0.0605) Grad: 2.0015  LR: 0.00000133  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1074  time: 5420s\n",
      "Epoch 3 - content_rmse: 0.4174 - wording_rmse: 0.5108 - mcrmse: 0.4641\n",
      "Epoch 3 avg_val_loss: 0.1056  time: 5675s\n",
      "Epoch 3 - ema_content_rmse: 0.4170 - ema_wording_rmse: 0.5041 - ema_mcrmse: 0.4605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][900/1292] Elapsed 96m 49s (remain 42m 0s) Loss: 0.0354(0.0601) Grad: 4.1613  LR: 0.00000120  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1047  time: 6064s\n",
      "Epoch 3 - content_rmse: 0.4113 - wording_rmse: 0.5050 - mcrmse: 0.4581\n",
      "Epoch 3 - Save Best Score: 0.4581 Model\n",
      "Epoch 3 avg_val_loss: 0.1047  time: 6324s\n",
      "Epoch 3 - ema_content_rmse: 0.4128 - ema_wording_rmse: 0.5038 - ema_mcrmse: 0.4583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1000/1292] Elapsed 107m 36s (remain 31m 17s) Loss: 0.0739(0.0607) Grad: 5.4903  LR: 0.00000107  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1014  time: 6712s\n",
      "Epoch 3 - content_rmse: 0.4108 - wording_rmse: 0.4925 - mcrmse: 0.4516\n",
      "Epoch 3 - Save Best Score: 0.4516 Model\n",
      "Epoch 3 avg_val_loss: 0.1018  time: 6971s\n",
      "Epoch 3 - ema_content_rmse: 0.4090 - ema_wording_rmse: 0.4954 - ema_mcrmse: 0.4522\n",
      "Epoch 3 - ema_Save Best Score: 0.4522 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1100/1292] Elapsed 118m 27s (remain 20m 33s) Loss: 0.0823(0.0602) Grad: 6.1501  LR: 0.00000095  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1030  time: 7363s\n",
      "Epoch 3 - content_rmse: 0.4133 - wording_rmse: 0.4972 - mcrmse: 0.4552\n",
      "Epoch 3 avg_val_loss: 0.1038  time: 7619s\n",
      "Epoch 3 - ema_content_rmse: 0.4131 - ema_wording_rmse: 0.5001 - ema_mcrmse: 0.4566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1200/1292] Elapsed 129m 12s (remain 9m 47s) Loss: 0.0811(0.0596) Grad: 6.4799  LR: 0.00000083  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1038  time: 8008s\n",
      "Epoch 3 - content_rmse: 0.4154 - wording_rmse: 0.4982 - mcrmse: 0.4568\n",
      "Epoch 3 avg_val_loss: 0.1039  time: 8263s\n",
      "Epoch 3 - ema_content_rmse: 0.4115 - ema_wording_rmse: 0.5016 - ema_mcrmse: 0.4566\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1291/1292] Elapsed 139m 44s (remain 0m 0s) Loss: 0.0920(0.0595) Grad: 3.5704  LR: 0.00000073  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1061  time: 8640s\n",
      "Epoch 3 - content_rmse: 0.4101 - wording_rmse: 0.5118 - mcrmse: 0.4610\n",
      "Epoch 3 avg_val_loss: 0.1071  time: 8895s\n",
      "Epoch 3 - ema_content_rmse: 0.4094 - ema_wording_rmse: 0.5163 - ema_mcrmse: 0.4628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/1292] Elapsed 0m 1s (remain 33m 45s) Loss: 0.0200(0.0200) Grad: 2.1681  LR: 0.00000073  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1062  time: 257s\n",
      "Epoch 4 - content_rmse: 0.4100 - wording_rmse: 0.5122 - mcrmse: 0.4611\n",
      "Epoch 4 avg_val_loss: 0.1071  time: 513s\n",
      "Epoch 4 - ema_content_rmse: 0.4094 - ema_wording_rmse: 0.5161 - ema_mcrmse: 0.4627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][100/1292] Elapsed 10m 45s (remain 126m 56s) Loss: 0.0245(0.0435) Grad: 1.9923  LR: 0.00000063  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1071  time: 901s\n",
      "Epoch 4 - content_rmse: 0.4119 - wording_rmse: 0.5145 - mcrmse: 0.4632\n",
      "Epoch 4 avg_val_loss: 0.1070  time: 1157s\n",
      "Epoch 4 - ema_content_rmse: 0.4115 - ema_wording_rmse: 0.5142 - ema_mcrmse: 0.4629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][200/1292] Elapsed 21m 30s (remain 116m 44s) Loss: 0.0347(0.0412) Grad: 2.4873  LR: 0.00000053  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1074  time: 1546s\n",
      "Epoch 4 - content_rmse: 0.4114 - wording_rmse: 0.5158 - mcrmse: 0.4636\n",
      "Epoch 4 avg_val_loss: 0.1072  time: 1801s\n",
      "Epoch 4 - ema_content_rmse: 0.4119 - ema_wording_rmse: 0.5149 - ema_mcrmse: 0.4634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][300/1292] Elapsed 32m 15s (remain 106m 11s) Loss: 0.0805(0.0403) Grad: 4.0934  LR: 0.00000044  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1075  time: 2191s\n",
      "Epoch 4 - content_rmse: 0.4118 - wording_rmse: 0.5161 - mcrmse: 0.4639\n",
      "Epoch 4 avg_val_loss: 0.1062  time: 2446s\n",
      "Epoch 4 - ema_content_rmse: 0.4119 - ema_wording_rmse: 0.5109 - ema_mcrmse: 0.4614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][400/1292] Elapsed 42m 59s (remain 95m 30s) Loss: 0.0503(0.0418) Grad: 4.7571  LR: 0.00000036  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1068  time: 2834s\n",
      "Epoch 4 - content_rmse: 0.4141 - wording_rmse: 0.5114 - mcrmse: 0.4627\n",
      "Epoch 4 avg_val_loss: 0.1068  time: 3090s\n",
      "Epoch 4 - ema_content_rmse: 0.4141 - ema_wording_rmse: 0.5115 - ema_mcrmse: 0.4628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][500/1292] Elapsed 53m 43s (remain 84m 48s) Loss: 0.0232(0.0407) Grad: 1.7933  LR: 0.00000028  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1064  time: 3478s\n",
      "Epoch 4 - content_rmse: 0.4173 - wording_rmse: 0.5076 - mcrmse: 0.4624\n",
      "Epoch 4 avg_val_loss: 0.1070  time: 3734s\n",
      "Epoch 4 - ema_content_rmse: 0.4132 - ema_wording_rmse: 0.5130 - ema_mcrmse: 0.4631\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][600/1292] Elapsed 64m 27s (remain 74m 6s) Loss: 0.0735(0.0404) Grad: 4.4796  LR: 0.00000022  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1061  time: 4123s\n",
      "Epoch 4 - content_rmse: 0.4127 - wording_rmse: 0.5100 - mcrmse: 0.4614\n",
      "Epoch 4 avg_val_loss: 0.1063  time: 4378s\n",
      "Epoch 4 - ema_content_rmse: 0.4136 - ema_wording_rmse: 0.5101 - ema_mcrmse: 0.4619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][700/1292] Elapsed 75m 11s (remain 63m 23s) Loss: 0.0201(0.0401) Grad: 1.6478  LR: 0.00000016  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1054  time: 4767s\n",
      "Epoch 4 - content_rmse: 0.4136 - wording_rmse: 0.5066 - mcrmse: 0.4601\n",
      "Epoch 4 avg_val_loss: 0.1056  time: 5022s\n",
      "Epoch 4 - ema_content_rmse: 0.4138 - ema_wording_rmse: 0.5070 - ema_mcrmse: 0.4604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][800/1292] Elapsed 85m 55s (remain 52m 40s) Loss: 0.0201(0.0402) Grad: 2.7246  LR: 0.00000011  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1057  time: 5411s\n",
      "Epoch 4 - content_rmse: 0.4133 - wording_rmse: 0.5079 - mcrmse: 0.4606\n",
      "Epoch 4 avg_val_loss: 0.1056  time: 5666s\n",
      "Epoch 4 - ema_content_rmse: 0.4131 - ema_wording_rmse: 0.5076 - ema_mcrmse: 0.4604\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][900/1292] Elapsed 96m 39s (remain 41m 56s) Loss: 0.0150(0.0399) Grad: 2.5146  LR: 0.00000007  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1055  time: 6055s\n",
      "Epoch 4 - content_rmse: 0.4145 - wording_rmse: 0.5060 - mcrmse: 0.4603\n",
      "Epoch 4 avg_val_loss: 0.1055  time: 6310s\n",
      "Epoch 4 - ema_content_rmse: 0.4135 - ema_wording_rmse: 0.5070 - ema_mcrmse: 0.4603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][1000/1292] Elapsed 107m 23s (remain 31m 13s) Loss: 0.0287(0.0400) Grad: 1.9929  LR: 0.00000004  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1047  time: 6699s\n",
      "Epoch 4 - content_rmse: 0.4146 - wording_rmse: 0.5029 - mcrmse: 0.4588\n",
      "Epoch 4 avg_val_loss: 0.1049  time: 6954s\n",
      "Epoch 4 - ema_content_rmse: 0.4145 - ema_wording_rmse: 0.5038 - ema_mcrmse: 0.4592\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][1100/1292] Elapsed 118m 7s (remain 20m 29s) Loss: 0.0696(0.0399) Grad: 6.3669  LR: 0.00000002  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1049  time: 7343s\n",
      "Epoch 4 - content_rmse: 0.4140 - wording_rmse: 0.5041 - mcrmse: 0.4591\n",
      "Epoch 4 avg_val_loss: 0.1048  time: 7598s\n",
      "Epoch 4 - ema_content_rmse: 0.4142 - ema_wording_rmse: 0.5035 - ema_mcrmse: 0.4589\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][1200/1292] Elapsed 128m 52s (remain 9m 45s) Loss: 0.0172(0.0396) Grad: 2.0420  LR: 0.00000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1049  time: 7987s\n",
      "Epoch 4 - content_rmse: 0.4139 - wording_rmse: 0.5042 - mcrmse: 0.4591\n",
      "Epoch 4 avg_val_loss: 0.1049  time: 8243s\n",
      "Epoch 4 - ema_content_rmse: 0.4140 - ema_wording_rmse: 0.5042 - ema_mcrmse: 0.4591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][1291/1292] Elapsed 139m 23s (remain 0m 0s) Loss: 0.0135(0.0397) Grad: 2.5051  LR: 0.00000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1049  time: 8619s\n",
      "Epoch 4 - content_rmse: 0.4139 - wording_rmse: 0.5042 - mcrmse: 0.4591\n",
      "Epoch 4 avg_val_loss: 0.1049  time: 8875s\n",
      "Epoch 4 - ema_content_rmse: 0.4139 - ema_wording_rmse: 0.5042 - ema_mcrmse: 0.4591\n"
     ]
    }
   ],
   "source": [
    "train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BV3Rp1bbXUQ-"
   },
   "outputs": [],
   "source": [
    "## total_complex = []\n",
    "# for fold in range(4):\n",
    "#     va_data = train_df[train_df['fold'] == fold]\n",
    "#     preds = torch.load('/content/drive/MyDrive/deb_simple/microsoft_deberta-v3-large_best{}.pth'.format(fold))['predictions']\n",
    "#     va_data['preds'] = preds\n",
    "#     va_data = va_data[['id', 'preds', 'score']]\n",
    "#     print(compute_metrics(va_data['preds'].values.reshape(-1,1), va_data['score'].values))\n",
    "#     total_complex.append(va_data)\n",
    "# total_complex = pd.concat(total_complex)\n",
    "# compute_metrics(total_complex['preds'].values.reshape(-1,1), total_complex['score'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11_KW3qSx1IK",
    "outputId": "e5423c46-d702-49da-d29b-c5d125a665e4"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /root/.kaggle\n",
    "# !cp /content/drive/MyDrive/kaggle/kaggle.json /root/.kaggle/\n",
    "# !chmod 600 /root/.kaggle/kaggle.json\n",
    "# !kaggle datasets init -p /content/drive/MyDrive/elc_mean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cN6F3A_hKthE",
    "outputId": "0358b1da-6195-44ba-8ab5-2eacb5268fff"
   },
   "outputs": [],
   "source": [
    "#!kaggle datasets create -p /content/drive/MyDrive/elc_mean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "BGinNBYJFw3O"
   },
   "outputs": [],
   "source": [
    "# deberta v3 large\n",
    "# 1.5 0.8228\n",
    "# 2 0.8197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVfKappB86jX",
    "tags": []
   },
   "source": [
    "# 1.5  8137\n",
    "#2 8175\n",
    "#2.5 8181\n",
    "#3 8181\n",
    "#3.5 8175\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
