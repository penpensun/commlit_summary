{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log\n",
    "* use 512 instead of 1024 as max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AmhfxdCR86jB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedGroupKFold\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForWholeWordMask\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n",
    "from transformers.modeling_outputs  import BaseModelOutput,SequenceClassifierOutput\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "# imports the torch_xla package\n",
    "import wandb\n",
    "from torch.nn.parameter import Parameter\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1dMoHaCUn29I",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Model training')\n",
    "    # params of training\n",
    "    parser.add_argument(\n",
    "        \"--fold\", dest=\"fold\", help=\"Train fold\", default=None, type=int)\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        dest='batch_size',\n",
    "        help='Mini batch size of one gpu or cpu',\n",
    "        type=int,\n",
    "        default=None)\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6G2r5GS86jE"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Bojtddae86jG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    pretraining = False\n",
    "    load_pretrained = False\n",
    "    input_path = './input/'\n",
    "    input_type = '2'\n",
    "    model_path = 'microsoft/deberta-v3-large' #  nghuyong/ernie-2.0-large-en studio-ousia/luke-large\n",
    "    model_type = 'pool'\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5  # 1.5\n",
    "    num_warmup_steps = 0\n",
    "    max_input_length = 512\n",
    "    max_position_embeddings = 512\n",
    "    folds = [2,3]\n",
    "    epochs = 4  # 5\n",
    "    # layer - wise larning rate \n",
    "    discriminative_learning_rate = False\n",
    "    discriminative_learning_rate_num_groups = 1\n",
    "    discriminative_learning_rate_decay_rate = 0.99\n",
    "    # reinint layer\n",
    "    reinit_layers = 0\n",
    "    \n",
    "    encoder_lr = 5e-6\n",
    "    head_lr = 5e-6\n",
    "    min_lr = 1e-7\n",
    "    eps = 1e-7\n",
    "    betas = (0.9, 0.999)\n",
    "    weight_decay = 0\n",
    "    dropout = 0\n",
    "    num_fold = 5\n",
    "    batch_size = 4\n",
    "    seed = 42\n",
    "    OUTPUT_DIR = './pretrain/'\n",
    "    num_workers = 2\n",
    "    device='cuda'\n",
    "    print_freq = 100\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FaoPHcM86jH"
   },
   "source": [
    "## logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RWkPxbhq86jI",
    "outputId": "1f97e316-be01-408d-c79b-1b128aaff9c9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===============lr_5e-06===============\n",
      "===============seed_42===============\n",
      "===============total_epochs_4===============\n",
      "===============num_warmup_steps_0===============\n"
     ]
    }
   ],
   "source": [
    "def get_logger(filename=CFG.OUTPUT_DIR+ 'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    if not os.path.exists(CFG.OUTPUT_DIR):\n",
    "        os.makedirs(CFG.OUTPUT_DIR)\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "LOGGER.info('===============lr_{}==============='.format(CFG.encoder_lr))\n",
    "LOGGER.info('===============seed_{}==============='.format(CFG.seed))\n",
    "LOGGER.info('===============total_epochs_{}==============='.format(CFG.epochs))\n",
    "LOGGER.info('===============num_warmup_steps_{}==============='.format(CFG.num_warmup_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSqTonVY86jJ"
   },
   "source": [
    "# Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ACTTNzFL86jK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf = pd.read_csv(f\"{CFG.input_path}/prompts_train.csv\")\n",
    "sdf = pd.read_csv(f\"{CFG.input_path}/summaries_train.csv\")\n",
    "\n",
    "df = pdf.merge(sdf, on=\"prompt_id\")\n",
    "\n",
    "# 4 prompt ids, 4 folds\n",
    "id2fold = {\n",
    "    \"814d6b\": 0,\n",
    "    \"39c16e\": 1,\n",
    "    \"3b9047\": 2,\n",
    "    \"ebad26\": 3,\n",
    "}\n",
    "\n",
    "df[\"fold\"] = df[\"prompt_id\"].map(id2fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "FY5ZdDSh86jL",
    "outputId": "f25ff859-20f1-4a74-f363-eb0dee6c3ba1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>student_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00791789cc1f</td>\n",
       "      <td>1 element of an ideal tragedy is that it shoul...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>0086ef22de8f</td>\n",
       "      <td>The three elements of an ideal tragedy are:  H...</td>\n",
       "      <td>-0.970237</td>\n",
       "      <td>-0.417058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>0094589c7a22</td>\n",
       "      <td>Aristotle states that an ideal tragedy should ...</td>\n",
       "      <td>-0.387791</td>\n",
       "      <td>-0.584181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00cd5736026a</td>\n",
       "      <td>One element of an Ideal tragedy is having a co...</td>\n",
       "      <td>0.088882</td>\n",
       "      <td>-0.594710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00d98b8ff756</td>\n",
       "      <td>The 3 ideal of tragedy is how complex you need...</td>\n",
       "      <td>-0.687288</td>\n",
       "      <td>-0.460886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff37545b2805</td>\n",
       "      <td>In paragraph two, they would use pickle meat a...</td>\n",
       "      <td>1.520355</td>\n",
       "      <td>-0.292990</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff4ed38ef099</td>\n",
       "      <td>in the first paragraph  it says \"either can it...</td>\n",
       "      <td>-1.204574</td>\n",
       "      <td>-1.169784</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff53b94f7ce0</td>\n",
       "      <td>They would have piles of filthy meat on the fl...</td>\n",
       "      <td>0.328739</td>\n",
       "      <td>-1.053294</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff7c7e70df07</td>\n",
       "      <td>They used all sorts of chemical concoctions to...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>fffbccfd8a08</td>\n",
       "      <td>The meat would smell sour but the would \"rub i...</td>\n",
       "      <td>1.771596</td>\n",
       "      <td>0.547742</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7165 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt_id                                    prompt_question  \\\n",
       "0       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "1       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "2       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "3       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "4       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "...        ...                                                ...   \n",
       "7160    ebad26  Summarize the various ways the factory would u...   \n",
       "7161    ebad26  Summarize the various ways the factory would u...   \n",
       "7162    ebad26  Summarize the various ways the factory would u...   \n",
       "7163    ebad26  Summarize the various ways the factory would u...   \n",
       "7164    ebad26  Summarize the various ways the factory would u...   \n",
       "\n",
       "                 prompt_title  \\\n",
       "0                  On Tragedy   \n",
       "1                  On Tragedy   \n",
       "2                  On Tragedy   \n",
       "3                  On Tragedy   \n",
       "4                  On Tragedy   \n",
       "...                       ...   \n",
       "7160  Excerpt from The Jungle   \n",
       "7161  Excerpt from The Jungle   \n",
       "7162  Excerpt from The Jungle   \n",
       "7163  Excerpt from The Jungle   \n",
       "7164  Excerpt from The Jungle   \n",
       "\n",
       "                                            prompt_text    student_id  \\\n",
       "0     Chapter 13 \\r\\nAs the sequel to what has alrea...  00791789cc1f   \n",
       "1     Chapter 13 \\r\\nAs the sequel to what has alrea...  0086ef22de8f   \n",
       "2     Chapter 13 \\r\\nAs the sequel to what has alrea...  0094589c7a22   \n",
       "3     Chapter 13 \\r\\nAs the sequel to what has alrea...  00cd5736026a   \n",
       "4     Chapter 13 \\r\\nAs the sequel to what has alrea...  00d98b8ff756   \n",
       "...                                                 ...           ...   \n",
       "7160  With one member trimming beef in a cannery, an...  ff37545b2805   \n",
       "7161  With one member trimming beef in a cannery, an...  ff4ed38ef099   \n",
       "7162  With one member trimming beef in a cannery, an...  ff53b94f7ce0   \n",
       "7163  With one member trimming beef in a cannery, an...  ff7c7e70df07   \n",
       "7164  With one member trimming beef in a cannery, an...  fffbccfd8a08   \n",
       "\n",
       "                                                   text   content   wording  \\\n",
       "0     1 element of an ideal tragedy is that it shoul... -0.210614 -0.471415   \n",
       "1     The three elements of an ideal tragedy are:  H... -0.970237 -0.417058   \n",
       "2     Aristotle states that an ideal tragedy should ... -0.387791 -0.584181   \n",
       "3     One element of an Ideal tragedy is having a co...  0.088882 -0.594710   \n",
       "4     The 3 ideal of tragedy is how complex you need... -0.687288 -0.460886   \n",
       "...                                                 ...       ...       ...   \n",
       "7160  In paragraph two, they would use pickle meat a...  1.520355 -0.292990   \n",
       "7161  in the first paragraph  it says \"either can it... -1.204574 -1.169784   \n",
       "7162  They would have piles of filthy meat on the fl...  0.328739 -1.053294   \n",
       "7163  They used all sorts of chemical concoctions to...  0.205683  0.380538   \n",
       "7164  The meat would smell sour but the would \"rub i...  1.771596  0.547742   \n",
       "\n",
       "      fold  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "...    ...  \n",
       "7160     3  \n",
       "7161     3  \n",
       "7162     3  \n",
       "7163     3  \n",
       "7164     3  \n",
       "\n",
       "[7165 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2bILjGET86jN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vpC_oJw86jO"
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3p6AtTj_86jO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/deberta-v3-large/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f054c453b80>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 068386ae-1672-4ba6-bacd-8a3ad46acbc7)')' thrown while requesting HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSHcmlTt86jP"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X-XxwPIR86jP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_lm_datacollator = DataCollatorForWholeWordMask(tokenizer)\n",
    "def data_collator(batch):\n",
    "    input_ids = [{'input_ids':i[0]} for i in batch]\n",
    "    token_type_ids = [i[1] for i in batch]\n",
    "    attention_mask = [i[2] for i in batch]\n",
    "    labels = [i[3] for i in batch]\n",
    "    masked_input = mask_lm_datacollator(input_ids)['input_ids']\n",
    "    return masked_input,\\\n",
    "               torch.stack(token_type_ids),\\\n",
    "               torch.stack(attention_mask),\\\n",
    "               torch.stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.prompt_title = df['prompt_title'].values.astype(str)\n",
    "        self.prompt_text = df['prompt_text'].values.astype(str)\n",
    "        self.prompt_question = df['prompt_question'].values.astype(str)\n",
    "        self.text = df['text'].values.astype(str)\n",
    "        self.content = df['content'].values\n",
    "        self.wording = df['wording'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.prompt_title)\n",
    "    \n",
    "    def tokenize(self, example):\n",
    "        sep = self.tokenizer.sep_token\n",
    "        if  CFG.input_type == '1':\n",
    "            prompt = sep.join([example[\"prompt_title\"], example[\"prompt_text\"], example[\"prompt_question\"]])\n",
    "        else:\n",
    "            prompt = example[\"prompt_question\"] \n",
    "        \n",
    "        labels = [float(example[\"content\"]), float(example[\"wording\"])]\n",
    "\n",
    "        tokenized = tokenizer(\n",
    "            example[\"text\"],\n",
    "            prompt,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=CFG.max_input_length,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        example = {\n",
    "                    \"prompt_title\":self.prompt_title[item],\n",
    "                    \"prompt_text\":self.prompt_text[item],\n",
    "                    \"prompt_question\":self.prompt_question[item],\n",
    "                    \"text\":self.text[item],\n",
    "                    \"content\":self.content[item],\n",
    "                    \"wording\":self.wording[item],\n",
    "                  }\n",
    "        \n",
    "        out = self.tokenize(example)\n",
    "       \n",
    "        return {\n",
    "                'input_ids': torch.as_tensor(out['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids': torch.as_tensor(out['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask': torch.as_tensor(out['attention_mask'], dtype=torch.long),\n",
    "                'labels': torch.as_tensor(out['labels'], dtype=torch.float),\n",
    "        }\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP7TFS-c86jQ"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0A7qv3qn86jQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_params(module_lst):\n",
    "    for module in module_lst:\n",
    "        for param in module.parameters():\n",
    "            if param.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "    return\n",
    "\n",
    "class Custom_Bert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\"output_hidden_states\":True})\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "\n",
    "        dim = config.hidden_size\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        n_weights = 24\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(dim,1)\n",
    "        )\n",
    "        init_params([self.cls,self.attention])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                )\n",
    "\n",
    "        cls_outputs = torch.stack(\n",
    "            [self.dropout(layer) for layer in base_output['hidden_states'][-24:]], dim=0\n",
    "        )\n",
    "        cls_output = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * cls_outputs).sum(0)\n",
    "\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [torch.sum(self.attention(self.high_dropout(cls_output)) * cls_output, dim=1) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        output = self.cls(logits)\n",
    "        if labels is None:\n",
    "            return output\n",
    "\n",
    "        else:\n",
    "            return (nn.MSELoss()(torch.squeeze(output,1),labels), output)\n",
    "\n",
    "\n",
    "class Custom_Bert_Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "        dim = config.hidden_size\n",
    "        self.dropout = nn.Dropout(p=0)\n",
    "        self.cls = nn.Linear(dim,2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids\n",
    "                               )\n",
    "        output = base_output.last_hidden_state\n",
    "        output = self.cls(torch.mean(output, dim=1))\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=nn.MSELoss()(output,labels),\n",
    "            logits=output, \n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "\n",
    "class GeMText(nn.Module):\n",
    "    def __init__(self, dim = 1, p=3, eps=1e-6):\n",
    "        super(GeMText, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.feat_mult = 1\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n",
    "        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n",
    "        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n",
    "        ret = ret.pow(1 / self.p)\n",
    "        return ret    \n",
    "\n",
    "class Custom_Bert_Pool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        self.config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "        #self.base = AutoModel.from_pretrained(CFG.model_path, config=self.config)\n",
    "        print('load pretrained model ...');\n",
    "        self.base = AutoModel.from_pretrained('./input/pretrain/pretrained_model', config = self.config)\n",
    "        \n",
    "        self.pool = GeMText()\n",
    "        self.cls = nn.Linear(self.config.hidden_size,2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids\n",
    "                               )\n",
    "        output = base_output.last_hidden_state\n",
    "        output = self.cls(self.pool(output, attention_mask))\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=nn.SmoothL1Loss()(output,labels),\n",
    "            logits=output, \n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "class Custom_Bert_Mean(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.output_hidden_states=True\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "        dim = config.hidden_size\n",
    "        self.dropout = nn.Dropout(p=0)\n",
    "        self.cls = nn.Linear(dim,1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,labels=None):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                            )\n",
    "\n",
    "\n",
    "        output = base_output.hidden_states[-1]\n",
    "        output = self.cls(self.dropout(torch.mean(output, dim=1)))\n",
    "        if labels is None:\n",
    "            return output\n",
    "\n",
    "        else:\n",
    "            return (nn.MSELoss()(torch.squeeze(output,1),labels), output)\n",
    "\n",
    "class Custom_Bert_M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\"output_hidden_states\":True})\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "\n",
    "        dim = config.hidden_size\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        n_weights = 24\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.cls_0 = nn.Sequential(\n",
    "            nn.Linear(dim,1)\n",
    "        )\n",
    "\n",
    "        self.cls_1 = nn.Linear(dim,5)\n",
    "        init_params([self.cls_0,self.cls_1,self.attention])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                             )\n",
    "\n",
    "        cls_outputs = torch.stack(\n",
    "            [self.dropout(layer) for layer in base_output['hidden_states'][-24:]], dim=0\n",
    "        )\n",
    "        cls_output = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * cls_outputs).sum(0)\n",
    "\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [torch.sum(self.attention(self.high_dropout(cls_output)) * cls_output, dim=1) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        output_0 = self.cls_0(logits)\n",
    "        output_1 = self.cls_1(logits)\n",
    "        if labels is None:\n",
    "            return output_0\n",
    "\n",
    "        else:\n",
    "            regression_loss = nn.MSELoss()(torch.squeeze(output_0,1),labels)\n",
    "            labels = labels.double()\n",
    "            cls_labels = torch.where(labels==1.,4.0,labels)\n",
    "            cls_labels = torch.where(cls_labels==0.25,1.0,cls_labels)\n",
    "            cls_labels = torch.where(cls_labels==0.5,2.0,cls_labels)\n",
    "            cls_labels = torch.where(cls_labels==0.75,3.0,cls_labels)\n",
    "            cls_labels = cls_labels.long()\n",
    "            cls_loss = nn.CrossEntropyLoss()(output_1, cls_labels)\n",
    "            return ( 0.8 * regression_loss + 0.2 * cls_loss, output_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    if CFG.model_type == 'base':\n",
    "        model_config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        model_config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "\n",
    "        #print(model_config)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            CFG.model_path, config=model_config\n",
    "        )\n",
    "    if CFG.model_type == 'simple':\n",
    "        model = Custom_Bert_Simple()\n",
    "    if CFG.model_type == 'pool':\n",
    "        model = Custom_Bert_Pool()\n",
    "        if CFG.reinit_layers > 0:\n",
    "            print(\"==\"*40)\n",
    "            print(f\"Reinitialize the last {CFG.reinit_layers} layer(s).\")\n",
    "            for layer in model.base.encoder.layer[-CFG.reinit_layers:]:\n",
    "                print(\"===\")\n",
    "                layer.apply(model._init_weights)\n",
    "            print(\"==\"*40)\n",
    "        if CFG.load_pretrained:\n",
    "            model.load_state_dict(torch.load('./pretrained/microsoft_deberta-v3-base_best_ema.pth')['model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op1OHevU86jR"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "class ModelEMA:\n",
    "    \"\"\"Model Exponential Moving Average from https://github.com/rwightman/\n",
    "    pytorch-image-models Keep a moving average of everything in the model\n",
    "    state_dict (parameters and buffers).\n",
    "\n",
    "    This is intended to allow functionality like\n",
    "    https://www.tensorflow.org/api_docs/python/tf/train/\n",
    "    ExponentialMovingAverage\n",
    "    A smoothed version of the weights is necessary for some training\n",
    "    schemes to perform well.\n",
    "    This class is sensitive where it is initialized in the sequence\n",
    "    of model init, GPU assignment and distributed training wrappers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay=0.9999, updates=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (nn.Module): model to apply EMA.\n",
    "            decay (float): ema decay reate.\n",
    "            updates (int): counter of EMA updates.\n",
    "        \"\"\"\n",
    "        # Create EMA(FP32)\n",
    "        self.ema_model = deepcopy(model).eval()\n",
    "        self.ema = self.ema_model\n",
    "        self.updates = updates\n",
    "        # decay exponential ramp (to help early epochs)\n",
    "        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        # Update EMA parameters\n",
    "        with torch.no_grad():\n",
    "            self.updates += 1\n",
    "            d = self.decay(self.updates)\n",
    "            msd =  model.state_dict()# model state_dict\n",
    "            for k, v in self.ema.state_dict().items():\n",
    "                if v.dtype.is_floating_point:\n",
    "                    v *= d\n",
    "                    v += (1.0 - d) * msd[k].detach()\n",
    "\n",
    "class EMAHook:\n",
    "    \"\"\"EMAHook used in BEVDepth.\n",
    "\n",
    "    Modified from https://github.com/Megvii-Base\n",
    "    Detection/BEVDepth/blob/main/callbacks/ema.py.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, init_updates=0, decay=0.9990, resume=None, logger=None):\n",
    "        super().__init__()\n",
    "        self.init_updates = init_updates\n",
    "        self.resume = resume\n",
    "        self.decay = decay\n",
    "        self.ema_model = self.before_run(model)\n",
    "        self.logger = logger\n",
    "\n",
    "    def before_run(self, model):\n",
    "        from torch.nn.modules.batchnorm import SyncBatchNorm\n",
    "\n",
    "        bn_model_list = list()\n",
    "        bn_model_dist_group_list = list()\n",
    "        for model_ref in model.modules():\n",
    "            if isinstance(model_ref, SyncBatchNorm):\n",
    "                bn_model_list.append(model_ref)\n",
    "                bn_model_dist_group_list.append(model_ref.process_group)\n",
    "                model_ref.process_group = None\n",
    "        ema_model = ModelEMA(model, self.decay)\n",
    "\n",
    "        for bn_model, dist_group in zip(bn_model_list,\n",
    "                                        bn_model_dist_group_list):\n",
    "            bn_model.process_group = dist_group\n",
    "        ema_model.updates = self.init_updates\n",
    "\n",
    "        if self.resume is not None:\n",
    "            self.logger.info(f'resume ema checkpoint from {self.resume}')\n",
    "            cpt = torch.load(self.resume, map_location='cpu')\n",
    "            load_state_dict(ema_model.ema, cpt['state_dict'])\n",
    "            ema_model.updates = cpt['updates']\n",
    "\n",
    "        return ema_model\n",
    "\n",
    "    def after_train_iter(self, model):\n",
    "        self.ema_model.update(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "O__vnlBV86jR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qYJKjn8H86jR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.discriminative_learning_rate_num_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_llr_params(model, type='s'):\n",
    "    \"\"\"\n",
    "    Setup the optimizer.\n",
    "    We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
    "    Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
    "\n",
    "    MODIFIED VERSION:\n",
    "    * added support for differential learning rates per layer\n",
    "\n",
    "    reference: https://github.com/huggingface/transformers/blob/05fa1a7ac17bb7aa07b9e0c1e138ecb31a28bbfe/src/transformers/trainer.py#L804\n",
    "    \"\"\"\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
    "    ### ADDED\n",
    "    if CFG.discriminative_learning_rate:\n",
    "\n",
    "        num_layers = model.config.num_hidden_layers\n",
    "\n",
    "        learning_rate_powers = range(0, num_layers, num_layers//CFG.discriminative_learning_rate_num_groups)\n",
    "        layer_wise_learning_rates = [\n",
    "            pow(CFG.discriminative_learning_rate_decay_rate, power) * CFG.encoder_lr \n",
    "            for power in learning_rate_powers \n",
    "            for _ in range(num_layers//CFG.discriminative_learning_rate_num_groups)\n",
    "          ]\n",
    "        layer_wise_learning_rates = layer_wise_learning_rates[::-1]\n",
    "        print('Layer-wise learning rates:', layer_wise_learning_rates)\n",
    "\n",
    "        # group embedding paramters from the transformer encoder\n",
    "        embedding_layer = model.base.embeddings\n",
    "        optimizer_grouped_parameters = [\n",
    "          {\n",
    "              \"params\": [p for n, p in embedding_layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "              \"lr\": pow(CFG.discriminative_learning_rate_decay_rate, num_layers) * CFG.encoder_lr ,\n",
    "              \"weight_decay\": CFG.weight_decay,\n",
    "          },\n",
    "          {\n",
    "              \"params\": [p for n, p in embedding_layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "              \"lr\": pow(CFG.discriminative_learning_rate_decay_rate, num_layers) * CFG.encoder_lr ,\n",
    "              \"weight_decay\": 0.0,\n",
    "          },\n",
    "        ]\n",
    "\n",
    "        # group encoding paramters from the transformer encoder\n",
    "        encoding_layers = [layer for layer in model.base.encoder.layer]\n",
    "        for i, layer in enumerate(encoding_layers):\n",
    "            optimizer_grouped_parameters += [\n",
    "                {\n",
    "                    \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                    \"lr\": layer_wise_learning_rates[i],\n",
    "                    \"weight_decay\": CFG.weight_decay,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                    \"lr\": layer_wise_learning_rates[i],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "            ]    \n",
    "        print(f\"Detected unattached modules in model.encoder: {[n for n, p in model.base.encoder.named_parameters() if not n.startswith('layer')]}\")\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.base.encoder.named_parameters() if not n.startswith('layer') and not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": layer_wise_learning_rates[-1],\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.base.encoder.named_parameters() if not n.startswith('layer') and any(nd in n for nd in no_decay)],\n",
    "                \"lr\": layer_wise_learning_rates[-1],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # group paramters from the task specific head\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if 'base' not in n and not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.head_lr,\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if 'base' not in n and any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.head_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    ### END ADDED\n",
    "    else:\n",
    "        # group paramters for the entire network\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.encoder_lr,\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.encoder_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3HIHxVsj86jS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    start = end = time.time()\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        for key, value in batch.items():\n",
    "            batch[key] = value.to(device)\n",
    "        batch_size = batch['labels'].size(0)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**batch)\n",
    "        label = batch['labels']\n",
    "        loss, logits = model_output.loss, model_output.logits\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(logits.to('cpu').numpy())\n",
    "        labels.append(label.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, epoch, scheduler, device, valid_loader, start_time, best_score, best_score_ema,ema_hook,wandb, fold):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        for key, value in batch.items():\n",
    "            batch[key] = value.to(device)\n",
    "        batch_size = batch['labels'].size(0)\n",
    "        loss = model(**batch).loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        optimizer.step()\n",
    "        ema_hook.after_train_iter(model)\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        \n",
    "        wandb.log({\n",
    "                'train loss': loss.item(),\n",
    "                'step': global_step,\n",
    "                'epoch': epoch,\n",
    "                'fold': fold,\n",
    "                'batch_size':CFG.batch_size\n",
    "            })\n",
    "        \n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "            \n",
    "            # eval\n",
    "            avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, model, CFG.device)\n",
    "\n",
    "            # scoring\n",
    "            score = compute_mcrmse((predictions, valid_labels))\n",
    "\n",
    "\n",
    "            content_rmse, wording_rmse, mcrmse = list(score.values())\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch + 1} avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - content_rmse: {content_rmse:.4f} - wording_rmse: {wording_rmse:.4f} - mcrmse: {mcrmse:.4f}')\n",
    "            \n",
    "            \n",
    "            if best_score > score['mcrmse']:\n",
    "                if best_score != float('inf'):\n",
    "                    os.remove(CFG.OUTPUT_DIR + \"{}_best{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score))\n",
    "                best_score = score['mcrmse']\n",
    "                best_predictions = predictions\n",
    "                LOGGER.info(f'Epoch {epoch + 1} - Save Best Score: {best_score:.4f} Model')\n",
    "                torch.save({'model': model.state_dict(),\n",
    "                            'predictions': predictions},\n",
    "                           CFG.OUTPUT_DIR + \"{}_best{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score))\n",
    "            \n",
    "            \n",
    "            avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, ema_hook.ema_model.ema, CFG.device)\n",
    "            # ema scoring\n",
    "            ema_score = compute_mcrmse((predictions, valid_labels))\n",
    "\n",
    "\n",
    "            content_rmse, wording_rmse, mcrmse = list(ema_score.values())\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch + 1} avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - ema_content_rmse: {content_rmse:.4f} - ema_wording_rmse: {wording_rmse:.4f} - ema_mcrmse: {mcrmse:.4f}')\n",
    "            \n",
    "            \n",
    "            if best_score_ema > ema_score['mcrmse']:\n",
    "                if best_score_ema != float('inf'):\n",
    "                    os.remove(CFG.OUTPUT_DIR + \"{}_best_ema{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score_ema))\n",
    "                best_score_ema = ema_score['mcrmse']\n",
    "                best_predictions = predictions\n",
    "                LOGGER.info(f'Epoch {epoch + 1} - ema_Save Best Score: {best_score_ema:.4f} Model')\n",
    "                torch.save({'model': ema_hook.ema_model.ema.state_dict(),\n",
    "                            'predictions': predictions},\n",
    "                           CFG.OUTPUT_DIR + \"{}_best_ema{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold,best_score_ema))\n",
    "            \n",
    "            wandb.log({\n",
    "            'learning rate': optimizer.param_groups[0]['lr'],\n",
    "            'validation mcrmse': score['mcrmse'],\n",
    "            'validation ema mcrmse': ema_score['mcrmse'],\n",
    "            'step': global_step,\n",
    "            'epoch': epoch,\n",
    "        })\n",
    "            \n",
    "            model.train()\n",
    "    return losses.avg, best_score, best_score_ema\n",
    "\n",
    "\n",
    "\n",
    "def train_loop():\n",
    "    LOGGER.info(f\"========== training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    wandb.init(project='kaggle-commonlit-eval-student-summaries-0809')\n",
    "    wandb.config = dict(epochs=CFG.epochs, \n",
    "                            batch_size=CFG.batch_size, \n",
    "                            learning_rate=CFG.encoder_lr,\n",
    "                            save_checkpoint=True,\n",
    "                            )\n",
    "    for fold in CFG.folds:\n",
    "        \n",
    "        if CFG.pretraining:\n",
    "            tr_data = pd.read_csv('tmp_pessudo.csv')\n",
    "            tr_data['prompt_title'] = ''\n",
    "            tr_data = tr_data[-(tr_data['prompt_question'].isin(pdf['prompt_question'].tolist()))]\n",
    "            va_data = df #df[df['fold']==fold].reset_index(drop=True)\n",
    "        else:\n",
    "            tr_data = df[df['fold']!=fold].reset_index(drop=True)\n",
    "            va_data = df[df['fold']==fold].reset_index(drop=True)\n",
    "        train_dataset = TrainDataset(tr_data, tokenizer)\n",
    "        valid_dataset = TrainDataset(va_data, tokenizer)\n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                                  batch_size=CFG.batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "        valid_loader = DataLoader(valid_dataset,\n",
    "                                  batch_size=CFG.batch_size * 2,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "        # ====================================================\n",
    "        # model & optimizer\n",
    "        # ====================================================\n",
    "        model = build_model()\n",
    "        #model = AutoModelForSequenceClassification.from_pretrained(CFG.model_path, num_labels=1)\n",
    "        model.to(CFG.device)\n",
    "        # for param in model.base.parameters():\n",
    "        #         param.requires_grad = False\n",
    "        ema_hook = EMAHook(model, init_updates=3000, logger=LOGGER)\n",
    "        def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "            param_optimizer = list(model.named_parameters())\n",
    "            no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_parameters = [\n",
    "                {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                 'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "                {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                 'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            ]\n",
    "            return optimizer_parameters\n",
    "\n",
    "        optimizer_parameters = get_optimizer_llr_params(model)\n",
    "        optimizer = AdamW(optimizer_parameters, eps=CFG.eps, betas=CFG.betas)\n",
    "\n",
    "\n",
    "        \n",
    "        # ====================================================\n",
    "        # scheduler\n",
    "        # ====================================================\n",
    "        def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "            cfg.num_warmup_steps = cfg.num_warmup_steps * num_train_steps\n",
    "            if cfg.scheduler == 'linear':\n",
    "                scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "                )\n",
    "            elif cfg.scheduler == 'cosine':\n",
    "                scheduler = get_cosine_schedule_with_warmup(\n",
    "                    optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps,\n",
    "                    num_cycles=cfg.num_cycles\n",
    "                )\n",
    "            return scheduler\n",
    "\n",
    "        num_train_steps = int(len(train_dataset) / CFG.batch_size * CFG.epochs)\n",
    "        scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "        # ====================================================\n",
    "        # loop\n",
    "        # ====================================================\n",
    "        # criterion = torch.nn.CrossEntropyLoss(ignore_index=- 1)\n",
    "\n",
    "        # criterion = LabelSmoothingLoss()\n",
    "        best_score = float('inf')\n",
    "        best_score_ema = float('inf')\n",
    "        for epoch in range(CFG.epochs):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # train\n",
    "            avg_loss, best_score, best_score_ema = train_fn(train_loader, model, optimizer, epoch, scheduler, CFG.device, valid_loader, start_time, best_score, best_score_ema ,ema_hook, wandb,fold)\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        del scheduler, optimizer, model\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbgHecjCyz5D",
    "outputId": "09f6fba5-dd11-4adc-a85c-8805c6d6c90d",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== training ==========\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeng_sun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5010759289b24093ada6e4fc62d48f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669741932613155, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/workspace/commonlit/wandb/run-20230909_224859-zls2li2s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-0809/runs/zls2li2s' target=\"_blank\">silvery-frost-4</a></strong> to <a href='https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-0809' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-0809' target=\"_blank\">https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-0809</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-0809/runs/zls2li2s' target=\"_blank\">https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-0809/runs/zls2li2s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/deberta-v3-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f0512bd0790>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 9ade698c-942f-4420-8831-7f9e5766553f)')' thrown while requesting HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained model ...\n",
      "Epoch: [1][0/1289] Elapsed 0m 1s (remain 29m 11s) Loss: 0.2912(0.2912) Grad: 8.8301  LR: 0.00000500  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.5114  time: 94s\n",
      "Epoch 1 - content_rmse: 1.2331 - wording_rmse: 0.9461 - mcrmse: 1.0896\n",
      "Epoch 1 - Save Best Score: 1.0896 Model\n",
      "Epoch 1 avg_val_loss: 0.5233  time: 191s\n",
      "Epoch 1 - ema_content_rmse: 1.2552 - ema_wording_rmse: 0.9529 - ema_mcrmse: 1.1041\n",
      "Epoch 1 - ema_Save Best Score: 1.1041 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][100/1289] Elapsed 4m 12s (remain 49m 24s) Loss: 0.1044(0.2803) Grad: 5.9134  LR: 0.00000500  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2557  time: 345s\n",
      "Epoch 1 - content_rmse: 0.6624 - wording_rmse: 0.8254 - mcrmse: 0.7439\n",
      "Epoch 1 - Save Best Score: 0.7439 Model\n",
      "Epoch 1 avg_val_loss: 0.2767  time: 441s\n",
      "Epoch 1 - ema_content_rmse: 0.6982 - ema_wording_rmse: 0.8530 - ema_mcrmse: 0.7756\n",
      "Epoch 1 - ema_Save Best Score: 0.7756 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][200/1289] Elapsed 8m 23s (remain 45m 24s) Loss: 0.1550(0.2195) Grad: 6.6780  LR: 0.00000498  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2274  time: 597s\n",
      "Epoch 1 - content_rmse: 0.5760 - wording_rmse: 0.8254 - mcrmse: 0.7007\n",
      "Epoch 1 - Save Best Score: 0.7007 Model\n",
      "Epoch 1 avg_val_loss: 0.2569  time: 692s\n",
      "Epoch 1 - ema_content_rmse: 0.6322 - ema_wording_rmse: 0.8681 - ema_mcrmse: 0.7502\n",
      "Epoch 1 - ema_Save Best Score: 0.7502 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][300/1289] Elapsed 12m 33s (remain 41m 14s) Loss: 0.0834(0.1942) Grad: 3.8660  LR: 0.00000496  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2622  time: 847s\n",
      "Epoch 1 - content_rmse: 0.5817 - wording_rmse: 0.9270 - mcrmse: 0.7544\n",
      "Epoch 1 avg_val_loss: 0.3027  time: 940s\n",
      "Epoch 1 - ema_content_rmse: 0.6415 - ema_wording_rmse: 0.9914 - ema_mcrmse: 0.8164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][400/1289] Elapsed 16m 38s (remain 36m 51s) Loss: 0.1736(0.1798) Grad: 5.0381  LR: 0.00000493  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2336  time: 1092s\n",
      "Epoch 1 - content_rmse: 0.5852 - wording_rmse: 0.8437 - mcrmse: 0.7144\n",
      "Epoch 1 avg_val_loss: 0.2334  time: 1185s\n",
      "Epoch 1 - ema_content_rmse: 0.5892 - ema_wording_rmse: 0.8377 - ema_mcrmse: 0.7135\n",
      "Epoch 1 - ema_Save Best Score: 0.7135 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][500/1289] Elapsed 20m 46s (remain 32m 40s) Loss: 0.4088(0.1695) Grad: 12.1389  LR: 0.00000488  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1979  time: 1340s\n",
      "Epoch 1 - content_rmse: 0.5171 - wording_rmse: 0.7834 - mcrmse: 0.6502\n",
      "Epoch 1 - Save Best Score: 0.6502 Model\n",
      "Epoch 1 avg_val_loss: 0.1948  time: 1436s\n",
      "Epoch 1 - ema_content_rmse: 0.5159 - ema_wording_rmse: 0.7713 - ema_mcrmse: 0.6436\n",
      "Epoch 1 - ema_Save Best Score: 0.6436 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][600/1289] Elapsed 24m 57s (remain 28m 34s) Loss: 0.0755(0.1614) Grad: 4.0893  LR: 0.00000483  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2523  time: 1590s\n",
      "Epoch 1 - content_rmse: 0.5381 - wording_rmse: 0.9592 - mcrmse: 0.7486\n",
      "Epoch 1 avg_val_loss: 0.2903  time: 1684s\n",
      "Epoch 1 - ema_content_rmse: 0.6422 - ema_wording_rmse: 0.9896 - ema_mcrmse: 0.8159\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][700/1289] Elapsed 29m 2s (remain 24m 21s) Loss: 0.1455(0.1564) Grad: 4.1415  LR: 0.00000478  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2391  time: 1835s\n",
      "Epoch 1 - content_rmse: 0.6170 - wording_rmse: 0.8403 - mcrmse: 0.7286\n",
      "Epoch 1 avg_val_loss: 0.2316  time: 1928s\n",
      "Epoch 1 - ema_content_rmse: 0.5940 - ema_wording_rmse: 0.8358 - ema_mcrmse: 0.7149\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][800/1289] Elapsed 33m 7s (remain 20m 10s) Loss: 0.0195(0.1517) Grad: 3.5119  LR: 0.00000471  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2194  time: 2081s\n",
      "Epoch 1 - content_rmse: 0.5230 - wording_rmse: 0.8756 - mcrmse: 0.6993\n",
      "Epoch 1 avg_val_loss: 0.2339  time: 2174s\n",
      "Epoch 1 - ema_content_rmse: 0.5441 - ema_wording_rmse: 0.9050 - ema_mcrmse: 0.7245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][900/1289] Elapsed 37m 12s (remain 16m 1s) Loss: 0.0293(0.1491) Grad: 1.5222  LR: 0.00000463  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2338  time: 2325s\n",
      "Epoch 1 - content_rmse: 0.5991 - wording_rmse: 0.8500 - mcrmse: 0.7245\n",
      "Epoch 1 avg_val_loss: 0.2239  time: 2419s\n",
      "Epoch 1 - ema_content_rmse: 0.5721 - ema_wording_rmse: 0.8403 - ema_mcrmse: 0.7062\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1000/1289] Elapsed 41m 17s (remain 11m 52s) Loss: 0.1400(0.1448) Grad: 8.4428  LR: 0.00000455  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2086  time: 2570s\n",
      "Epoch 1 - content_rmse: 0.5301 - wording_rmse: 0.8257 - mcrmse: 0.6779\n",
      "Epoch 1 avg_val_loss: 0.2561  time: 2664s\n",
      "Epoch 1 - ema_content_rmse: 0.5685 - ema_wording_rmse: 0.9460 - ema_mcrmse: 0.7573\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1100/1289] Elapsed 45m 22s (remain 7m 44s) Loss: 0.1973(0.1414) Grad: 9.4114  LR: 0.00000446  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1748  time: 2816s\n",
      "Epoch 1 - content_rmse: 0.4965 - wording_rmse: 0.7247 - mcrmse: 0.6106\n",
      "Epoch 1 - Save Best Score: 0.6106 Model\n",
      "Epoch 1 avg_val_loss: 0.1791  time: 2912s\n",
      "Epoch 1 - ema_content_rmse: 0.5019 - ema_wording_rmse: 0.7401 - ema_mcrmse: 0.6210\n",
      "Epoch 1 - ema_Save Best Score: 0.6210 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1200/1289] Elapsed 49m 33s (remain 3m 37s) Loss: 0.0915(0.1385) Grad: 4.8777  LR: 0.00000436  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1900  time: 3067s\n",
      "Epoch 1 - content_rmse: 0.4996 - wording_rmse: 0.7822 - mcrmse: 0.6409\n",
      "Epoch 1 avg_val_loss: 0.1953  time: 3160s\n",
      "Epoch 1 - ema_content_rmse: 0.5140 - ema_wording_rmse: 0.7880 - ema_mcrmse: 0.6510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1288/1289] Elapsed 53m 31s (remain 0m 0s) Loss: 0.0167(0.1361) Grad: 1.3190  LR: 0.00000427  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1792  time: 3305s\n",
      "Epoch 1 - content_rmse: 0.5162 - wording_rmse: 0.7213 - mcrmse: 0.6188\n",
      "Epoch 1 avg_val_loss: 0.1957  time: 3398s\n",
      "Epoch 1 - ema_content_rmse: 0.5230 - ema_wording_rmse: 0.7783 - ema_mcrmse: 0.6506\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/1289] Elapsed 0m 0s (remain 16m 52s) Loss: 0.0877(0.0877) Grad: 4.1368  LR: 0.00000427  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1804  time: 94s\n",
      "Epoch 2 - content_rmse: 0.5189 - wording_rmse: 0.7232 - mcrmse: 0.6210\n",
      "Epoch 2 avg_val_loss: 0.1934  time: 187s\n",
      "Epoch 2 - ema_content_rmse: 0.5223 - ema_wording_rmse: 0.7706 - ema_mcrmse: 0.6465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][100/1289] Elapsed 4m 6s (remain 48m 14s) Loss: 0.0650(0.0851) Grad: 3.9025  LR: 0.00000416  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2194  time: 339s\n",
      "Epoch 2 - content_rmse: 0.5136 - wording_rmse: 0.8677 - mcrmse: 0.6906\n",
      "Epoch 2 avg_val_loss: 0.2348  time: 432s\n",
      "Epoch 2 - ema_content_rmse: 0.5727 - ema_wording_rmse: 0.8630 - ema_mcrmse: 0.7178\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][200/1289] Elapsed 8m 11s (remain 44m 18s) Loss: 0.1204(0.0883) Grad: 8.4377  LR: 0.00000404  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2327  time: 584s\n",
      "Epoch 2 - content_rmse: 0.5531 - wording_rmse: 0.8916 - mcrmse: 0.7223\n",
      "Epoch 2 avg_val_loss: 0.2235  time: 677s\n",
      "Epoch 2 - ema_content_rmse: 0.5396 - ema_wording_rmse: 0.8757 - ema_mcrmse: 0.7077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][300/1289] Elapsed 12m 16s (remain 40m 15s) Loss: 0.0431(0.0907) Grad: 2.7832  LR: 0.00000392  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1823  time: 829s\n",
      "Epoch 2 - content_rmse: 0.5110 - wording_rmse: 0.7437 - mcrmse: 0.6273\n",
      "Epoch 2 avg_val_loss: 0.1961  time: 922s\n",
      "Epoch 2 - ema_content_rmse: 0.5179 - ema_wording_rmse: 0.7958 - ema_mcrmse: 0.6569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][400/1289] Elapsed 16m 21s (remain 36m 12s) Loss: 0.0767(0.0898) Grad: 3.2480  LR: 0.00000379  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2244  time: 1074s\n",
      "Epoch 2 - content_rmse: 0.5439 - wording_rmse: 0.8605 - mcrmse: 0.7022\n",
      "Epoch 2 avg_val_loss: 0.1882  time: 1167s\n",
      "Epoch 2 - ema_content_rmse: 0.5039 - ema_wording_rmse: 0.7698 - ema_mcrmse: 0.6368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][500/1289] Elapsed 20m 26s (remain 32m 8s) Loss: 0.0918(0.0871) Grad: 3.3536  LR: 0.00000365  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2143  time: 1319s\n",
      "Epoch 2 - content_rmse: 0.5320 - wording_rmse: 0.8556 - mcrmse: 0.6938\n",
      "Epoch 2 avg_val_loss: 0.2093  time: 1412s\n",
      "Epoch 2 - ema_content_rmse: 0.5251 - ema_wording_rmse: 0.8439 - ema_mcrmse: 0.6845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][600/1289] Elapsed 24m 31s (remain 28m 3s) Loss: 0.1111(0.0852) Grad: 6.6944  LR: 0.00000352  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2876  time: 1564s\n",
      "Epoch 2 - content_rmse: 0.6234 - wording_rmse: 0.9952 - mcrmse: 0.8093\n",
      "Epoch 2 avg_val_loss: 0.2414  time: 1657s\n",
      "Epoch 2 - ema_content_rmse: 0.5697 - ema_wording_rmse: 0.9057 - ema_mcrmse: 0.7377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][700/1289] Elapsed 28m 36s (remain 23m 59s) Loss: 0.1247(0.0857) Grad: 4.5414  LR: 0.00000338  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2067  time: 1809s\n",
      "Epoch 2 - content_rmse: 0.4915 - wording_rmse: 0.8587 - mcrmse: 0.6751\n",
      "Epoch 2 avg_val_loss: 0.2050  time: 1902s\n",
      "Epoch 2 - ema_content_rmse: 0.4935 - ema_wording_rmse: 0.8509 - ema_mcrmse: 0.6722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][800/1289] Elapsed 32m 40s (remain 19m 54s) Loss: 0.0723(0.0859) Grad: 2.4786  LR: 0.00000323  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2245  time: 2054s\n",
      "Epoch 2 - content_rmse: 0.5278 - wording_rmse: 0.8902 - mcrmse: 0.7090\n",
      "Epoch 2 avg_val_loss: 0.2122  time: 2147s\n",
      "Epoch 2 - ema_content_rmse: 0.5135 - ema_wording_rmse: 0.8624 - ema_mcrmse: 0.6879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][900/1289] Elapsed 36m 45s (remain 15m 49s) Loss: 0.1031(0.0843) Grad: 5.8189  LR: 0.00000309  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2146  time: 2299s\n",
      "Epoch 2 - content_rmse: 0.5012 - wording_rmse: 0.8709 - mcrmse: 0.6861\n",
      "Epoch 2 avg_val_loss: 0.2045  time: 2392s\n",
      "Epoch 2 - ema_content_rmse: 0.5075 - ema_wording_rmse: 0.8379 - ema_mcrmse: 0.6727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1000/1289] Elapsed 40m 50s (remain 11m 45s) Loss: 0.0555(0.0833) Grad: 3.7869  LR: 0.00000294  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1926  time: 2544s\n",
      "Epoch 2 - content_rmse: 0.4993 - wording_rmse: 0.8011 - mcrmse: 0.6502\n",
      "Epoch 2 avg_val_loss: 0.2029  time: 2637s\n",
      "Epoch 2 - ema_content_rmse: 0.5006 - ema_wording_rmse: 0.8386 - ema_mcrmse: 0.6696\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1100/1289] Elapsed 44m 55s (remain 7m 40s) Loss: 0.0631(0.0834) Grad: 3.3673  LR: 0.00000279  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2282  time: 2789s\n",
      "Epoch 2 - content_rmse: 0.5228 - wording_rmse: 0.9122 - mcrmse: 0.7175\n",
      "Epoch 2 avg_val_loss: 0.2115  time: 2882s\n",
      "Epoch 2 - ema_content_rmse: 0.5080 - ema_wording_rmse: 0.8653 - ema_mcrmse: 0.6867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1200/1289] Elapsed 49m 0s (remain 3m 35s) Loss: 0.0283(0.0833) Grad: 2.8944  LR: 0.00000263  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1976  time: 3034s\n",
      "Epoch 2 - content_rmse: 0.5084 - wording_rmse: 0.8150 - mcrmse: 0.6617\n",
      "Epoch 2 avg_val_loss: 0.2012  time: 3127s\n",
      "Epoch 2 - ema_content_rmse: 0.5080 - ema_wording_rmse: 0.8290 - ema_mcrmse: 0.6685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1288/1289] Elapsed 52m 58s (remain 0m 0s) Loss: 0.2433(0.0845) Grad: 7.2836  LR: 0.00000250  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1887  time: 3272s\n",
      "Epoch 2 - content_rmse: 0.5133 - wording_rmse: 0.7736 - mcrmse: 0.6435\n",
      "Epoch 2 avg_val_loss: 0.1932  time: 3365s\n",
      "Epoch 2 - ema_content_rmse: 0.5090 - ema_wording_rmse: 0.7965 - ema_mcrmse: 0.6527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/1289] Elapsed 0m 0s (remain 16m 58s) Loss: 0.2052(0.2052) Grad: 8.2187  LR: 0.00000250  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1902  time: 94s\n",
      "Epoch 3 - content_rmse: 0.5072 - wording_rmse: 0.7860 - mcrmse: 0.6466\n",
      "Epoch 3 avg_val_loss: 0.1930  time: 187s\n",
      "Epoch 3 - ema_content_rmse: 0.5089 - ema_wording_rmse: 0.7958 - ema_mcrmse: 0.6524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][100/1289] Elapsed 4m 5s (remain 48m 11s) Loss: 0.0080(0.0548) Grad: 1.3659  LR: 0.00000235  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1996  time: 339s\n",
      "Epoch 3 - content_rmse: 0.5112 - wording_rmse: 0.8165 - mcrmse: 0.6639\n",
      "Epoch 3 avg_val_loss: 0.1916  time: 432s\n",
      "Epoch 3 - ema_content_rmse: 0.5047 - ema_wording_rmse: 0.7919 - ema_mcrmse: 0.6483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][200/1289] Elapsed 8m 10s (remain 44m 17s) Loss: 0.0843(0.0563) Grad: 5.9269  LR: 0.00000219  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2054  time: 584s\n",
      "Epoch 3 - content_rmse: 0.5091 - wording_rmse: 0.8396 - mcrmse: 0.6744\n",
      "Epoch 3 avg_val_loss: 0.1942  time: 677s\n",
      "Epoch 3 - ema_content_rmse: 0.5084 - ema_wording_rmse: 0.7998 - ema_mcrmse: 0.6541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][300/1289] Elapsed 12m 15s (remain 40m 15s) Loss: 0.0232(0.0551) Grad: 0.8880  LR: 0.00000204  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1844  time: 829s\n",
      "Epoch 3 - content_rmse: 0.5021 - wording_rmse: 0.7637 - mcrmse: 0.6329\n",
      "Epoch 3 avg_val_loss: 0.1907  time: 922s\n",
      "Epoch 3 - ema_content_rmse: 0.5012 - ema_wording_rmse: 0.7916 - ema_mcrmse: 0.6464\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][400/1289] Elapsed 16m 21s (remain 36m 12s) Loss: 0.0536(0.0551) Grad: 5.3127  LR: 0.00000190  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1778  time: 1074s\n",
      "Epoch 3 - content_rmse: 0.4895 - wording_rmse: 0.7514 - mcrmse: 0.6205\n",
      "Epoch 3 avg_val_loss: 0.1855  time: 1167s\n",
      "Epoch 3 - ema_content_rmse: 0.4911 - ema_wording_rmse: 0.7817 - ema_mcrmse: 0.6364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][500/1289] Elapsed 20m 26s (remain 32m 8s) Loss: 0.0326(0.0552) Grad: 1.8196  LR: 0.00000175  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2164  time: 1319s\n",
      "Epoch 3 - content_rmse: 0.5021 - wording_rmse: 0.8861 - mcrmse: 0.6941\n",
      "Epoch 3 avg_val_loss: 0.2047  time: 1412s\n",
      "Epoch 3 - ema_content_rmse: 0.4987 - ema_wording_rmse: 0.8496 - ema_mcrmse: 0.6741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][600/1289] Elapsed 24m 31s (remain 28m 3s) Loss: 0.0912(0.0549) Grad: 5.6422  LR: 0.00000160  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1856  time: 1564s\n",
      "Epoch 3 - content_rmse: 0.4931 - wording_rmse: 0.7809 - mcrmse: 0.6370\n",
      "Epoch 3 avg_val_loss: 0.1935  time: 1657s\n",
      "Epoch 3 - ema_content_rmse: 0.4953 - ema_wording_rmse: 0.8094 - ema_mcrmse: 0.6524\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][700/1289] Elapsed 28m 36s (remain 23m 59s) Loss: 0.0272(0.0541) Grad: 2.0384  LR: 0.00000146  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1958  time: 1809s\n",
      "Epoch 3 - content_rmse: 0.4930 - wording_rmse: 0.8236 - mcrmse: 0.6583\n",
      "Epoch 3 avg_val_loss: 0.2014  time: 1903s\n",
      "Epoch 3 - ema_content_rmse: 0.4941 - ema_wording_rmse: 0.8420 - ema_mcrmse: 0.6680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][800/1289] Elapsed 32m 41s (remain 19m 54s) Loss: 0.0853(0.0547) Grad: 3.2022  LR: 0.00000133  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2189  time: 2054s\n",
      "Epoch 3 - content_rmse: 0.5018 - wording_rmse: 0.8973 - mcrmse: 0.6996\n",
      "Epoch 3 avg_val_loss: 0.2051  time: 2148s\n",
      "Epoch 3 - ema_content_rmse: 0.4952 - ema_wording_rmse: 0.8544 - ema_mcrmse: 0.6748\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][900/1289] Elapsed 36m 46s (remain 15m 50s) Loss: 0.0717(0.0545) Grad: 4.7546  LR: 0.00000120  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2101  time: 2299s\n",
      "Epoch 3 - content_rmse: 0.4972 - wording_rmse: 0.8683 - mcrmse: 0.6828\n",
      "Epoch 3 avg_val_loss: 0.1997  time: 2392s\n",
      "Epoch 3 - ema_content_rmse: 0.4939 - ema_wording_rmse: 0.8345 - ema_mcrmse: 0.6642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1000/1289] Elapsed 40m 51s (remain 11m 45s) Loss: 0.0470(0.0546) Grad: 2.6135  LR: 0.00000107  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1981  time: 2544s\n",
      "Epoch 3 - content_rmse: 0.4951 - wording_rmse: 0.8261 - mcrmse: 0.6606\n",
      "Epoch 3 avg_val_loss: 0.1984  time: 2638s\n",
      "Epoch 3 - ema_content_rmse: 0.4942 - ema_wording_rmse: 0.8297 - ema_mcrmse: 0.6619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1100/1289] Elapsed 44m 56s (remain 7m 40s) Loss: 0.0287(0.0550) Grad: 3.6719  LR: 0.00000095  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1895  time: 2789s\n",
      "Epoch 3 - content_rmse: 0.4917 - wording_rmse: 0.7979 - mcrmse: 0.6448\n",
      "Epoch 3 avg_val_loss: 0.1906  time: 2883s\n",
      "Epoch 3 - ema_content_rmse: 0.4912 - ema_wording_rmse: 0.8036 - ema_mcrmse: 0.6474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1200/1289] Elapsed 49m 1s (remain 3m 35s) Loss: 0.0332(0.0550) Grad: 3.6690  LR: 0.00000083  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1916  time: 3034s\n",
      "Epoch 3 - content_rmse: 0.4919 - wording_rmse: 0.8024 - mcrmse: 0.6472\n",
      "Epoch 3 avg_val_loss: 0.1950  time: 3127s\n",
      "Epoch 3 - ema_content_rmse: 0.4930 - ema_wording_rmse: 0.8139 - ema_mcrmse: 0.6535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1288/1289] Elapsed 52m 59s (remain 0m 0s) Loss: 0.0465(0.0550) Grad: 4.4955  LR: 0.00000073  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1941  time: 3272s\n",
      "Epoch 3 - content_rmse: 0.4931 - wording_rmse: 0.8153 - mcrmse: 0.6542\n",
      "Epoch 3 avg_val_loss: 0.1946  time: 3366s\n",
      "Epoch 3 - ema_content_rmse: 0.4922 - ema_wording_rmse: 0.8164 - ema_mcrmse: 0.6543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/1289] Elapsed 0m 0s (remain 16m 46s) Loss: 0.0246(0.0246) Grad: 2.5300  LR: 0.00000073  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1938  time: 94s\n",
      "Epoch 4 - content_rmse: 0.4930 - wording_rmse: 0.8144 - mcrmse: 0.6537\n",
      "Epoch 4 avg_val_loss: 0.1946  time: 187s\n",
      "Epoch 4 - ema_content_rmse: 0.4922 - ema_wording_rmse: 0.8164 - ema_mcrmse: 0.6543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][100/1289] Elapsed 4m 5s (remain 48m 11s) Loss: 0.0418(0.0379) Grad: 2.7967  LR: 0.00000063  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1970  time: 339s\n",
      "Epoch 4 - content_rmse: 0.4903 - wording_rmse: 0.8264 - mcrmse: 0.6584\n",
      "Epoch 4 avg_val_loss: 0.1938  time: 432s\n",
      "Epoch 4 - ema_content_rmse: 0.4908 - ema_wording_rmse: 0.8154 - ema_mcrmse: 0.6531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][200/1289] Elapsed 8m 10s (remain 44m 16s) Loss: 0.0821(0.0401) Grad: 2.8100  LR: 0.00000053  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.2079  time: 584s\n",
      "Epoch 4 - content_rmse: 0.4952 - wording_rmse: 0.8642 - mcrmse: 0.6797\n",
      "Epoch 4 avg_val_loss: 0.2052  time: 677s\n",
      "Epoch 4 - ema_content_rmse: 0.4955 - ema_wording_rmse: 0.8539 - ema_mcrmse: 0.6747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][300/1289] Elapsed 12m 15s (remain 40m 15s) Loss: 0.0374(0.0414) Grad: 2.5145  LR: 0.00000044  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1974  time: 829s\n",
      "Epoch 4 - content_rmse: 0.4920 - wording_rmse: 0.8280 - mcrmse: 0.6600\n",
      "Epoch 4 avg_val_loss: 0.1964  time: 922s\n",
      "Epoch 4 - ema_content_rmse: 0.4915 - ema_wording_rmse: 0.8242 - ema_mcrmse: 0.6578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][400/1289] Elapsed 16m 20s (remain 36m 12s) Loss: 0.0254(0.0405) Grad: 3.1591  LR: 0.00000036  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.2009  time: 1074s\n",
      "Epoch 4 - content_rmse: 0.4928 - wording_rmse: 0.8405 - mcrmse: 0.6667\n",
      "Epoch 4 avg_val_loss: 0.1996  time: 1167s\n",
      "Epoch 4 - ema_content_rmse: 0.4918 - ema_wording_rmse: 0.8359 - ema_mcrmse: 0.6638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][500/1289] Elapsed 20m 25s (remain 32m 8s) Loss: 0.0418(0.0399) Grad: 1.8376  LR: 0.00000028  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1970  time: 1319s\n",
      "Epoch 4 - content_rmse: 0.4922 - wording_rmse: 0.8262 - mcrmse: 0.6592\n",
      "Epoch 4 avg_val_loss: 0.1999  time: 1412s\n",
      "Epoch 4 - ema_content_rmse: 0.4923 - ema_wording_rmse: 0.8365 - ema_mcrmse: 0.6644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][600/1289] Elapsed 24m 30s (remain 28m 3s) Loss: 0.0376(0.0393) Grad: 2.9845  LR: 0.00000022  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.2014  time: 1564s\n",
      "Epoch 4 - content_rmse: 0.4924 - wording_rmse: 0.8414 - mcrmse: 0.6669\n",
      "Epoch 4 avg_val_loss: 0.1982  time: 1657s\n",
      "Epoch 4 - ema_content_rmse: 0.4921 - ema_wording_rmse: 0.8305 - ema_mcrmse: 0.6613\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][700/1289] Elapsed 28m 36s (remain 23m 59s) Loss: 0.0164(0.0384) Grad: 1.8073  LR: 0.00000016  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.2007  time: 1809s\n",
      "Epoch 4 - content_rmse: 0.4930 - wording_rmse: 0.8387 - mcrmse: 0.6659\n",
      "Epoch 4 avg_val_loss: 0.1990  time: 1902s\n",
      "Epoch 4 - ema_content_rmse: 0.4927 - ema_wording_rmse: 0.8326 - ema_mcrmse: 0.6626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][800/1289] Elapsed 32m 41s (remain 19m 54s) Loss: 0.0661(0.0383) Grad: 3.4604  LR: 0.00000011  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1972  time: 2054s\n",
      "Epoch 4 - content_rmse: 0.4926 - wording_rmse: 0.8270 - mcrmse: 0.6598\n",
      "Epoch 4 avg_val_loss: 0.2004  time: 2147s\n",
      "Epoch 4 - ema_content_rmse: 0.4929 - ema_wording_rmse: 0.8381 - ema_mcrmse: 0.6655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][900/1289] Elapsed 36m 45s (remain 15m 49s) Loss: 0.0299(0.0382) Grad: 3.2330  LR: 0.00000007  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1962  time: 2299s\n",
      "Epoch 4 - content_rmse: 0.4929 - wording_rmse: 0.8220 - mcrmse: 0.6575\n",
      "Epoch 4 avg_val_loss: 0.1957  time: 2392s\n",
      "Epoch 4 - ema_content_rmse: 0.4926 - ema_wording_rmse: 0.8209 - ema_mcrmse: 0.6568\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][1000/1289] Elapsed 40m 51s (remain 11m 45s) Loss: 0.0697(0.0378) Grad: 4.3609  LR: 0.00000004  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1984  time: 2544s\n",
      "Epoch 4 - content_rmse: 0.4924 - wording_rmse: 0.8307 - mcrmse: 0.6615\n",
      "Epoch 4 avg_val_loss: 0.1984  time: 2638s\n",
      "Epoch 4 - ema_content_rmse: 0.4925 - ema_wording_rmse: 0.8305 - ema_mcrmse: 0.6615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][1100/1289] Elapsed 44m 56s (remain 7m 40s) Loss: 0.0254(0.0374) Grad: 1.5703  LR: 0.00000002  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1977  time: 2789s\n",
      "Epoch 4 - content_rmse: 0.4923 - wording_rmse: 0.8280 - mcrmse: 0.6601\n",
      "Epoch 4 avg_val_loss: 0.1978  time: 2883s\n",
      "Epoch 4 - ema_content_rmse: 0.4924 - ema_wording_rmse: 0.8282 - ema_mcrmse: 0.6603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][1200/1289] Elapsed 49m 1s (remain 3m 35s) Loss: 0.0248(0.0371) Grad: 2.8660  LR: 0.00000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1980  time: 3034s\n",
      "Epoch 4 - content_rmse: 0.4922 - wording_rmse: 0.8291 - mcrmse: 0.6606\n",
      "Epoch 4 avg_val_loss: 0.1979  time: 3128s\n",
      "Epoch 4 - ema_content_rmse: 0.4922 - ema_wording_rmse: 0.8289 - ema_mcrmse: 0.6605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][1288/1289] Elapsed 52m 59s (remain 0m 0s) Loss: 0.0467(0.0369) Grad: 4.3034  LR: 0.00000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1979  time: 3273s\n",
      "Epoch 4 - content_rmse: 0.4922 - wording_rmse: 0.8289 - mcrmse: 0.6606\n",
      "Epoch 4 avg_val_loss: 0.1979  time: 3366s\n",
      "Epoch 4 - ema_content_rmse: 0.4922 - ema_wording_rmse: 0.8289 - ema_mcrmse: 0.6606\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/deberta-v3-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f05129b6460>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 3b11de2e-8c4e-4b80-a427-6d72d8088e3f)')' thrown while requesting HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load pretrained model ...\n",
      "Epoch: [1][0/1292] Elapsed 0m 0s (remain 20m 58s) Loss: 0.5580(0.5580) Grad: 6.8567  LR: 0.00000500  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.4905  time: 94s\n",
      "Epoch 1 - content_rmse: 0.9952 - wording_rmse: 1.1457 - mcrmse: 1.0705\n",
      "Epoch 1 - Save Best Score: 1.0705 Model\n",
      "Epoch 1 avg_val_loss: 0.4782  time: 189s\n",
      "Epoch 1 - ema_content_rmse: 0.9964 - ema_wording_rmse: 1.1151 - ema_mcrmse: 1.0558\n",
      "Epoch 1 - ema_Save Best Score: 1.0558 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][100/1292] Elapsed 4m 10s (remain 49m 18s) Loss: 0.4795(0.3457) Grad: 5.7691  LR: 0.00000500  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2675  time: 344s\n",
      "Epoch 1 - content_rmse: 0.6175 - wording_rmse: 0.8744 - mcrmse: 0.7460\n",
      "Epoch 1 - Save Best Score: 0.7460 Model\n",
      "Epoch 1 avg_val_loss: 0.2862  time: 440s\n",
      "Epoch 1 - ema_content_rmse: 0.6569 - ema_wording_rmse: 0.8920 - ema_mcrmse: 0.7745\n",
      "Epoch 1 - ema_Save Best Score: 0.7745 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][200/1292] Elapsed 8m 21s (remain 45m 19s) Loss: 0.1544(0.2745) Grad: 4.0213  LR: 0.00000498  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2210  time: 594s\n",
      "Epoch 1 - content_rmse: 0.5379 - wording_rmse: 0.8189 - mcrmse: 0.6784\n",
      "Epoch 1 - Save Best Score: 0.6784 Model\n",
      "Epoch 1 avg_val_loss: 0.2294  time: 690s\n",
      "Epoch 1 - ema_content_rmse: 0.5602 - ema_wording_rmse: 0.8265 - ema_mcrmse: 0.6933\n",
      "Epoch 1 - ema_Save Best Score: 0.6933 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][300/1292] Elapsed 12m 31s (remain 41m 15s) Loss: 0.0481(0.2393) Grad: 4.0808  LR: 0.00000496  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2214  time: 845s\n",
      "Epoch 1 - content_rmse: 0.5148 - wording_rmse: 0.8247 - mcrmse: 0.6698\n",
      "Epoch 1 - Save Best Score: 0.6698 Model\n",
      "Epoch 1 avg_val_loss: 0.2211  time: 940s\n",
      "Epoch 1 - ema_content_rmse: 0.4992 - ema_wording_rmse: 0.8360 - ema_mcrmse: 0.6676\n",
      "Epoch 1 - ema_Save Best Score: 0.6676 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][400/1292] Elapsed 16m 41s (remain 37m 5s) Loss: 0.0596(0.2231) Grad: 4.0894  LR: 0.00000493  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1895  time: 1095s\n",
      "Epoch 1 - content_rmse: 0.4682 - wording_rmse: 0.7676 - mcrmse: 0.6179\n",
      "Epoch 1 - Save Best Score: 0.6179 Model\n",
      "Epoch 1 avg_val_loss: 0.2120  time: 1190s\n",
      "Epoch 1 - ema_content_rmse: 0.4632 - ema_wording_rmse: 0.8318 - ema_mcrmse: 0.6475\n",
      "Epoch 1 - ema_Save Best Score: 0.6475 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][500/1292] Elapsed 20m 51s (remain 32m 56s) Loss: 0.0961(0.2080) Grad: 5.4275  LR: 0.00000488  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1682  time: 1345s\n",
      "Epoch 1 - content_rmse: 0.4817 - wording_rmse: 0.6985 - mcrmse: 0.5901\n",
      "Epoch 1 - Save Best Score: 0.5901 Model\n",
      "Epoch 1 avg_val_loss: 0.1599  time: 1440s\n",
      "Epoch 1 - ema_content_rmse: 0.4548 - ema_wording_rmse: 0.6896 - ema_mcrmse: 0.5722\n",
      "Epoch 1 - ema_Save Best Score: 0.5722 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][600/1292] Elapsed 25m 1s (remain 28m 46s) Loss: 0.0819(0.1956) Grad: 5.2134  LR: 0.00000484  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1585  time: 1595s\n",
      "Epoch 1 - content_rmse: 0.4622 - wording_rmse: 0.6657 - mcrmse: 0.5639\n",
      "Epoch 1 - Save Best Score: 0.5639 Model\n",
      "Epoch 1 avg_val_loss: 0.1526  time: 1690s\n",
      "Epoch 1 - ema_content_rmse: 0.4513 - ema_wording_rmse: 0.6542 - ema_mcrmse: 0.5527\n",
      "Epoch 1 - ema_Save Best Score: 0.5527 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][700/1292] Elapsed 29m 11s (remain 24m 37s) Loss: 0.1562(0.1851) Grad: 5.2645  LR: 0.00000478  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1490  time: 1845s\n",
      "Epoch 1 - content_rmse: 0.4623 - wording_rmse: 0.6297 - mcrmse: 0.5460\n",
      "Epoch 1 - Save Best Score: 0.5460 Model\n",
      "Epoch 1 avg_val_loss: 0.1522  time: 1940s\n",
      "Epoch 1 - ema_content_rmse: 0.4698 - ema_wording_rmse: 0.6354 - ema_mcrmse: 0.5526\n",
      "Epoch 1 - ema_Save Best Score: 0.5526 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][800/1292] Elapsed 33m 21s (remain 20m 27s) Loss: 0.2985(0.1800) Grad: 7.8579  LR: 0.00000471  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1264  time: 2095s\n",
      "Epoch 1 - content_rmse: 0.4387 - wording_rmse: 0.5718 - mcrmse: 0.5053\n",
      "Epoch 1 - Save Best Score: 0.5053 Model\n",
      "Epoch 1 avg_val_loss: 0.1281  time: 2190s\n",
      "Epoch 1 - ema_content_rmse: 0.4470 - ema_wording_rmse: 0.5712 - ema_mcrmse: 0.5091\n",
      "Epoch 1 - ema_Save Best Score: 0.5091 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][900/1292] Elapsed 37m 32s (remain 16m 17s) Loss: 0.0515(0.1747) Grad: 2.3749  LR: 0.00000463  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1489  time: 2345s\n",
      "Epoch 1 - content_rmse: 0.4628 - wording_rmse: 0.6345 - mcrmse: 0.5487\n",
      "Epoch 1 avg_val_loss: 0.1421  time: 2438s\n",
      "Epoch 1 - ema_content_rmse: 0.4651 - ema_wording_rmse: 0.6082 - ema_mcrmse: 0.5367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1000/1292] Elapsed 41m 36s (remain 12m 5s) Loss: 0.1875(0.1694) Grad: 4.0287  LR: 0.00000455  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1158  time: 2589s\n",
      "Epoch 1 - content_rmse: 0.4452 - wording_rmse: 0.5242 - mcrmse: 0.4847\n",
      "Epoch 1 - Save Best Score: 0.4847 Model\n",
      "Epoch 1 avg_val_loss: 0.1250  time: 2685s\n",
      "Epoch 1 - ema_content_rmse: 0.4412 - ema_wording_rmse: 0.5627 - ema_mcrmse: 0.5020\n",
      "Epoch 1 - ema_Save Best Score: 0.5020 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1100/1292] Elapsed 45m 46s (remain 7m 56s) Loss: 0.2600(0.1654) Grad: 12.4162  LR: 0.00000446  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1700  time: 2839s\n",
      "Epoch 1 - content_rmse: 0.5284 - wording_rmse: 0.6565 - mcrmse: 0.5925\n",
      "Epoch 1 avg_val_loss: 0.1397  time: 2932s\n",
      "Epoch 1 - ema_content_rmse: 0.4661 - ema_wording_rmse: 0.6015 - ema_mcrmse: 0.5338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1200/1292] Elapsed 49m 50s (remain 3m 46s) Loss: 0.3401(0.1618) Grad: 5.8140  LR: 0.00000436  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1316  time: 3083s\n",
      "Epoch 1 - content_rmse: 0.4509 - wording_rmse: 0.5830 - mcrmse: 0.5170\n",
      "Epoch 1 avg_val_loss: 0.1240  time: 3176s\n",
      "Epoch 1 - ema_content_rmse: 0.4389 - ema_wording_rmse: 0.5632 - ema_mcrmse: 0.5010\n",
      "Epoch 1 - ema_Save Best Score: 0.5010 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1291/1292] Elapsed 53m 52s (remain 0m 0s) Loss: 0.1508(0.1592) Grad: 4.5553  LR: 0.00000427  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1322  time: 3325s\n",
      "Epoch 1 - content_rmse: 0.4555 - wording_rmse: 0.5852 - mcrmse: 0.5204\n",
      "Epoch 1 avg_val_loss: 0.1353  time: 3418s\n",
      "Epoch 1 - ema_content_rmse: 0.4473 - ema_wording_rmse: 0.6043 - ema_mcrmse: 0.5258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/1292] Elapsed 0m 0s (remain 19m 5s) Loss: 0.0560(0.0560) Grad: 2.9591  LR: 0.00000427  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1324  time: 94s\n",
      "Epoch 2 - content_rmse: 0.4606 - wording_rmse: 0.5814 - mcrmse: 0.5210\n",
      "Epoch 2 avg_val_loss: 0.1342  time: 186s\n",
      "Epoch 2 - ema_content_rmse: 0.4461 - ema_wording_rmse: 0.6011 - ema_mcrmse: 0.5236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][100/1292] Elapsed 4m 5s (remain 48m 12s) Loss: 0.1162(0.0980) Grad: 4.3164  LR: 0.00000416  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1468  time: 338s\n",
      "Epoch 2 - content_rmse: 0.4355 - wording_rmse: 0.6506 - mcrmse: 0.5430\n",
      "Epoch 2 avg_val_loss: 0.1334  time: 431s\n",
      "Epoch 2 - ema_content_rmse: 0.4335 - ema_wording_rmse: 0.6031 - ema_mcrmse: 0.5183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][200/1292] Elapsed 8m 9s (remain 44m 16s) Loss: 0.0908(0.1024) Grad: 5.5367  LR: 0.00000404  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1177  time: 582s\n",
      "Epoch 2 - content_rmse: 0.4339 - wording_rmse: 0.5413 - mcrmse: 0.4876\n",
      "Epoch 2 avg_val_loss: 0.1181  time: 675s\n",
      "Epoch 2 - ema_content_rmse: 0.4345 - ema_wording_rmse: 0.5426 - ema_mcrmse: 0.4885\n",
      "Epoch 2 - ema_Save Best Score: 0.4885 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][300/1292] Elapsed 12m 16s (remain 40m 25s) Loss: 0.1089(0.0967) Grad: 5.5363  LR: 0.00000392  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1112  time: 829s\n",
      "Epoch 2 - content_rmse: 0.4262 - wording_rmse: 0.5197 - mcrmse: 0.4729\n",
      "Epoch 2 - Save Best Score: 0.4729 Model\n",
      "Epoch 2 avg_val_loss: 0.1139  time: 925s\n",
      "Epoch 2 - ema_content_rmse: 0.4225 - ema_wording_rmse: 0.5333 - ema_mcrmse: 0.4779\n",
      "Epoch 2 - ema_Save Best Score: 0.4779 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][400/1292] Elapsed 16m 26s (remain 36m 32s) Loss: 0.0355(0.0952) Grad: 3.1333  LR: 0.00000379  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1241  time: 1080s\n",
      "Epoch 2 - content_rmse: 0.4481 - wording_rmse: 0.5529 - mcrmse: 0.5005\n",
      "Epoch 2 avg_val_loss: 0.1176  time: 1173s\n",
      "Epoch 2 - ema_content_rmse: 0.4362 - ema_wording_rmse: 0.5382 - ema_mcrmse: 0.4872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][500/1292] Elapsed 20m 31s (remain 32m 23s) Loss: 0.1177(0.0941) Grad: 5.8405  LR: 0.00000366  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1203  time: 1324s\n",
      "Epoch 2 - content_rmse: 0.4423 - wording_rmse: 0.5481 - mcrmse: 0.4952\n",
      "Epoch 2 avg_val_loss: 0.1302  time: 1417s\n",
      "Epoch 2 - ema_content_rmse: 0.4436 - ema_wording_rmse: 0.5866 - ema_mcrmse: 0.5151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][600/1292] Elapsed 24m 35s (remain 28m 16s) Loss: 0.0235(0.0960) Grad: 3.2223  LR: 0.00000352  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1181  time: 1568s\n",
      "Epoch 2 - content_rmse: 0.4203 - wording_rmse: 0.5509 - mcrmse: 0.4856\n",
      "Epoch 2 avg_val_loss: 0.1113  time: 1661s\n",
      "Epoch 2 - ema_content_rmse: 0.4208 - ema_wording_rmse: 0.5249 - ema_mcrmse: 0.4728\n",
      "Epoch 2 - ema_Save Best Score: 0.4728 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][700/1292] Elapsed 28m 42s (remain 24m 12s) Loss: 0.0626(0.0970) Grad: 6.0898  LR: 0.00000338  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1157  time: 1815s\n",
      "Epoch 2 - content_rmse: 0.4310 - wording_rmse: 0.5364 - mcrmse: 0.4837\n",
      "Epoch 2 avg_val_loss: 0.1225  time: 1908s\n",
      "Epoch 2 - ema_content_rmse: 0.4282 - ema_wording_rmse: 0.5663 - ema_mcrmse: 0.4973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][800/1292] Elapsed 32m 46s (remain 20m 5s) Loss: 0.0909(0.0965) Grad: 2.2548  LR: 0.00000324  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1146  time: 2059s\n",
      "Epoch 2 - content_rmse: 0.4230 - wording_rmse: 0.5372 - mcrmse: 0.4801\n",
      "Epoch 2 avg_val_loss: 0.1273  time: 2152s\n",
      "Epoch 2 - ema_content_rmse: 0.4358 - ema_wording_rmse: 0.5780 - ema_mcrmse: 0.5069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][900/1292] Elapsed 36m 50s (remain 15m 59s) Loss: 0.0950(0.0948) Grad: 4.4822  LR: 0.00000309  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1190  time: 2303s\n",
      "Epoch 2 - content_rmse: 0.4199 - wording_rmse: 0.5573 - mcrmse: 0.4886\n",
      "Epoch 2 avg_val_loss: 0.1172  time: 2396s\n",
      "Epoch 2 - ema_content_rmse: 0.4181 - ema_wording_rmse: 0.5512 - ema_mcrmse: 0.4846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1000/1292] Elapsed 40m 54s (remain 11m 53s) Loss: 0.0165(0.0949) Grad: 2.4111  LR: 0.00000294  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1300  time: 2548s\n",
      "Epoch 2 - content_rmse: 0.4389 - wording_rmse: 0.5883 - mcrmse: 0.5136\n",
      "Epoch 2 avg_val_loss: 0.1273  time: 2640s\n",
      "Epoch 2 - ema_content_rmse: 0.4390 - ema_wording_rmse: 0.5766 - ema_mcrmse: 0.5078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1100/1292] Elapsed 44m 59s (remain 7m 48s) Loss: 0.0377(0.0946) Grad: 4.5903  LR: 0.00000279  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1142  time: 2792s\n",
      "Epoch 2 - content_rmse: 0.4581 - wording_rmse: 0.5039 - mcrmse: 0.4810\n",
      "Epoch 2 avg_val_loss: 0.1081  time: 2885s\n",
      "Epoch 2 - ema_content_rmse: 0.4250 - ema_wording_rmse: 0.5082 - ema_mcrmse: 0.4666\n",
      "Epoch 2 - ema_Save Best Score: 0.4666 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1200/1292] Elapsed 49m 6s (remain 3m 43s) Loss: 0.0952(0.0937) Grad: 4.7948  LR: 0.00000264  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1390  time: 3039s\n",
      "Epoch 2 - content_rmse: 0.4392 - wording_rmse: 0.6236 - mcrmse: 0.5314\n",
      "Epoch 2 avg_val_loss: 0.1280  time: 3132s\n",
      "Epoch 2 - ema_content_rmse: 0.4324 - ema_wording_rmse: 0.5858 - ema_mcrmse: 0.5091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1291/1292] Elapsed 53m 5s (remain 0m 0s) Loss: 0.0607(0.0939) Grad: 2.8404  LR: 0.00000250  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1172  time: 3278s\n",
      "Epoch 2 - content_rmse: 0.4210 - wording_rmse: 0.5502 - mcrmse: 0.4856\n",
      "Epoch 2 avg_val_loss: 0.1194  time: 3371s\n",
      "Epoch 2 - ema_content_rmse: 0.4235 - ema_wording_rmse: 0.5600 - ema_mcrmse: 0.4917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/1292] Elapsed 0m 0s (remain 19m 0s) Loss: 0.1702(0.1702) Grad: 4.0934  LR: 0.00000250  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1167  time: 94s\n",
      "Epoch 3 - content_rmse: 0.4203 - wording_rmse: 0.5486 - mcrmse: 0.4844\n",
      "Epoch 3 avg_val_loss: 0.1192  time: 186s\n",
      "Epoch 3 - ema_content_rmse: 0.4231 - ema_wording_rmse: 0.5593 - ema_mcrmse: 0.4912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][100/1292] Elapsed 4m 5s (remain 48m 13s) Loss: 0.0557(0.0735) Grad: 1.4735  LR: 0.00000235  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1179  time: 338s\n",
      "Epoch 3 - content_rmse: 0.4352 - wording_rmse: 0.5433 - mcrmse: 0.4893\n",
      "Epoch 3 avg_val_loss: 0.1248  time: 431s\n",
      "Epoch 3 - ema_content_rmse: 0.4338 - ema_wording_rmse: 0.5750 - ema_mcrmse: 0.5044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][200/1292] Elapsed 8m 9s (remain 44m 16s) Loss: 0.0560(0.0661) Grad: 4.2541  LR: 0.00000220  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1221  time: 582s\n",
      "Epoch 3 - content_rmse: 0.4310 - wording_rmse: 0.5656 - mcrmse: 0.4983\n",
      "Epoch 3 avg_val_loss: 0.1272  time: 675s\n",
      "Epoch 3 - ema_content_rmse: 0.4319 - ema_wording_rmse: 0.5841 - ema_mcrmse: 0.5080\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][300/1292] Elapsed 12m 13s (remain 40m 15s) Loss: 0.0456(0.0619) Grad: 3.4829  LR: 0.00000205  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1150  time: 827s\n",
      "Epoch 3 - content_rmse: 0.4231 - wording_rmse: 0.5399 - mcrmse: 0.4815\n",
      "Epoch 3 avg_val_loss: 0.1165  time: 919s\n",
      "Epoch 3 - ema_content_rmse: 0.4248 - ema_wording_rmse: 0.5456 - ema_mcrmse: 0.4852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][400/1292] Elapsed 16m 18s (remain 36m 13s) Loss: 0.0196(0.0611) Grad: 1.7661  LR: 0.00000190  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1311  time: 1071s\n",
      "Epoch 3 - content_rmse: 0.4349 - wording_rmse: 0.5941 - mcrmse: 0.5145\n",
      "Epoch 3 avg_val_loss: 0.1187  time: 1164s\n",
      "Epoch 3 - ema_content_rmse: 0.4308 - ema_wording_rmse: 0.5505 - ema_mcrmse: 0.4907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][500/1292] Elapsed 20m 22s (remain 32m 10s) Loss: 0.0274(0.0606) Grad: 2.0721  LR: 0.00000175  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1214  time: 1315s\n",
      "Epoch 3 - content_rmse: 0.4392 - wording_rmse: 0.5521 - mcrmse: 0.4957\n",
      "Epoch 3 avg_val_loss: 0.1254  time: 1408s\n",
      "Epoch 3 - ema_content_rmse: 0.4339 - ema_wording_rmse: 0.5728 - ema_mcrmse: 0.5033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][600/1292] Elapsed 24m 26s (remain 28m 6s) Loss: 0.0711(0.0611) Grad: 4.2896  LR: 0.00000161  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1159  time: 1560s\n",
      "Epoch 3 - content_rmse: 0.4392 - wording_rmse: 0.5281 - mcrmse: 0.4837\n",
      "Epoch 3 avg_val_loss: 0.1182  time: 1652s\n",
      "Epoch 3 - ema_content_rmse: 0.4312 - ema_wording_rmse: 0.5449 - ema_mcrmse: 0.4880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][700/1292] Elapsed 28m 31s (remain 24m 2s) Loss: 0.0243(0.0610) Grad: 3.5175  LR: 0.00000147  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1165  time: 1804s\n",
      "Epoch 3 - content_rmse: 0.4243 - wording_rmse: 0.5442 - mcrmse: 0.4843\n",
      "Epoch 3 avg_val_loss: 0.1177  time: 1897s\n",
      "Epoch 3 - ema_content_rmse: 0.4238 - ema_wording_rmse: 0.5488 - ema_mcrmse: 0.4863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][800/1292] Elapsed 32m 35s (remain 19m 58s) Loss: 0.0327(0.0608) Grad: 1.7326  LR: 0.00000133  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1239  time: 2048s\n",
      "Epoch 3 - content_rmse: 0.4309 - wording_rmse: 0.5650 - mcrmse: 0.4980\n",
      "Epoch 3 avg_val_loss: 0.1192  time: 2141s\n",
      "Epoch 3 - ema_content_rmse: 0.4236 - ema_wording_rmse: 0.5538 - ema_mcrmse: 0.4887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][900/1292] Elapsed 36m 39s (remain 15m 54s) Loss: 0.0444(0.0612) Grad: 2.7261  LR: 0.00000120  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1112  time: 2292s\n",
      "Epoch 3 - content_rmse: 0.4219 - wording_rmse: 0.5247 - mcrmse: 0.4733\n",
      "Epoch 3 avg_val_loss: 0.1123  time: 2385s\n",
      "Epoch 3 - ema_content_rmse: 0.4203 - ema_wording_rmse: 0.5297 - ema_mcrmse: 0.4750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1000/1292] Elapsed 40m 43s (remain 11m 50s) Loss: 0.0366(0.0607) Grad: 3.7632  LR: 0.00000107  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1301  time: 2537s\n",
      "Epoch 3 - content_rmse: 0.4281 - wording_rmse: 0.5934 - mcrmse: 0.5108\n",
      "Epoch 3 avg_val_loss: 0.1210  time: 2629s\n",
      "Epoch 3 - ema_content_rmse: 0.4267 - ema_wording_rmse: 0.5597 - ema_mcrmse: 0.4932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1100/1292] Elapsed 44m 48s (remain 7m 46s) Loss: 0.0492(0.0603) Grad: 1.6996  LR: 0.00000095  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1154  time: 2781s\n",
      "Epoch 3 - content_rmse: 0.4307 - wording_rmse: 0.5354 - mcrmse: 0.4830\n",
      "Epoch 3 avg_val_loss: 0.1181  time: 2874s\n",
      "Epoch 3 - ema_content_rmse: 0.4287 - ema_wording_rmse: 0.5471 - ema_mcrmse: 0.4879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1200/1292] Elapsed 48m 52s (remain 3m 42s) Loss: 0.0580(0.0604) Grad: 3.3690  LR: 0.00000083  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1130  time: 3025s\n",
      "Epoch 3 - content_rmse: 0.4348 - wording_rmse: 0.5217 - mcrmse: 0.4783\n",
      "Epoch 3 avg_val_loss: 0.1137  time: 3118s\n",
      "Epoch 3 - ema_content_rmse: 0.4337 - ema_wording_rmse: 0.5260 - ema_mcrmse: 0.4798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1291/1292] Elapsed 52m 51s (remain 0m 0s) Loss: 0.0486(0.0606) Grad: 4.0713  LR: 0.00000073  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1090  time: 3264s\n",
      "Epoch 3 - content_rmse: 0.4261 - wording_rmse: 0.5132 - mcrmse: 0.4696\n",
      "Epoch 3 - Save Best Score: 0.4696 Model\n",
      "Epoch 3 avg_val_loss: 0.1112  time: 3360s\n",
      "Epoch 3 - ema_content_rmse: 0.4295 - ema_wording_rmse: 0.5191 - ema_mcrmse: 0.4743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/1292] Elapsed 0m 0s (remain 19m 14s) Loss: 0.0297(0.0297) Grad: 3.6454  LR: 0.00000073  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1092  time: 94s\n",
      "Epoch 4 - content_rmse: 0.4261 - wording_rmse: 0.5137 - mcrmse: 0.4699\n",
      "Epoch 4 avg_val_loss: 0.1111  time: 186s\n",
      "Epoch 4 - ema_content_rmse: 0.4294 - ema_wording_rmse: 0.5189 - ema_mcrmse: 0.4741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][100/1292] Elapsed 4m 5s (remain 48m 14s) Loss: 0.0414(0.0462) Grad: 3.2925  LR: 0.00000063  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1135  time: 338s\n",
      "Epoch 4 - content_rmse: 0.4305 - wording_rmse: 0.5274 - mcrmse: 0.4790\n",
      "Epoch 4 avg_val_loss: 0.1137  time: 431s\n",
      "Epoch 4 - ema_content_rmse: 0.4286 - ema_wording_rmse: 0.5298 - ema_mcrmse: 0.4792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][200/1292] Elapsed 8m 9s (remain 44m 17s) Loss: 0.0140(0.0424) Grad: 1.7108  LR: 0.00000053  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1166  time: 582s\n",
      "Epoch 4 - content_rmse: 0.4309 - wording_rmse: 0.5394 - mcrmse: 0.4851\n",
      "Epoch 4 avg_val_loss: 0.1168  time: 675s\n",
      "Epoch 4 - ema_content_rmse: 0.4315 - ema_wording_rmse: 0.5395 - ema_mcrmse: 0.4855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][300/1292] Elapsed 12m 13s (remain 40m 15s) Loss: 0.0178(0.0418) Grad: 1.6522  LR: 0.00000044  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1134  time: 826s\n",
      "Epoch 4 - content_rmse: 0.4334 - wording_rmse: 0.5243 - mcrmse: 0.4788\n",
      "Epoch 4 avg_val_loss: 0.1150  time: 919s\n",
      "Epoch 4 - ema_content_rmse: 0.4311 - ema_wording_rmse: 0.5324 - ema_mcrmse: 0.4817\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][400/1292] Elapsed 16m 17s (remain 36m 12s) Loss: 0.0208(0.0414) Grad: 1.4298  LR: 0.00000036  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1112  time: 1071s\n",
      "Epoch 4 - content_rmse: 0.4287 - wording_rmse: 0.5193 - mcrmse: 0.4740\n",
      "Epoch 4 avg_val_loss: 0.1124  time: 1163s\n",
      "Epoch 4 - ema_content_rmse: 0.4311 - ema_wording_rmse: 0.5222 - ema_mcrmse: 0.4766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][500/1292] Elapsed 20m 22s (remain 32m 9s) Loss: 0.0996(0.0419) Grad: 3.3710  LR: 0.00000028  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1204  time: 1315s\n",
      "Epoch 4 - content_rmse: 0.4362 - wording_rmse: 0.5497 - mcrmse: 0.4929\n",
      "Epoch 4 avg_val_loss: 0.1165  time: 1408s\n",
      "Epoch 4 - ema_content_rmse: 0.4338 - ema_wording_rmse: 0.5363 - ema_mcrmse: 0.4851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][600/1292] Elapsed 24m 26s (remain 28m 5s) Loss: 0.0195(0.0418) Grad: 1.7199  LR: 0.00000022  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1188  time: 1559s\n",
      "Epoch 4 - content_rmse: 0.4326 - wording_rmse: 0.5463 - mcrmse: 0.4895\n",
      "Epoch 4 avg_val_loss: 0.1162  time: 1652s\n",
      "Epoch 4 - ema_content_rmse: 0.4338 - ema_wording_rmse: 0.5352 - ema_mcrmse: 0.4845\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][700/1292] Elapsed 28m 30s (remain 24m 2s) Loss: 0.0205(0.0413) Grad: 3.1120  LR: 0.00000016  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1143  time: 1803s\n",
      "Epoch 4 - content_rmse: 0.4335 - wording_rmse: 0.5278 - mcrmse: 0.4806\n",
      "Epoch 4 avg_val_loss: 0.1154  time: 1896s\n",
      "Epoch 4 - ema_content_rmse: 0.4336 - ema_wording_rmse: 0.5323 - ema_mcrmse: 0.4830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][800/1292] Elapsed 32m 34s (remain 19m 58s) Loss: 0.0445(0.0411) Grad: 2.4572  LR: 0.00000011  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1170  time: 2048s\n",
      "Epoch 4 - content_rmse: 0.4311 - wording_rmse: 0.5407 - mcrmse: 0.4859\n",
      "Epoch 4 avg_val_loss: 0.1155  time: 2140s\n",
      "Epoch 4 - ema_content_rmse: 0.4308 - ema_wording_rmse: 0.5350 - ema_mcrmse: 0.4829\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][900/1292] Elapsed 36m 38s (remain 15m 54s) Loss: 0.0539(0.0410) Grad: 3.0510  LR: 0.00000007  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1157  time: 2292s\n",
      "Epoch 4 - content_rmse: 0.4340 - wording_rmse: 0.5331 - mcrmse: 0.4835\n",
      "Epoch 4 avg_val_loss: 0.1165  time: 2384s\n",
      "Epoch 4 - ema_content_rmse: 0.4328 - ema_wording_rmse: 0.5374 - ema_mcrmse: 0.4851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][1000/1292] Elapsed 40m 43s (remain 11m 50s) Loss: 0.0283(0.0413) Grad: 3.7726  LR: 0.00000004  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1154  time: 2536s\n",
      "Epoch 4 - content_rmse: 0.4349 - wording_rmse: 0.5311 - mcrmse: 0.4830\n",
      "Epoch 4 avg_val_loss: 0.1154  time: 2629s\n",
      "Epoch 4 - ema_content_rmse: 0.4349 - ema_wording_rmse: 0.5315 - ema_mcrmse: 0.4832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][1100/1292] Elapsed 44m 47s (remain 7m 46s) Loss: 0.0254(0.0412) Grad: 3.2773  LR: 0.00000002  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1154  time: 2780s\n",
      "Epoch 4 - content_rmse: 0.4337 - wording_rmse: 0.5322 - mcrmse: 0.4829\n",
      "Epoch 4 avg_val_loss: 0.1154  time: 2873s\n",
      "Epoch 4 - ema_content_rmse: 0.4344 - ema_wording_rmse: 0.5316 - ema_mcrmse: 0.4830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][1200/1292] Elapsed 48m 51s (remain 3m 42s) Loss: 0.0495(0.0414) Grad: 5.8561  LR: 0.00000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1156  time: 3024s\n",
      "Epoch 4 - content_rmse: 0.4334 - wording_rmse: 0.5332 - mcrmse: 0.4833\n",
      "Epoch 4 avg_val_loss: 0.1155  time: 3117s\n",
      "Epoch 4 - ema_content_rmse: 0.4336 - ema_wording_rmse: 0.5327 - ema_mcrmse: 0.4832\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][1291/1292] Elapsed 52m 50s (remain 0m 0s) Loss: 0.0195(0.0417) Grad: 2.9176  LR: 0.00000000  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4 avg_val_loss: 0.1156  time: 3263s\n",
      "Epoch 4 - content_rmse: 0.4334 - wording_rmse: 0.5332 - mcrmse: 0.4833\n",
      "Epoch 4 avg_val_loss: 0.1156  time: 3356s\n",
      "Epoch 4 - ema_content_rmse: 0.4334 - ema_wording_rmse: 0.5331 - ema_mcrmse: 0.4833\n"
     ]
    }
   ],
   "source": [
    "train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BV3Rp1bbXUQ-"
   },
   "outputs": [],
   "source": [
    "## total_complex = []\n",
    "# for fold in range(4):\n",
    "#     va_data = train_df[train_df['fold'] == fold]\n",
    "#     preds = torch.load('/content/drive/MyDrive/deb_simple/microsoft_deberta-v3-large_best{}.pth'.format(fold))['predictions']\n",
    "#     va_data['preds'] = preds\n",
    "#     va_data = va_data[['id', 'preds', 'score']]\n",
    "#     print(compute_metrics(va_data['preds'].values.reshape(-1,1), va_data['score'].values))\n",
    "#     total_complex.append(va_data)\n",
    "# total_complex = pd.concat(total_complex)\n",
    "# compute_metrics(total_complex['preds'].values.reshape(-1,1), total_complex['score'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11_KW3qSx1IK",
    "outputId": "e5423c46-d702-49da-d29b-c5d125a665e4"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /root/.kaggle\n",
    "# !cp /content/drive/MyDrive/kaggle/kaggle.json /root/.kaggle/\n",
    "# !chmod 600 /root/.kaggle/kaggle.json\n",
    "# !kaggle datasets init -p /content/drive/MyDrive/elc_mean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cN6F3A_hKthE",
    "outputId": "0358b1da-6195-44ba-8ab5-2eacb5268fff"
   },
   "outputs": [],
   "source": [
    "#!kaggle datasets create -p /content/drive/MyDrive/elc_mean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "BGinNBYJFw3O"
   },
   "outputs": [],
   "source": [
    "# deberta v3 large\n",
    "# 1.5 0.8228\n",
    "# 2 0.8197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVfKappB86jX",
    "tags": []
   },
   "source": [
    "# 1.5  8137\n",
    "#2 8175\n",
    "#2.5 8181\n",
    "#3 8181\n",
    "#3.5 8175\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
