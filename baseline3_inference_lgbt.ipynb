{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41344d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedGroupKFold\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import logging\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForWholeWordMask\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n",
    "from transformers.modeling_outputs  import BaseModelOutput,SequenceClassifierOutput\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "from datasets import load_metric, disable_progress_bar\n",
    "import datasets\n",
    "# imports the torch_xla package\n",
    "import wandb\n",
    "from torch.nn.parameter import Parameter\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "from tqdm import tqdm\n",
    "from spellchecker import SpellChecker\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import spacy\n",
    "import lightgbm as lgb\n",
    "import warnings\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "\n",
    "warnings.simplefilter('ignore')\n",
    "logging.disable(logging.ERROR);\n",
    "os.environ['TOKENIZER_PARALLELISM'] = 'false'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "disable_progress_bar()\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66193e29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.7.2'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spellchecker\n",
    "spellchecker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a90655b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "def seed_everything(seed: int):\n",
    "    import random, os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23ff38",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95a371e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    pretraining = False\n",
    "    load_pretrained = False\n",
    "    input_path = './input/'\n",
    "    input_type = '2'\n",
    "    model_path = 'microsoft/deberta-v3-large' #  nghuyong/ernie-2.0-large-en studio-ousia/luke-large\n",
    "    model_type = 'custom'\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5  # 1.5\n",
    "    num_warmup_steps = 0\n",
    "    max_input_length = 512\n",
    "    max_position_embeddings = 512\n",
    "    n_folds = 4\n",
    "    folds = [1]\n",
    "    epochs = 4  # 5\n",
    "    # layer - wise larning rate \n",
    "    discriminative_learning_rate = False\n",
    "    discriminative_learning_rate_num_groups = 1\n",
    "    discriminative_learning_rate_decay_rate = 0.99\n",
    "    # reinint layer\n",
    "    reinit_layers = 0\n",
    "    \n",
    "#     encoder_lr = 5e-6\n",
    "#     head_lr = 5e-6\n",
    "    encoder_lr = 20e-6\n",
    "    head_lr = 10e-5\n",
    "    \n",
    "    min_lr = 1e-7\n",
    "    eps = 1e-7\n",
    "    betas = (0.9, 0.999)\n",
    "    weight_decay = 0\n",
    "    dropout = 0\n",
    "    num_fold = 5\n",
    "    batch_size = 8\n",
    "    seed = 42\n",
    "    OUTPUT_DIR = './pretrain/'\n",
    "    num_workers = 2\n",
    "    device='cuda'\n",
    "    print_freq = 100\n",
    "\n",
    "    \n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name_or_path: Optional[str] = field(\n",
    "        default=\"microsoft/deberta-v3-base\",\n",
    "        metadata={\"help\": \"Model name or path\"},\n",
    "    )\n",
    "\n",
    "    data_dir: Optional[str] = field(\n",
    "        default=\"/kaggle/input/commonlit-evaluate-student-summaries\",\n",
    "        metadata={\"help\": \"Data directory\"},\n",
    "    )\n",
    "\n",
    "    max_seq_length: Optional[int] = field(\n",
    "        default=1600,\n",
    "        metadata={\"help\": \"Max sequence length\"},\n",
    "    )\n",
    "\n",
    "    add_prompt_question: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Add prompt question into input\"},\n",
    "    )\n",
    "\n",
    "    add_prompt_text: Optional[bool] = field(\n",
    "        default=False,\n",
    "        metadata={\"help\": \"Add prompt text into input\"},\n",
    "    )\n",
    "\n",
    "    fold: Optional[int] = field(\n",
    "        default=0,\n",
    "        metadata={\"help\": \"Fold\"},\n",
    "    )\n",
    "\n",
    "    num_proc: Optional[int] = field(\n",
    "        default=4,\n",
    "        metadata={\"help\": \"Number of processes\"},\n",
    "    )\n",
    "\n",
    "    dropout: Optional[float] = field(\n",
    "        default=0.,\n",
    "        metadata={\"help\": \"Amount of dropout to apply\"},\n",
    "    )\n",
    "    max_position_embeddings: Optional[int] = field(\n",
    "        default=1600,\n",
    "        metadata={\"help\": \"Amount of dropout to apply\"},\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62f8e816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf = pd.read_csv(f\"{CFG.input_path}/prompts_test.csv\")\n",
    "pdf = pd.read_csv(f'{CFG.input_path}/prompts_train.csv');\n",
    "# sdf = pd.read_csv(f\"{CFG.input_path}/summaries_test.csv\")\n",
    "sdf = pd.read_csv(f'{CFG.input_path}/summaries_train.csv');\n",
    "\n",
    "df = pdf.merge(sdf, on=\"prompt_id\")\n",
    "\n",
    "# 4 prompt ids, 4 folds\n",
    "# id2fold = {\n",
    "#     \"814d6b\": 0,\n",
    "#     \"39c16e\": 1,\n",
    "#     \"3b9047\": 2,\n",
    "#     \"ebad26\": 3,\n",
    "# }\n",
    "\n",
    "# df[\"fold\"] = df[\"prompt_id\"].map(id2fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab9c06b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append('/root/workspace/commonlit/nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fbd085b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(CFG.model_path);\n",
    "        self.STOP_WORDS = set(stopwords.words('english'));\n",
    "        \n",
    "        self.spacy_ner_model = spacy.load('en_core_web_sm', )\n",
    "        self.speller = SpellChecker()\n",
    "        \n",
    "    def count_text_length(self, df: pd.DataFrame, col:str) -> pd.Series:\n",
    "        \"\"\" text length \"\"\"\n",
    "        tokenizer=self.tokenizer\n",
    "        return df[col].progress_apply(lambda x: len(tokenizer.encode(x)))\n",
    "\n",
    "    def word_overlap_count(self, row):\n",
    "        \"\"\" intersection(prompt_text, text) \"\"\"        \n",
    "        def check_is_stop_word(word):\n",
    "            return word in self.STOP_WORDS\n",
    "        \n",
    "        prompt_words = row['prompt_tokens']\n",
    "        summary_words = row['summary_tokens']\n",
    "        if self.STOP_WORDS:\n",
    "            prompt_words = list(filter(check_is_stop_word, prompt_words))\n",
    "            summary_words = list(filter(check_is_stop_word, summary_words))\n",
    "        return len(set(prompt_words).intersection(set(summary_words)))\n",
    "            \n",
    "    def ngrams(self, token, n):\n",
    "        # Use the zip function to help us generate n-grams\n",
    "        # Concatentate the tokens into ngrams and return\n",
    "        ngrams = zip(*[token[i:] for i in range(n)])\n",
    "        return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "    def ngram_co_occurrence(self, row, n: int):\n",
    "        # Tokenize the original text and summary into words\n",
    "        original_tokens = row['prompt_tokens']\n",
    "        summary_tokens = row['summary_tokens']\n",
    "\n",
    "        # Generate n-grams for the original text and summary\n",
    "        original_ngrams = set(self.ngrams(original_tokens, n))\n",
    "        summary_ngrams = set(self.ngrams(summary_tokens, n))\n",
    "\n",
    "        # Calculate the number of common n-grams\n",
    "        common_ngrams = original_ngrams.intersection(summary_ngrams)\n",
    "\n",
    "        # # Optionally, you can get the frequency of common n-grams for a more nuanced analysis\n",
    "        # original_ngram_freq = Counter(ngrams(original_words, n))\n",
    "        # summary_ngram_freq = Counter(ngrams(summary_words, n))\n",
    "        # common_ngram_freq = {ngram: min(original_ngram_freq[ngram], summary_ngram_freq[ngram]) for ngram in common_ngrams}\n",
    "\n",
    "        return len(common_ngrams)\n",
    "    \n",
    "    def ner_overlap_count(self, row, mode:str):\n",
    "        model = self.spacy_ner_model\n",
    "        def clean_ners(ner_list):\n",
    "            return set([(ner[0].lower(), ner[1]) for ner in ner_list])\n",
    "        prompt = model(row['prompt_text'])\n",
    "        summary = model(row['text'])\n",
    "\n",
    "        if \"spacy\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.label_) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.label_) for token in summary.ents])\n",
    "        elif \"stanza\" in str(model):\n",
    "            prompt_ner = set([(token.text, token.type) for token in prompt.ents])\n",
    "            summary_ner = set([(token.text, token.type) for token in summary.ents])\n",
    "        else:\n",
    "            raise Exception(\"Model not supported\")\n",
    "\n",
    "        prompt_ner = clean_ners(prompt_ner)\n",
    "        summary_ner = clean_ners(summary_ner)\n",
    "\n",
    "        intersecting_ners = prompt_ner.intersection(summary_ner)\n",
    "        \n",
    "        ner_dict = dict(Counter([ner[1] for ner in intersecting_ners]))\n",
    "        \n",
    "        if mode == \"train\":\n",
    "            return ner_dict\n",
    "        elif mode == \"test\":\n",
    "            return {key: ner_dict.get(key) for key in self.ner_keys}\n",
    "\n",
    "    \n",
    "    def quotes_count(self, row):\n",
    "        summary = row['text']\n",
    "        text = row['prompt_text']\n",
    "        quotes_from_summary = re.findall(r'\"([^\"]*)\"', summary)\n",
    "        if len(quotes_from_summary)>0:\n",
    "            return [quote in text for quote in quotes_from_summary].count(True)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def spelling(self, text):\n",
    "        \n",
    "        wordlist=text.split()\n",
    "        amount_miss = len(list(self.speller.unknown(wordlist)))\n",
    "\n",
    "        return amount_miss\n",
    "    \n",
    "    def run(self, \n",
    "            prompts: pd.DataFrame,\n",
    "            summaries:pd.DataFrame,\n",
    "            mode:str\n",
    "        ) -> pd.DataFrame:\n",
    "        \n",
    "        # before merge preprocess\n",
    "        prompts[\"prompt_length\"] = prompts[\"prompt_text\"].apply(\n",
    "            lambda x: len(self.tokenizer.encode(x))\n",
    "        )\n",
    "        prompts[\"prompt_tokens\"] = prompts[\"prompt_text\"].apply(\n",
    "            lambda x: self.tokenizer.convert_ids_to_tokens(\n",
    "                self.tokenizer.encode(x), \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        summaries[\"summary_length\"] = summaries[\"text\"].apply(\n",
    "            lambda x: len(self.tokenizer.encode(x))\n",
    "        )\n",
    "        summaries[\"summary_tokens\"] = summaries[\"text\"].apply(\n",
    "            lambda x: self.tokenizer.convert_ids_to_tokens(\n",
    "                self.tokenizer.encode(x), \n",
    "                skip_special_tokens=True\n",
    "            )\n",
    "\n",
    "        )\n",
    "        summaries[\"splling_err_num\"] = summaries[\"text\"].progress_apply(self.spelling)\n",
    "\n",
    "        # merge prompts and summaries\n",
    "        input_df = summaries.merge(prompts, how=\"left\", on=\"prompt_id\")\n",
    "\n",
    "        # after merge preprocess\n",
    "        input_df['length_ratio'] = input_df['summary_length'] / input_df['prompt_length']\n",
    "        \n",
    "        input_df['word_overlap_count'] = input_df.progress_apply(self.word_overlap_count, axis=1)\n",
    "        input_df['bigram_overlap_count'] = input_df.progress_apply(\n",
    "            self.ngram_co_occurrence,args=(2,), axis=1 \n",
    "        )\n",
    "        input_df['trigram_overlap_count'] = input_df.progress_apply(\n",
    "            self.ngram_co_occurrence, args=(3,), axis=1\n",
    "        )\n",
    "        \n",
    "        # Crate dataframe with count of each category NERs overlap for all the summaries\n",
    "        # Because it spends too much time for this feature, I don't use this time.\n",
    "#         ners_count_df  = input_df.progress_apply(\n",
    "#             lambda row: pd.Series(self.ner_overlap_count(row, mode=mode), dtype='float64'), axis=1\n",
    "#         ).fillna(0)\n",
    "#         self.ner_keys = ners_count_df.columns\n",
    "#         ners_count_df['sum'] = ners_count_df.sum(axis=1)\n",
    "#         ners_count_df.columns = ['NER_' + col for col in ners_count_df.columns]\n",
    "#         # join ner count dataframe with train dataframe\n",
    "#         input_df = pd.concat([input_df, ners_count_df], axis=1)\n",
    "        \n",
    "        input_df['quotes_count'] = input_df.progress_apply(self.quotes_count, axis=1)\n",
    "        \n",
    "        return input_df.drop(columns=[\"summary_tokens\", \"prompt_tokens\"])\n",
    "    \n",
    "preprocessor = Preprocessor()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0939091",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 8966.98it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4321.80it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 5149.54it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 5412.01it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 4106.02it/s]\n"
     ]
    }
   ],
   "source": [
    "#train = preprocessor.run(pdf, sdf, mode=\"train\")\n",
    "data = preprocessor.run(pdf, sdf, mode='test')\n",
    "#test = preprocessor.run(prompts_test, summaries_test, mode=\"test\")\n",
    "\n",
    "#test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47aa28e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>student_id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "      <th>summary_length</th>\n",
       "      <th>splling_err_num</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>prompt_length</th>\n",
       "      <th>length_ratio</th>\n",
       "      <th>word_overlap_count</th>\n",
       "      <th>bigram_overlap_count</th>\n",
       "      <th>trigram_overlap_count</th>\n",
       "      <th>quotes_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>000000ffffff</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>111111eeeeee</td>\n",
       "      <td>def789</td>\n",
       "      <td>Example text 2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 2</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>222222cccccc</td>\n",
       "      <td>abc123</td>\n",
       "      <td>Example text 3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>Summarize...</td>\n",
       "      <td>Example Title 1</td>\n",
       "      <td>Heading\\nText...</td>\n",
       "      <td>7</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     student_id prompt_id            text  summary_length  splling_err_num  \\\n",
       "0  000000ffffff    abc123  Example text 1               5                0   \n",
       "1  111111eeeeee    def789  Example text 2               5                0   \n",
       "2  222222cccccc    abc123  Example text 3               5                0   \n",
       "\n",
       "  prompt_question     prompt_title       prompt_text  prompt_length  \\\n",
       "0    Summarize...  Example Title 1  Heading\\nText...              7   \n",
       "1    Summarize...  Example Title 2  Heading\\nText...              7   \n",
       "2    Summarize...  Example Title 1  Heading\\nText...              7   \n",
       "\n",
       "   length_ratio  word_overlap_count  bigram_overlap_count  \\\n",
       "0      0.714286                   0                     0   \n",
       "1      0.714286                   0                     0   \n",
       "2      0.714286                   0                     0   \n",
       "\n",
       "   trigram_overlap_count  quotes_count  \n",
       "0                      0             0  \n",
       "1                      0             0  \n",
       "2                      0             0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "976532ab",
   "metadata": {},
   "source": [
    "## Deberta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8af51ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_inf(example, tokenizer, config):\n",
    "    sep = tokenizer.sep_token;\n",
    "    prompt = sep.join([example[\"prompt_title\"], example[\"prompt_text\"], example[\"prompt_question\"]])\n",
    "    tokenized = tokenizer(\n",
    "        example[\"text\"],\n",
    "        prompt,\n",
    "        padding=False,\n",
    "        truncation=False,\n",
    "        max_length=config.max_seq_length,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        **tokenized\n",
    "    }\n",
    "\n",
    "model_paths = [\n",
    "    #'./pretrain/2308_model/output_fold0_seed42_2308/output_fold0_seed42_2308',\n",
    "    #'./pretrain/2308_model/output_fold1_seed42_2308/output_fold1_seed42_2308',\n",
    "    #'./pretrain/2308_model/output_fold2_seed42_2308/output_fold2_seed42_2308',\n",
    "    './pretrain/2308_model/output_fold3_seed42_2308/output_fold3_seed42_2308'\n",
    "]\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_paths[0]);\n",
    "model_config = AutoConfig.from_pretrained(model_paths[0]);\n",
    "model_config.update({\n",
    "    'hidden_dropout_prob': 0,\n",
    "    'attention_probs_dropout_prob': 0,\n",
    "    'num_labels':2,\n",
    "    'problem_type': 'regression',\n",
    "    'max_position_embeddings': 1600,\n",
    "});\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer = tokenizer,\n",
    "    pad_to_multiple_of=16,\n",
    ")\n",
    "\n",
    "# Do not use pretrained model\n",
    "models = []\n",
    "for model_path in model_paths:\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path, config = model_config);\n",
    "    models.append(model)\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4bc39c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ds = datasets.Dataset.from_pandas(data)\n",
    "tokenized_data_ds = data_ds.map(tokenize_inf, batched = False, num_proc = 4, fn_kwargs= {'tokenizer': tokenizer, 'config': config})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cd13115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "infer_args = TrainingArguments(\n",
    "    output_dir = './',\n",
    "    do_train = False,\n",
    "    do_predict = True,\n",
    "    per_device_eval_batch_size = 16,   \n",
    "    dataloader_drop_last = False,\n",
    "    eval_accumulation_steps=1,\n",
    ")\n",
    "\n",
    "# init trainer\n",
    "trainer = Trainer(\n",
    "    model = model, \n",
    "    args = infer_args,\n",
    "    data_collator = data_collator,\n",
    "    tokenizer = tokenizer\n",
    ")\n",
    "\n",
    "deberta_res = []\n",
    "for model in models:\n",
    "    deberta_res.append(trainer.predict(tokenized_data_ds)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64a431a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deberta_res_mean = np.mean(np.array(deberta_res), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9c65081",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['pred_content_score'] = deberta_res_mean[:,0]\n",
    "data['pred_wording_score'] = deberta_res_mean[:,1]\n",
    "#train.to_csv('./deberta_infer.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d470ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train = pd.read_csv('./train_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d0c3aaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }\n",
    "\n",
    "\n",
    "# mcrmse = compute_mcrmse((train.loc[:, ['pred_content_score', 'pred_wording_score']].values, \n",
    "#                train.loc[:, ['content', 'wording']].values))\n",
    "# mcrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b099a2",
   "metadata": {},
   "source": [
    "### Lightgbm Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e54e9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"content\", \"wording\"]\n",
    "\n",
    "drop_columns = [\"student_id\", \"prompt_id\", \"text\", \n",
    "                \"prompt_question\", \"prompt_title\", \n",
    "                \"prompt_text\"\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73972797",
   "metadata": {},
   "outputs": [],
   "source": [
    "### test load\n",
    "model_targets = ['content', 'wording']\n",
    "\n",
    "lgb_model_dict = {}\n",
    "for model_target in model_targets:\n",
    "    lgb_model_dict[model_target] = [];\n",
    "    for idx in range(4):\n",
    "        lgb_model_dict[model_target].append(lgb.Booster(model_file = f'./lgbt_{model_target}_{idx}.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1caf96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_model_content_0 = lgb_model_dict['content'][0]\n",
    "lgb_model_wording_0 = lgb_model_dict['wording'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0d2ffee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.92559399, -0.11320166],\n",
       "       [-0.89140067, -0.10740623],\n",
       "       [-0.92559399, -0.11320166],\n",
       "       [-0.92559399, -0.11320166]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_content_res = lgb_model_content_0.predict(data.drop(columns = drop_columns)).reshape(-1,1)\n",
    "train_wording_res = lgb_model_wording_0.predict(data.drop(columns = drop_columns)).reshape(-1,1)\n",
    "preds = np.concatenate([train_content_res, train_wording_res], axis = 1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1372adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['content_score'] =  preds[:, 0]\n",
    "data['wording_score'] =  preds[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34f84b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
