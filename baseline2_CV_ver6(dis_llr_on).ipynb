{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log\n",
    "* use 512 instead of 1024 as max_seq_len\n",
    "* turn discriminative learning rates on\n",
    "* use 1536 as max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "AmhfxdCR86jB",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, StratifiedGroupKFold\n",
    "import shutil\n",
    "import time\n",
    "import gc\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import transformers\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForWholeWordMask\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n",
    "from transformers.modeling_outputs  import BaseModelOutput,SequenceClassifierOutput\n",
    "from torch import nn\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup, DataCollatorWithPadding\n",
    "# imports the torch_xla package\n",
    "import wandb\n",
    "from torch.nn.parameter import Parameter\n",
    "#os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "1dMoHaCUn29I",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Model training')\n",
    "    # params of training\n",
    "    parser.add_argument(\n",
    "        \"--fold\", dest=\"fold\", help=\"Train fold\", default=None, type=int)\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        dest='batch_size',\n",
    "        help='Mini batch size of one gpu or cpu',\n",
    "        type=int,\n",
    "        default=None)\n",
    "    return parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6G2r5GS86jE"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Bojtddae86jG",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    pretraining = False\n",
    "    load_pretrained = False\n",
    "    input_path = './input/'\n",
    "    input_type = '2'\n",
    "    model_path = 'microsoft/deberta-v3-large' #  nghuyong/ernie-2.0-large-en studio-ousia/luke-large\n",
    "    model_type = 'pool'\n",
    "    scheduler = 'cosine'  # ['linear', 'cosine']\n",
    "    batch_scheduler = True\n",
    "    num_cycles = 0.5  # 1.5\n",
    "    num_warmup_steps = 0\n",
    "    max_input_length = 1536\n",
    "    max_position_embeddings = 1536\n",
    "    folds = [2]\n",
    "    epochs = 4  # 5\n",
    "    # layer - wise larning rate \n",
    "    discriminative_learning_rate = True\n",
    "    discriminative_learning_rate_num_groups = 1\n",
    "    discriminative_learning_rate_decay_rate = 0.99\n",
    "    # reinint layer\n",
    "    reinit_layers = 0\n",
    "    \n",
    "#     encoder_lr = 5e-6\n",
    "#     head_lr = 5e-6\n",
    "    encoder_lr = 6e-6\n",
    "    head_lr = 1e-5\n",
    "    \n",
    "    min_lr = 1e-7\n",
    "    eps = 1e-7\n",
    "    betas = (0.9, 0.999)\n",
    "    weight_decay = 0\n",
    "    dropout = 0\n",
    "    num_fold = 5\n",
    "    batch_size = 2\n",
    "    seed = 42\n",
    "    OUTPUT_DIR = './pretrain/'\n",
    "    num_workers = 2\n",
    "    device='cuda'\n",
    "    print_freq = 100\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FaoPHcM86jH"
   },
   "source": [
    "## logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RWkPxbhq86jI",
    "outputId": "1f97e316-be01-408d-c79b-1b128aaff9c9",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===============lr_6e-06===============\n",
      "===============seed_42===============\n",
      "===============total_epochs_4===============\n",
      "===============num_warmup_steps_0===============\n"
     ]
    }
   ],
   "source": [
    "def get_logger(filename=CFG.OUTPUT_DIR+ 'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    if not os.path.exists(CFG.OUTPUT_DIR):\n",
    "        os.makedirs(CFG.OUTPUT_DIR)\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "LOGGER.info('===============lr_{}==============='.format(CFG.encoder_lr))\n",
    "LOGGER.info('===============seed_{}==============='.format(CFG.seed))\n",
    "LOGGER.info('===============total_epochs_{}==============='.format(CFG.epochs))\n",
    "LOGGER.info('===============num_warmup_steps_{}==============='.format(CFG.num_warmup_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSqTonVY86jJ"
   },
   "source": [
    "# Preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ACTTNzFL86jK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "pdf = pd.read_csv(f\"{CFG.input_path}/prompts_train.csv\")\n",
    "sdf = pd.read_csv(f\"{CFG.input_path}/summaries_train.csv\")\n",
    "\n",
    "df = pdf.merge(sdf, on=\"prompt_id\")\n",
    "\n",
    "# 4 prompt ids, 4 folds\n",
    "id2fold = {\n",
    "    \"814d6b\": 0,\n",
    "    \"39c16e\": 1,\n",
    "    \"3b9047\": 2,\n",
    "    \"ebad26\": 3,\n",
    "}\n",
    "\n",
    "df[\"fold\"] = df[\"prompt_id\"].map(id2fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "FY5ZdDSh86jL",
    "outputId": "f25ff859-20f1-4a74-f363-eb0dee6c3ba1",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>prompt_question</th>\n",
       "      <th>prompt_title</th>\n",
       "      <th>prompt_text</th>\n",
       "      <th>student_id</th>\n",
       "      <th>text</th>\n",
       "      <th>content</th>\n",
       "      <th>wording</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00791789cc1f</td>\n",
       "      <td>1 element of an ideal tragedy is that it shoul...</td>\n",
       "      <td>-0.210614</td>\n",
       "      <td>-0.471415</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>0086ef22de8f</td>\n",
       "      <td>The three elements of an ideal tragedy are:  H...</td>\n",
       "      <td>-0.970237</td>\n",
       "      <td>-0.417058</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>0094589c7a22</td>\n",
       "      <td>Aristotle states that an ideal tragedy should ...</td>\n",
       "      <td>-0.387791</td>\n",
       "      <td>-0.584181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00cd5736026a</td>\n",
       "      <td>One element of an Ideal tragedy is having a co...</td>\n",
       "      <td>0.088882</td>\n",
       "      <td>-0.594710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39c16e</td>\n",
       "      <td>Summarize at least 3 elements of an ideal trag...</td>\n",
       "      <td>On Tragedy</td>\n",
       "      <td>Chapter 13 \\r\\nAs the sequel to what has alrea...</td>\n",
       "      <td>00d98b8ff756</td>\n",
       "      <td>The 3 ideal of tragedy is how complex you need...</td>\n",
       "      <td>-0.687288</td>\n",
       "      <td>-0.460886</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7160</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff37545b2805</td>\n",
       "      <td>In paragraph two, they would use pickle meat a...</td>\n",
       "      <td>1.520355</td>\n",
       "      <td>-0.292990</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7161</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff4ed38ef099</td>\n",
       "      <td>in the first paragraph  it says \"either can it...</td>\n",
       "      <td>-1.204574</td>\n",
       "      <td>-1.169784</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7162</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff53b94f7ce0</td>\n",
       "      <td>They would have piles of filthy meat on the fl...</td>\n",
       "      <td>0.328739</td>\n",
       "      <td>-1.053294</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7163</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>ff7c7e70df07</td>\n",
       "      <td>They used all sorts of chemical concoctions to...</td>\n",
       "      <td>0.205683</td>\n",
       "      <td>0.380538</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7164</th>\n",
       "      <td>ebad26</td>\n",
       "      <td>Summarize the various ways the factory would u...</td>\n",
       "      <td>Excerpt from The Jungle</td>\n",
       "      <td>With one member trimming beef in a cannery, an...</td>\n",
       "      <td>fffbccfd8a08</td>\n",
       "      <td>The meat would smell sour but the would \"rub i...</td>\n",
       "      <td>1.771596</td>\n",
       "      <td>0.547742</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7165 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt_id                                    prompt_question  \\\n",
       "0       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "1       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "2       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "3       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "4       39c16e  Summarize at least 3 elements of an ideal trag...   \n",
       "...        ...                                                ...   \n",
       "7160    ebad26  Summarize the various ways the factory would u...   \n",
       "7161    ebad26  Summarize the various ways the factory would u...   \n",
       "7162    ebad26  Summarize the various ways the factory would u...   \n",
       "7163    ebad26  Summarize the various ways the factory would u...   \n",
       "7164    ebad26  Summarize the various ways the factory would u...   \n",
       "\n",
       "                 prompt_title  \\\n",
       "0                  On Tragedy   \n",
       "1                  On Tragedy   \n",
       "2                  On Tragedy   \n",
       "3                  On Tragedy   \n",
       "4                  On Tragedy   \n",
       "...                       ...   \n",
       "7160  Excerpt from The Jungle   \n",
       "7161  Excerpt from The Jungle   \n",
       "7162  Excerpt from The Jungle   \n",
       "7163  Excerpt from The Jungle   \n",
       "7164  Excerpt from The Jungle   \n",
       "\n",
       "                                            prompt_text    student_id  \\\n",
       "0     Chapter 13 \\r\\nAs the sequel to what has alrea...  00791789cc1f   \n",
       "1     Chapter 13 \\r\\nAs the sequel to what has alrea...  0086ef22de8f   \n",
       "2     Chapter 13 \\r\\nAs the sequel to what has alrea...  0094589c7a22   \n",
       "3     Chapter 13 \\r\\nAs the sequel to what has alrea...  00cd5736026a   \n",
       "4     Chapter 13 \\r\\nAs the sequel to what has alrea...  00d98b8ff756   \n",
       "...                                                 ...           ...   \n",
       "7160  With one member trimming beef in a cannery, an...  ff37545b2805   \n",
       "7161  With one member trimming beef in a cannery, an...  ff4ed38ef099   \n",
       "7162  With one member trimming beef in a cannery, an...  ff53b94f7ce0   \n",
       "7163  With one member trimming beef in a cannery, an...  ff7c7e70df07   \n",
       "7164  With one member trimming beef in a cannery, an...  fffbccfd8a08   \n",
       "\n",
       "                                                   text   content   wording  \\\n",
       "0     1 element of an ideal tragedy is that it shoul... -0.210614 -0.471415   \n",
       "1     The three elements of an ideal tragedy are:  H... -0.970237 -0.417058   \n",
       "2     Aristotle states that an ideal tragedy should ... -0.387791 -0.584181   \n",
       "3     One element of an Ideal tragedy is having a co...  0.088882 -0.594710   \n",
       "4     The 3 ideal of tragedy is how complex you need... -0.687288 -0.460886   \n",
       "...                                                 ...       ...       ...   \n",
       "7160  In paragraph two, they would use pickle meat a...  1.520355 -0.292990   \n",
       "7161  in the first paragraph  it says \"either can it... -1.204574 -1.169784   \n",
       "7162  They would have piles of filthy meat on the fl...  0.328739 -1.053294   \n",
       "7163  They used all sorts of chemical concoctions to...  0.205683  0.380538   \n",
       "7164  The meat would smell sour but the would \"rub i...  1.771596  0.547742   \n",
       "\n",
       "      fold  \n",
       "0        1  \n",
       "1        1  \n",
       "2        1  \n",
       "3        1  \n",
       "4        1  \n",
       "...    ...  \n",
       "7160     3  \n",
       "7161     3  \n",
       "7162     3  \n",
       "7163     3  \n",
       "7164     3  \n",
       "\n",
       "[7165 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "2bILjGET86jN",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_vpC_oJw86jO"
   },
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "3p6AtTj_86jO",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/deberta-v3-large/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f5ac047b5e0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 432be5ad-1796-4a39-af51-faf5e8bb94dc)')' thrown while requesting HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSHcmlTt86jP"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X-XxwPIR86jP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "mask_lm_datacollator = DataCollatorForWholeWordMask(tokenizer)\n",
    "def data_collator(batch):\n",
    "    input_ids = [{'input_ids':i[0]} for i in batch]\n",
    "    token_type_ids = [i[1] for i in batch]\n",
    "    attention_mask = [i[2] for i in batch]\n",
    "    labels = [i[3] for i in batch]\n",
    "    masked_input = mask_lm_datacollator(input_ids)['input_ids']\n",
    "    return masked_input,\\\n",
    "               torch.stack(token_type_ids),\\\n",
    "               torch.stack(attention_mask),\\\n",
    "               torch.stack(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.prompt_title = df['prompt_title'].values.astype(str)\n",
    "        self.prompt_text = df['prompt_text'].values.astype(str)\n",
    "        self.prompt_question = df['prompt_question'].values.astype(str)\n",
    "        self.text = df['text'].values.astype(str)\n",
    "        self.content = df['content'].values\n",
    "        self.wording = df['wording'].values\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.prompt_title)\n",
    "    \n",
    "    def tokenize(self, example):\n",
    "        sep = self.tokenizer.sep_token\n",
    "        if  CFG.input_type == '1':\n",
    "            prompt = sep.join([example[\"prompt_title\"], example[\"prompt_text\"], example[\"prompt_question\"]])\n",
    "        else:\n",
    "            prompt = example[\"prompt_question\"] \n",
    "        \n",
    "        labels = [float(example[\"content\"]), float(example[\"wording\"])]\n",
    "\n",
    "        tokenized = tokenizer(\n",
    "            example[\"text\"],\n",
    "            prompt,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=CFG.max_input_length,\n",
    "            return_tensors=None,\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            **tokenized,\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        example = {\n",
    "                    \"prompt_title\":self.prompt_title[item],\n",
    "                    \"prompt_text\":self.prompt_text[item],\n",
    "                    \"prompt_question\":self.prompt_question[item],\n",
    "                    \"text\":self.text[item],\n",
    "                    \"content\":self.content[item],\n",
    "                    \"wording\":self.wording[item],\n",
    "                  }\n",
    "        \n",
    "        out = self.tokenize(example)\n",
    "       \n",
    "        return {\n",
    "                'input_ids': torch.as_tensor(out['input_ids'], dtype=torch.long),\n",
    "                'token_type_ids': torch.as_tensor(out['token_type_ids'], dtype=torch.long),\n",
    "                'attention_mask': torch.as_tensor(out['attention_mask'], dtype=torch.long),\n",
    "                'labels': torch.as_tensor(out['labels'], dtype=torch.float),\n",
    "        }\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GP7TFS-c86jQ"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "0A7qv3qn86jQ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_params(module_lst):\n",
    "    for module in module_lst:\n",
    "        for param in module.parameters():\n",
    "            if param.dim() > 1:\n",
    "                torch.nn.init.xavier_uniform_(param)\n",
    "    return\n",
    "\n",
    "class Custom_Bert(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\"output_hidden_states\":True})\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "\n",
    "        dim = config.hidden_size\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        n_weights = 24\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.cls = nn.Sequential(\n",
    "            nn.Linear(dim,1)\n",
    "        )\n",
    "        init_params([self.cls,self.attention])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                )\n",
    "\n",
    "        cls_outputs = torch.stack(\n",
    "            [self.dropout(layer) for layer in base_output['hidden_states'][-24:]], dim=0\n",
    "        )\n",
    "        cls_output = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * cls_outputs).sum(0)\n",
    "\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [torch.sum(self.attention(self.high_dropout(cls_output)) * cls_output, dim=1) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        output = self.cls(logits)\n",
    "        if labels is None:\n",
    "            return output\n",
    "\n",
    "        else:\n",
    "            return (nn.MSELoss()(torch.squeeze(output,1),labels), output)\n",
    "\n",
    "\n",
    "class Custom_Bert_Simple(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "        dim = config.hidden_size\n",
    "        self.dropout = nn.Dropout(p=0)\n",
    "        self.cls = nn.Linear(dim,2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids\n",
    "                               )\n",
    "        output = base_output.last_hidden_state\n",
    "        output = self.cls(torch.mean(output, dim=1))\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=nn.MSELoss()(output,labels),\n",
    "            logits=output, \n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "\n",
    "class GeMText(nn.Module):\n",
    "    def __init__(self, dim = 1, p=3, eps=1e-6):\n",
    "        super(GeMText, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.p = Parameter(torch.ones(1) * p)\n",
    "        self.eps = eps\n",
    "        self.feat_mult = 1\n",
    "\n",
    "    def forward(self, last_hidden_state, attention_mask):\n",
    "        attention_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.shape)\n",
    "        x = (last_hidden_state.clamp(min=self.eps) * attention_mask_expanded).pow(self.p).sum(self.dim)\n",
    "        ret = x / attention_mask_expanded.sum(self.dim).clip(min=self.eps)\n",
    "        ret = ret.pow(1 / self.p)\n",
    "        return ret    \n",
    "\n",
    "class Custom_Bert_Pool(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        self.config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "        print('no pretrained model loaded.')\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=self.config)\n",
    "        \n",
    "#         print('load pretrained model ...');\n",
    "#         self.base = AutoModel.from_pretrained('./input/pretrain/pretrained_model_1009', config = self.config)\n",
    "        \n",
    "        self.pool = GeMText()\n",
    "        self.cls = nn.Linear(self.config.hidden_size,2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                                token_type_ids=token_type_ids\n",
    "                               )\n",
    "        output = base_output.last_hidden_state\n",
    "        output = self.cls(self.pool(output, attention_mask))\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=nn.SmoothL1Loss()(output,labels),\n",
    "            logits=output, \n",
    "            hidden_states=None,\n",
    "            attentions=None\n",
    "        )\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            print(f'Re-initialize {module}')\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "class Custom_Bert_Mean(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.output_hidden_states=True\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "        dim = config.hidden_size\n",
    "        self.dropout = nn.Dropout(p=0)\n",
    "        self.cls = nn.Linear(dim,1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask,labels=None):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                                attention_mask=attention_mask,\n",
    "                            )\n",
    "\n",
    "\n",
    "        output = base_output.hidden_states[-1]\n",
    "        output = self.cls(self.dropout(torch.mean(output, dim=1)))\n",
    "        if labels is None:\n",
    "            return output\n",
    "\n",
    "        else:\n",
    "            return (nn.MSELoss()(torch.squeeze(output,1),labels), output)\n",
    "\n",
    "class Custom_Bert_M(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        config.update({\"output_hidden_states\":True})\n",
    "\n",
    "        self.base = AutoModel.from_pretrained(CFG.model_path, config=config)\n",
    "\n",
    "        dim = config.hidden_size\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.high_dropout = nn.Dropout(p=0.5)\n",
    "\n",
    "        n_weights = 24\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(config.hidden_size, config.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(config.hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        self.cls_0 = nn.Sequential(\n",
    "            nn.Linear(dim,1)\n",
    "        )\n",
    "\n",
    "        self.cls_1 = nn.Linear(dim,5)\n",
    "        init_params([self.cls_0,self.cls_1,self.attention])\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels):\n",
    "        base_output = self.base(input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                             )\n",
    "\n",
    "        cls_outputs = torch.stack(\n",
    "            [self.dropout(layer) for layer in base_output['hidden_states'][-24:]], dim=0\n",
    "        )\n",
    "        cls_output = (torch.softmax(self.layer_weights, dim=0).unsqueeze(1).unsqueeze(1).unsqueeze(1) * cls_outputs).sum(0)\n",
    "\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [torch.sum(self.attention(self.high_dropout(cls_output)) * cls_output, dim=1) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        output_0 = self.cls_0(logits)\n",
    "        output_1 = self.cls_1(logits)\n",
    "        if labels is None:\n",
    "            return output_0\n",
    "\n",
    "        else:\n",
    "            regression_loss = nn.MSELoss()(torch.squeeze(output_0,1),labels)\n",
    "            labels = labels.double()\n",
    "            cls_labels = torch.where(labels==1.,4.0,labels)\n",
    "            cls_labels = torch.where(cls_labels==0.25,1.0,cls_labels)\n",
    "            cls_labels = torch.where(cls_labels==0.5,2.0,cls_labels)\n",
    "            cls_labels = torch.where(cls_labels==0.75,3.0,cls_labels)\n",
    "            cls_labels = cls_labels.long()\n",
    "            cls_loss = nn.CrossEntropyLoss()(output_1, cls_labels)\n",
    "            return ( 0.8 * regression_loss + 0.2 * cls_loss, output_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    if CFG.model_type == 'base':\n",
    "        model_config = AutoConfig.from_pretrained(CFG.model_path)\n",
    "        model_config.update({\n",
    "            \"hidden_dropout_prob\": CFG.dropout,\n",
    "            \"attention_probs_dropout_prob\": CFG.dropout,\n",
    "            \"num_labels\": 2,\n",
    "            \"problem_type\": \"regression\",\n",
    "            \"max_position_embeddings\": CFG.max_position_embeddings\n",
    "        })\n",
    "\n",
    "        #print(model_config)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            CFG.model_path, config=model_config\n",
    "        )\n",
    "    if CFG.model_type == 'simple':\n",
    "        model = Custom_Bert_Simple()\n",
    "    if CFG.model_type == 'pool':\n",
    "        model = Custom_Bert_Pool()\n",
    "        if CFG.reinit_layers > 0:\n",
    "            print(\"==\"*40)\n",
    "            print(f\"Reinitialize the last {CFG.reinit_layers} layer(s).\")\n",
    "            for layer in model.base.encoder.layer[-CFG.reinit_layers:]:\n",
    "                print(\"===\")\n",
    "                layer.apply(model._init_weights)\n",
    "            print(\"==\"*40)\n",
    "        if CFG.load_pretrained:\n",
    "            model.load_state_dict(torch.load('./pretrained/microsoft_deberta-v3-base_best_ema.pth')['model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "op1OHevU86jR"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "class ModelEMA:\n",
    "    \"\"\"Model Exponential Moving Average from https://github.com/rwightman/\n",
    "    pytorch-image-models Keep a moving average of everything in the model\n",
    "    state_dict (parameters and buffers).\n",
    "\n",
    "    This is intended to allow functionality like\n",
    "    https://www.tensorflow.org/api_docs/python/tf/train/\n",
    "    ExponentialMovingAverage\n",
    "    A smoothed version of the weights is necessary for some training\n",
    "    schemes to perform well.\n",
    "    This class is sensitive where it is initialized in the sequence\n",
    "    of model init, GPU assignment and distributed training wrappers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, decay=0.9999, updates=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model (nn.Module): model to apply EMA.\n",
    "            decay (float): ema decay reate.\n",
    "            updates (int): counter of EMA updates.\n",
    "        \"\"\"\n",
    "        # Create EMA(FP32)\n",
    "        self.ema_model = deepcopy(model).eval()\n",
    "        self.ema = self.ema_model\n",
    "        self.updates = updates\n",
    "        # decay exponential ramp (to help early epochs)\n",
    "        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))\n",
    "        for p in self.ema.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "    def update(self, model):\n",
    "        # Update EMA parameters\n",
    "        with torch.no_grad():\n",
    "            self.updates += 1\n",
    "            d = self.decay(self.updates)\n",
    "            msd =  model.state_dict()# model state_dict\n",
    "            for k, v in self.ema.state_dict().items():\n",
    "                if v.dtype.is_floating_point:\n",
    "                    v *= d\n",
    "                    v += (1.0 - d) * msd[k].detach()\n",
    "\n",
    "class EMAHook:\n",
    "    \"\"\"EMAHook used in BEVDepth.\n",
    "\n",
    "    Modified from https://github.com/Megvii-Base\n",
    "    Detection/BEVDepth/blob/main/callbacks/ema.py.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, init_updates=0, decay=0.9990, resume=None, logger=None):\n",
    "        super().__init__()\n",
    "        self.init_updates = init_updates\n",
    "        self.resume = resume\n",
    "        self.decay = decay\n",
    "        self.ema_model = self.before_run(model)\n",
    "        self.logger = logger\n",
    "\n",
    "    def before_run(self, model):\n",
    "        from torch.nn.modules.batchnorm import SyncBatchNorm\n",
    "\n",
    "        bn_model_list = list()\n",
    "        bn_model_dist_group_list = list()\n",
    "        for model_ref in model.modules():\n",
    "            if isinstance(model_ref, SyncBatchNorm):\n",
    "                bn_model_list.append(model_ref)\n",
    "                bn_model_dist_group_list.append(model_ref.process_group)\n",
    "                model_ref.process_group = None\n",
    "        ema_model = ModelEMA(model, self.decay)\n",
    "\n",
    "        for bn_model, dist_group in zip(bn_model_list,\n",
    "                                        bn_model_dist_group_list):\n",
    "            bn_model.process_group = dist_group\n",
    "        ema_model.updates = self.init_updates\n",
    "\n",
    "        if self.resume is not None:\n",
    "            self.logger.info(f'resume ema checkpoint from {self.resume}')\n",
    "            cpt = torch.load(self.resume, map_location='cpu')\n",
    "            load_state_dict(ema_model.ema, cpt['state_dict'])\n",
    "            ema_model.updates = cpt['updates']\n",
    "\n",
    "        return ema_model\n",
    "\n",
    "    def after_train_iter(self, model):\n",
    "        self.ema_model.update(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "O__vnlBV86jR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_mcrmse(eval_pred):\n",
    "    \"\"\"\n",
    "    Calculates mean columnwise root mean squared error\n",
    "    https://www.kaggle.com/competitions/commonlit-evaluate-student-summaries/overview/evaluation\n",
    "    \"\"\"\n",
    "    preds, labels = eval_pred\n",
    "\n",
    "    col_rmse = np.sqrt(np.mean((preds - labels) ** 2, axis=0))\n",
    "    mcrmse = np.mean(col_rmse)\n",
    "\n",
    "    return {\n",
    "        \"content_rmse\": col_rmse[0],\n",
    "        \"wording_rmse\": col_rmse[1],\n",
    "        \"mcrmse\": mcrmse,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "qYJKjn8H86jR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CFG.discriminative_learning_rate_num_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_llr_params(model, type='s'):\n",
    "    \"\"\"\n",
    "    Setup the optimizer.\n",
    "    We provide a reasonable default that works well. If you want to use something else, you can pass a tuple in the\n",
    "    Trainer's init through :obj:`optimizers`, or subclass and override this method in a subclass.\n",
    "\n",
    "    MODIFIED VERSION:\n",
    "    * added support for differential learning rates per layer\n",
    "\n",
    "    reference: https://github.com/huggingface/transformers/blob/05fa1a7ac17bb7aa07b9e0c1e138ecb31a28bbfe/src/transformers/trainer.py#L804\n",
    "    \"\"\"\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\"]\n",
    "    ### ADDED\n",
    "    if CFG.discriminative_learning_rate:\n",
    "\n",
    "        num_layers = model.config.num_hidden_layers\n",
    "\n",
    "        learning_rate_powers = range(0, num_layers, num_layers//CFG.discriminative_learning_rate_num_groups)\n",
    "        layer_wise_learning_rates = [\n",
    "            pow(CFG.discriminative_learning_rate_decay_rate, power) * CFG.encoder_lr \n",
    "            for power in learning_rate_powers \n",
    "            for _ in range(num_layers//CFG.discriminative_learning_rate_num_groups)\n",
    "          ]\n",
    "        layer_wise_learning_rates = layer_wise_learning_rates[::-1]\n",
    "        print('Layer-wise learning rates:', layer_wise_learning_rates)\n",
    "\n",
    "        # group embedding paramters from the transformer encoder\n",
    "        embedding_layer = model.base.embeddings\n",
    "        optimizer_grouped_parameters = [\n",
    "          {\n",
    "              \"params\": [p for n, p in embedding_layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "              \"lr\": pow(CFG.discriminative_learning_rate_decay_rate, num_layers) * CFG.encoder_lr ,\n",
    "              \"weight_decay\": CFG.weight_decay,\n",
    "          },\n",
    "          {\n",
    "              \"params\": [p for n, p in embedding_layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "              \"lr\": pow(CFG.discriminative_learning_rate_decay_rate, num_layers) * CFG.encoder_lr ,\n",
    "              \"weight_decay\": 0.0,\n",
    "          },\n",
    "        ]\n",
    "\n",
    "        # group encoding paramters from the transformer encoder\n",
    "        encoding_layers = [layer for layer in model.base.encoder.layer]\n",
    "        for i, layer in enumerate(encoding_layers):\n",
    "            optimizer_grouped_parameters += [\n",
    "                {\n",
    "                    \"params\": [p for n, p in layer.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                    \"lr\": layer_wise_learning_rates[i],\n",
    "                    \"weight_decay\": CFG.weight_decay,\n",
    "                },\n",
    "                {\n",
    "                    \"params\": [p for n, p in layer.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                    \"lr\": layer_wise_learning_rates[i],\n",
    "                    \"weight_decay\": 0.0,\n",
    "                },\n",
    "            ]    \n",
    "        print(f\"Detected unattached modules in model.encoder: {[n for n, p in model.base.encoder.named_parameters() if not n.startswith('layer')]}\")\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.base.encoder.named_parameters() if not n.startswith('layer') and not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": layer_wise_learning_rates[-1],\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.base.encoder.named_parameters() if not n.startswith('layer') and any(nd in n for nd in no_decay)],\n",
    "                \"lr\": layer_wise_learning_rates[-1],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        # group paramters from the task specific head\n",
    "        optimizer_grouped_parameters += [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if 'base' not in n and not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.head_lr,\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if 'base' not in n and any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.head_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    ### END ADDED\n",
    "    else:\n",
    "        # group paramters for the entire network\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.encoder_lr,\n",
    "                \"weight_decay\": CFG.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"lr\": CFG.encoder_lr,\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3HIHxVsj86jS",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def valid_fn(valid_loader, model, device):\n",
    "    losses = AverageMeter()\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    labels = []\n",
    "    start = end = time.time()\n",
    "    for step, batch in enumerate(valid_loader):\n",
    "        for key, value in batch.items():\n",
    "            batch[key] = value.to(device)\n",
    "        batch_size = batch['labels'].size(0)\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**batch)\n",
    "        label = batch['labels']\n",
    "        loss, logits = model_output.loss, model_output.logits\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(logits.to('cpu').numpy())\n",
    "        labels.append(label.to('cpu').numpy())\n",
    "        end = time.time()\n",
    "    predictions = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    del loss, logits\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return losses.avg, predictions, labels\n",
    "\n",
    "def train_fn(train_loader, model, optimizer, epoch, scheduler, device, valid_loader, start_time, best_score, best_score_ema,ema_hook,wandb, fold):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time.time()\n",
    "    global_step = 0\n",
    "    for step, batch in enumerate(train_loader):\n",
    "        for key, value in batch.items():\n",
    "            batch[key] = value.to(device)\n",
    "        batch_size = batch['labels'].size(0)\n",
    "        loss = model(**batch).loss\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 10)\n",
    "        optimizer.step()\n",
    "        ema_hook.after_train_iter(model)\n",
    "        global_step += 1\n",
    "        scheduler.step()\n",
    "        end = time.time()\n",
    "        \n",
    "        wandb.log({\n",
    "                'train loss': loss.item(),\n",
    "                'step': global_step,\n",
    "                'epoch': epoch,\n",
    "                'fold': fold,\n",
    "                'batch_size':CFG.batch_size\n",
    "            })\n",
    "        \n",
    "        if step % CFG.print_freq == 0 or step == (len(train_loader) - 1):\n",
    "            print('Epoch: [{0}][{1}/{2}] '\n",
    "                  'Elapsed {remain:s} '\n",
    "                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n",
    "                  'Grad: {grad_norm:.4f}  '\n",
    "                  'LR: {lr:.8f}  '\n",
    "                  .format(epoch + 1, step, len(train_loader),\n",
    "                          remain=timeSince(start, float(step + 1) / len(train_loader)),\n",
    "                          loss=losses,\n",
    "                          grad_norm=grad_norm,\n",
    "                          lr=scheduler.get_lr()[0]))\n",
    "            \n",
    "            # eval\n",
    "            avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, model, CFG.device)\n",
    "\n",
    "            # scoring\n",
    "            score = compute_mcrmse((predictions, valid_labels))\n",
    "\n",
    "\n",
    "            content_rmse, wording_rmse, mcrmse = list(score.values())\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch + 1} avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - content_rmse: {content_rmse:.4f} - wording_rmse: {wording_rmse:.4f} - mcrmse: {mcrmse:.4f}')\n",
    "            \n",
    "            \n",
    "            if best_score > score['mcrmse']:\n",
    "                if best_score != float('inf'):\n",
    "                    os.remove(CFG.OUTPUT_DIR + \"{}_best{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score))\n",
    "                best_score = score['mcrmse']\n",
    "                best_predictions = predictions\n",
    "                LOGGER.info(f'Epoch {epoch + 1} - Save Best Score: {best_score:.4f} Model')\n",
    "                torch.save({'model': model.state_dict(),\n",
    "                            'predictions': predictions},\n",
    "                           CFG.OUTPUT_DIR + \"{}_best{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score))\n",
    "            \n",
    "            \n",
    "            avg_val_loss, predictions, valid_labels = valid_fn(valid_loader, ema_hook.ema_model.ema, CFG.device)\n",
    "            # ema scoring\n",
    "            ema_score = compute_mcrmse((predictions, valid_labels))\n",
    "\n",
    "\n",
    "            content_rmse, wording_rmse, mcrmse = list(ema_score.values())\n",
    "            elapsed = time.time() - start_time\n",
    "\n",
    "            LOGGER.info(\n",
    "                f'Epoch {epoch + 1} avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n",
    "            LOGGER.info(f'Epoch {epoch + 1} - ema_content_rmse: {content_rmse:.4f} - ema_wording_rmse: {wording_rmse:.4f} - ema_mcrmse: {mcrmse:.4f}')\n",
    "            \n",
    "            \n",
    "            if best_score_ema > ema_score['mcrmse']:\n",
    "                if best_score_ema != float('inf'):\n",
    "                    os.remove(CFG.OUTPUT_DIR + \"{}_best_ema{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold, best_score_ema))\n",
    "                best_score_ema = ema_score['mcrmse']\n",
    "                best_predictions = predictions\n",
    "                LOGGER.info(f'Epoch {epoch + 1} - ema_Save Best Score: {best_score_ema:.4f} Model')\n",
    "                torch.save({'model': ema_hook.ema_model.ema.state_dict(),\n",
    "                            'predictions': predictions},\n",
    "                           CFG.OUTPUT_DIR + \"{}_best_ema{}_{}.pth\".format(CFG.model_path.replace('/', '_'),fold,best_score_ema))\n",
    "            \n",
    "            wandb.log({\n",
    "                'learning rate': optimizer.param_groups[0]['lr'],\n",
    "                'validation mcrmse': score['mcrmse'],\n",
    "                'validation ema mcrmse': ema_score['mcrmse'],\n",
    "                'step': global_step,\n",
    "                'epoch': epoch,\n",
    "            })\n",
    "            \n",
    "        del batch, loss, grad_norm\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    return losses.avg, best_score, best_score_ema\n",
    "\n",
    "\n",
    "\n",
    "def train_loop():\n",
    "    LOGGER.info(f\"========== training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    wandb.init(project='kaggle-commonlit-eval-student-summaries-3009')\n",
    "    wandb.config = dict(epochs=CFG.epochs, \n",
    "                            batch_size=CFG.batch_size, \n",
    "                            learning_rate=CFG.encoder_lr,\n",
    "                            save_checkpoint=True,\n",
    "                            )\n",
    "    for fold in CFG.folds:\n",
    "        \n",
    "        if CFG.pretraining:\n",
    "            tr_data = pd.read_csv('tmp_pessudo.csv')\n",
    "            tr_data['prompt_title'] = ''\n",
    "            tr_data = tr_data[-(tr_data['prompt_question'].isin(pdf['prompt_question'].tolist()))]\n",
    "            va_data = df #df[df['fold']==fold].reset_index(drop=True)\n",
    "        else:\n",
    "            tr_data = df[df['fold']!=fold].reset_index(drop=True)\n",
    "            va_data = df[df['fold']==fold].reset_index(drop=True)\n",
    "        train_dataset = TrainDataset(tr_data, tokenizer)\n",
    "        valid_dataset = TrainDataset(va_data, tokenizer)\n",
    "        train_loader = DataLoader(train_dataset,\n",
    "                                  batch_size=CFG.batch_size,\n",
    "                                  shuffle=True,\n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "        valid_loader = DataLoader(valid_dataset,\n",
    "                                  batch_size=CFG.batch_size,\n",
    "                                  shuffle=False,\n",
    "                                  num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "        # ====================================================\n",
    "        # model & optimizer\n",
    "        # ====================================================\n",
    "        model = build_model()\n",
    "        #model = AutoModelForSequenceClassification.from_pretrained(CFG.model_path, num_labels=1)\n",
    "        model.to(CFG.device)\n",
    "        # for param in model.base.parameters():\n",
    "        #         param.requires_grad = False\n",
    "        ema_hook = EMAHook(model, init_updates=3000, logger=LOGGER)\n",
    "        def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n",
    "            param_optimizer = list(model.named_parameters())\n",
    "            no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "            optimizer_parameters = [\n",
    "                {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                 'lr': encoder_lr, 'weight_decay': weight_decay},\n",
    "                {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                 'lr': encoder_lr, 'weight_decay': 0.0},\n",
    "            ]\n",
    "            return optimizer_parameters\n",
    "\n",
    "        optimizer_parameters = get_optimizer_llr_params(model)\n",
    "        optimizer = AdamW(optimizer_parameters, eps=CFG.eps, betas=CFG.betas)\n",
    "\n",
    "\n",
    "        \n",
    "        # ====================================================\n",
    "        # scheduler\n",
    "        # ====================================================\n",
    "        def get_scheduler(cfg, optimizer, num_train_steps):\n",
    "            cfg.num_warmup_steps = cfg.num_warmup_steps * num_train_steps\n",
    "            if cfg.scheduler == 'linear':\n",
    "                scheduler = get_linear_schedule_with_warmup(\n",
    "                    optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps\n",
    "                )\n",
    "            elif cfg.scheduler == 'cosine':\n",
    "                scheduler = get_cosine_schedule_with_warmup(\n",
    "                    optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps,\n",
    "                    num_cycles=cfg.num_cycles\n",
    "                )\n",
    "            return scheduler\n",
    "\n",
    "        num_train_steps = int(len(train_dataset) / CFG.batch_size * CFG.epochs)\n",
    "        scheduler = get_scheduler(CFG, optimizer, num_train_steps)\n",
    "\n",
    "        # ====================================================\n",
    "        # loop\n",
    "        # ====================================================\n",
    "        # criterion = torch.nn.CrossEntropyLoss(ignore_index=- 1)\n",
    "\n",
    "        # criterion = LabelSmoothingLoss()\n",
    "        best_score = float('inf')\n",
    "        best_score_ema = float('inf')\n",
    "        for epoch in range(CFG.epochs):\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # train\n",
    "            avg_loss, best_score, best_score_ema = train_fn(train_loader, model, optimizer, epoch, scheduler, CFG.device, valid_loader, start_time, best_score, best_score_ema ,ema_hook, wandb,fold)\n",
    "\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        del scheduler, optimizer, model\n",
    "    return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbgHecjCyz5D",
    "outputId": "09f6fba5-dd11-4adc-a85c-8805c6d6c90d",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "========== training ==========\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpeng_sun\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d8012c98300471bad7df7616b521190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016669448977336288, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.15.11 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/workspace/commonlit/wandb/run-20230930_153306-7gjb6adf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-3009/runs/7gjb6adf' target=\"_blank\">drawn-butterfly-2</a></strong> to <a href='https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-3009' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-3009' target=\"_blank\">https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-3009</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-3009/runs/7gjb6adf' target=\"_blank\">https://wandb.ai/peng_sun/kaggle-commonlit-eval-student-summaries-3009/runs/7gjb6adf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/deberta-v3-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7f5a992f1ac0>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 4f5e3306-f821-427a-b6f1-97ab6b7ccde2)')' thrown while requesting HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no pretrained model loaded.\n",
      "Layer-wise learning rates: [6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06, 6e-06]\n",
      "Detected unattached modules in model.encoder: ['rel_embeddings.weight', 'LayerNorm.weight', 'LayerNorm.bias']\n",
      "Epoch: [1][0/2578] Elapsed 0m 2s (remain 88m 15s) Loss: 0.6022(0.6022) Grad: 14.0080  LR: 0.00000471  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.5362  time: 533s\n",
      "Epoch 1 - content_rmse: 1.2815 - wording_rmse: 0.9582 - mcrmse: 1.1199\n",
      "Epoch 1 - Save Best Score: 1.1199 Model\n",
      "Epoch 1 avg_val_loss: 0.5506  time: 1066s\n",
      "Epoch 1 - ema_content_rmse: 1.3035 - ema_wording_rmse: 0.9727 - ema_mcrmse: 1.1381\n",
      "Epoch 1 - ema_Save Best Score: 1.1381 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][100/2578] Elapsed 20m 21s (remain 499m 14s) Loss: 0.1278(0.2978) Grad: 7.9477  LR: 0.00000471  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.4376  time: 1753s\n",
      "Epoch 1 - content_rmse: 0.9091 - wording_rmse: 1.0723 - mcrmse: 0.9907\n",
      "Epoch 1 - Save Best Score: 0.9907 Model\n",
      "Epoch 1 avg_val_loss: 0.3747  time: 2286s\n",
      "Epoch 1 - ema_content_rmse: 0.8398 - ema_wording_rmse: 0.9779 - ema_mcrmse: 0.9088\n",
      "Epoch 1 - ema_Save Best Score: 0.9088 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][200/2578] Elapsed 40m 39s (remain 480m 52s) Loss: 0.1243(0.2460) Grad: 5.1945  LR: 0.00000471  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2331  time: 2972s\n",
      "Epoch 1 - content_rmse: 0.5750 - wording_rmse: 0.8425 - mcrmse: 0.7088\n",
      "Epoch 1 - Save Best Score: 0.7088 Model\n",
      "Epoch 1 avg_val_loss: 0.2833  time: 3506s\n",
      "Epoch 1 - ema_content_rmse: 0.6621 - ema_wording_rmse: 0.9199 - ema_mcrmse: 0.7910\n",
      "Epoch 1 - ema_Save Best Score: 0.7910 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][300/2578] Elapsed 60m 58s (remain 461m 18s) Loss: 0.0312(0.2253) Grad: 3.4290  LR: 0.00000470  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2222  time: 4189s\n",
      "Epoch 1 - content_rmse: 0.5539 - wording_rmse: 0.8374 - mcrmse: 0.6957\n",
      "Epoch 1 - Save Best Score: 0.6957 Model\n",
      "Epoch 1 avg_val_loss: 0.2858  time: 4723s\n",
      "Epoch 1 - ema_content_rmse: 0.6247 - ema_wording_rmse: 0.9783 - ema_mcrmse: 0.8015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][400/2578] Elapsed 81m 12s (remain 440m 53s) Loss: 0.0857(0.2096) Grad: 4.1975  LR: 0.00000470  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2435  time: 5404s\n",
      "Epoch 1 - content_rmse: 0.5917 - wording_rmse: 0.8700 - mcrmse: 0.7309\n",
      "Epoch 1 avg_val_loss: 0.2913  time: 5935s\n",
      "Epoch 1 - ema_content_rmse: 0.6651 - ema_wording_rmse: 0.9467 - ema_mcrmse: 0.8059\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][500/2578] Elapsed 101m 24s (remain 420m 26s) Loss: 0.3427(0.1959) Grad: 18.0536  LR: 0.00000469  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.4040  time: 6617s\n",
      "Epoch 1 - content_rmse: 0.8319 - wording_rmse: 1.1028 - mcrmse: 0.9673\n",
      "Epoch 1 avg_val_loss: 0.3367  time: 7148s\n",
      "Epoch 1 - ema_content_rmse: 0.7329 - ema_wording_rmse: 1.0188 - ema_mcrmse: 0.8758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][600/2578] Elapsed 121m 37s (remain 400m 5s) Loss: 0.1304(0.1865) Grad: 8.6544  LR: 0.00000467  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2519  time: 7829s\n",
      "Epoch 1 - content_rmse: 0.5789 - wording_rmse: 0.9112 - mcrmse: 0.7451\n",
      "Epoch 1 avg_val_loss: 0.2853  time: 8360s\n",
      "Epoch 1 - ema_content_rmse: 0.6349 - ema_wording_rmse: 0.9624 - ema_mcrmse: 0.7986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][700/2578] Elapsed 141m 50s (remain 379m 47s) Loss: 0.0941(0.1819) Grad: 3.5276  LR: 0.00000466  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2939  time: 9041s\n",
      "Epoch 1 - content_rmse: 0.7531 - wording_rmse: 0.8891 - mcrmse: 0.8211\n",
      "Epoch 1 avg_val_loss: 0.2653  time: 9572s\n",
      "Epoch 1 - ema_content_rmse: 0.7078 - ema_wording_rmse: 0.8399 - ema_mcrmse: 0.7738\n",
      "Epoch 1 - ema_Save Best Score: 0.7738 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][800/2578] Elapsed 162m 5s (remain 359m 35s) Loss: 0.3037(0.1755) Grad: 13.9654  LR: 0.00000464  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2637  time: 10257s\n",
      "Epoch 1 - content_rmse: 0.6095 - wording_rmse: 0.9379 - mcrmse: 0.7737\n",
      "Epoch 1 avg_val_loss: 0.2520  time: 10788s\n",
      "Epoch 1 - ema_content_rmse: 0.6027 - ema_wording_rmse: 0.9063 - ema_mcrmse: 0.7545\n",
      "Epoch 1 - ema_Save Best Score: 0.7545 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][900/2578] Elapsed 182m 25s (remain 339m 32s) Loss: 0.3047(0.1707) Grad: 9.6131  LR: 0.00000463  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2101  time: 11477s\n",
      "Epoch 1 - content_rmse: 0.5845 - wording_rmse: 0.7719 - mcrmse: 0.6782\n",
      "Epoch 1 - Save Best Score: 0.6782 Model\n",
      "Epoch 1 avg_val_loss: 0.2056  time: 12012s\n",
      "Epoch 1 - ema_content_rmse: 0.5690 - ema_wording_rmse: 0.7668 - ema_mcrmse: 0.6679\n",
      "Epoch 1 - ema_Save Best Score: 0.6679 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1000/2578] Elapsed 202m 44s (remain 319m 24s) Loss: 0.5769(0.1658) Grad: 17.0220  LR: 0.00000461  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2020  time: 12695s\n",
      "Epoch 1 - content_rmse: 0.5322 - wording_rmse: 0.7928 - mcrmse: 0.6625\n",
      "Epoch 1 - Save Best Score: 0.6625 Model\n",
      "Epoch 1 avg_val_loss: 0.2061  time: 13228s\n",
      "Epoch 1 - ema_content_rmse: 0.5208 - ema_wording_rmse: 0.8142 - ema_mcrmse: 0.6675\n",
      "Epoch 1 - ema_Save Best Score: 0.6675 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1100/2578] Elapsed 223m 0s (remain 299m 10s) Loss: 0.0134(0.1629) Grad: 1.9312  LR: 0.00000458  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2171  time: 13912s\n",
      "Epoch 1 - content_rmse: 0.5721 - wording_rmse: 0.8196 - mcrmse: 0.6958\n",
      "Epoch 1 avg_val_loss: 0.2235  time: 14443s\n",
      "Epoch 1 - ema_content_rmse: 0.5735 - ema_wording_rmse: 0.8377 - ema_mcrmse: 0.7056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1200/2578] Elapsed 243m 14s (remain 278m 53s) Loss: 0.0299(0.1580) Grad: 3.4412  LR: 0.00000456  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2354  time: 15126s\n",
      "Epoch 1 - content_rmse: 0.5082 - wording_rmse: 0.9305 - mcrmse: 0.7194\n",
      "Epoch 1 avg_val_loss: 0.2808  time: 15657s\n",
      "Epoch 1 - ema_content_rmse: 0.5609 - ema_wording_rmse: 1.0308 - ema_mcrmse: 0.7959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1300/2578] Elapsed 263m 29s (remain 258m 37s) Loss: 0.0295(0.1542) Grad: 1.5759  LR: 0.00000453  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2222  time: 16341s\n",
      "Epoch 1 - content_rmse: 0.5045 - wording_rmse: 0.8892 - mcrmse: 0.6969\n",
      "Epoch 1 avg_val_loss: 0.2324  time: 16872s\n",
      "Epoch 1 - ema_content_rmse: 0.5323 - ema_wording_rmse: 0.9032 - ema_mcrmse: 0.7177\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1400/2578] Elapsed 283m 42s (remain 238m 20s) Loss: 0.1861(0.1538) Grad: 9.4352  LR: 0.00000450  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2386  time: 17555s\n",
      "Epoch 1 - content_rmse: 0.6217 - wording_rmse: 0.8365 - mcrmse: 0.7291\n",
      "Epoch 1 avg_val_loss: 0.2217  time: 18085s\n",
      "Epoch 1 - ema_content_rmse: 0.5870 - ema_wording_rmse: 0.8154 - ema_mcrmse: 0.7012\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1500/2578] Elapsed 303m 55s (remain 218m 4s) Loss: 0.0711(0.1518) Grad: 5.1481  LR: 0.00000447  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2112  time: 18767s\n",
      "Epoch 1 - content_rmse: 0.5833 - wording_rmse: 0.7780 - mcrmse: 0.6807\n",
      "Epoch 1 avg_val_loss: 0.2105  time: 19298s\n",
      "Epoch 1 - ema_content_rmse: 0.5274 - ema_wording_rmse: 0.8425 - ema_mcrmse: 0.6850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1600/2578] Elapsed 324m 8s (remain 197m 48s) Loss: 0.1429(0.1496) Grad: 11.0712  LR: 0.00000444  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2064  time: 19979s\n",
      "Epoch 1 - content_rmse: 0.5005 - wording_rmse: 0.8487 - mcrmse: 0.6746\n",
      "Epoch 1 avg_val_loss: 0.2166  time: 20509s\n",
      "Epoch 1 - ema_content_rmse: 0.5236 - ema_wording_rmse: 0.8662 - ema_mcrmse: 0.6949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1700/2578] Elapsed 344m 20s (remain 177m 31s) Loss: 0.1033(0.1482) Grad: 7.9873  LR: 0.00000440  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2281  time: 21191s\n",
      "Epoch 1 - content_rmse: 0.5279 - wording_rmse: 0.9105 - mcrmse: 0.7192\n",
      "Epoch 1 avg_val_loss: 0.2763  time: 21722s\n",
      "Epoch 1 - ema_content_rmse: 0.5826 - ema_wording_rmse: 1.0163 - ema_mcrmse: 0.7994\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1800/2578] Elapsed 364m 31s (remain 157m 16s) Loss: 0.0466(0.1469) Grad: 2.9197  LR: 0.00000437  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2244  time: 22403s\n",
      "Epoch 1 - content_rmse: 0.5871 - wording_rmse: 0.8353 - mcrmse: 0.7112\n",
      "Epoch 1 avg_val_loss: 0.2134  time: 22934s\n",
      "Epoch 1 - ema_content_rmse: 0.5577 - ema_wording_rmse: 0.8255 - ema_mcrmse: 0.6916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][1900/2578] Elapsed 384m 44s (remain 137m 1s) Loss: 0.0420(0.1446) Grad: 2.7557  LR: 0.00000433  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2275  time: 23615s\n",
      "Epoch 1 - content_rmse: 0.5242 - wording_rmse: 0.9021 - mcrmse: 0.7131\n",
      "Epoch 1 avg_val_loss: 0.2673  time: 24146s\n",
      "Epoch 1 - ema_content_rmse: 0.5596 - ema_wording_rmse: 0.9995 - ema_mcrmse: 0.7795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2000/2578] Elapsed 404m 56s (remain 116m 45s) Loss: 0.0268(0.1424) Grad: 1.3120  LR: 0.00000429  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1831  time: 24828s\n",
      "Epoch 1 - content_rmse: 0.5043 - wording_rmse: 0.7597 - mcrmse: 0.6320\n",
      "Epoch 1 - Save Best Score: 0.6320 Model\n",
      "Epoch 1 avg_val_loss: 0.2069  time: 25362s\n",
      "Epoch 1 - ema_content_rmse: 0.5233 - ema_wording_rmse: 0.8324 - ema_mcrmse: 0.6779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2100/2578] Elapsed 425m 14s (remain 96m 32s) Loss: 0.0250(0.1409) Grad: 3.1018  LR: 0.00000425  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2260  time: 26046s\n",
      "Epoch 1 - content_rmse: 0.5348 - wording_rmse: 0.8816 - mcrmse: 0.7082\n",
      "Epoch 1 avg_val_loss: 0.2400  time: 26577s\n",
      "Epoch 1 - ema_content_rmse: 0.5638 - ema_wording_rmse: 0.9056 - ema_mcrmse: 0.7347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2200/2578] Elapsed 445m 28s (remain 76m 18s) Loss: 0.1471(0.1389) Grad: 9.5807  LR: 0.00000420  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1922  time: 27261s\n",
      "Epoch 1 - content_rmse: 0.5211 - wording_rmse: 0.7613 - mcrmse: 0.6412\n",
      "Epoch 1 avg_val_loss: 0.1860  time: 27792s\n",
      "Epoch 1 - ema_content_rmse: 0.5013 - ema_wording_rmse: 0.7675 - ema_mcrmse: 0.6344\n",
      "Epoch 1 - ema_Save Best Score: 0.6344 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2300/2578] Elapsed 465m 51s (remain 56m 4s) Loss: 0.0212(0.1382) Grad: 2.0552  LR: 0.00000416  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2634  time: 28484s\n",
      "Epoch 1 - content_rmse: 0.5787 - wording_rmse: 0.9691 - mcrmse: 0.7739\n",
      "Epoch 1 avg_val_loss: 0.2317  time: 29015s\n",
      "Epoch 1 - ema_content_rmse: 0.5341 - ema_wording_rmse: 0.9074 - ema_mcrmse: 0.7207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2400/2578] Elapsed 486m 5s (remain 35m 50s) Loss: 0.1084(0.1370) Grad: 7.4028  LR: 0.00000411  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.2009  time: 29697s\n",
      "Epoch 1 - content_rmse: 0.5014 - wording_rmse: 0.8236 - mcrmse: 0.6625\n",
      "Epoch 1 avg_val_loss: 0.2006  time: 30228s\n",
      "Epoch 1 - ema_content_rmse: 0.5192 - ema_wording_rmse: 0.8085 - ema_mcrmse: 0.6639\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2500/2578] Elapsed 506m 19s (remain 15m 35s) Loss: 0.0240(0.1359) Grad: 1.2281  LR: 0.00000406  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1983  time: 30910s\n",
      "Epoch 1 - content_rmse: 0.5119 - wording_rmse: 0.8123 - mcrmse: 0.6621\n",
      "Epoch 1 avg_val_loss: 0.2385  time: 31442s\n",
      "Epoch 1 - ema_content_rmse: 0.5564 - ema_wording_rmse: 0.9135 - ema_mcrmse: 0.7349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][2577/2578] Elapsed 526m 3s (remain 0m 0s) Loss: 0.0235(0.1349) Grad: 2.3248  LR: 0.00000402  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 avg_val_loss: 0.1981  time: 32096s\n",
      "Epoch 1 - content_rmse: 0.5488 - wording_rmse: 0.7679 - mcrmse: 0.6583\n",
      "Epoch 1 avg_val_loss: 0.1978  time: 32627s\n",
      "Epoch 1 - ema_content_rmse: 0.5435 - ema_wording_rmse: 0.7723 - ema_mcrmse: 0.6579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/2578] Elapsed 0m 1s (remain 63m 30s) Loss: 0.0749(0.0749) Grad: 5.3975  LR: 0.00000402  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1978  time: 534s\n",
      "Epoch 2 - content_rmse: 0.5466 - wording_rmse: 0.7688 - mcrmse: 0.6577\n",
      "Epoch 2 avg_val_loss: 0.1978  time: 1065s\n",
      "Epoch 2 - ema_content_rmse: 0.5436 - ema_wording_rmse: 0.7721 - ema_mcrmse: 0.6579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][100/2578] Elapsed 20m 18s (remain 497m 53s) Loss: 0.0016(0.0844) Grad: 0.8350  LR: 0.00000397  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2361  time: 1750s\n",
      "Epoch 2 - content_rmse: 0.5812 - wording_rmse: 0.8788 - mcrmse: 0.7300\n",
      "Epoch 2 avg_val_loss: 0.2041  time: 2281s\n",
      "Epoch 2 - ema_content_rmse: 0.5356 - ema_wording_rmse: 0.8069 - ema_mcrmse: 0.6713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][200/2578] Elapsed 40m 34s (remain 479m 45s) Loss: 0.0560(0.0833) Grad: 4.3733  LR: 0.00000392  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1949  time: 2966s\n",
      "Epoch 2 - content_rmse: 0.5126 - wording_rmse: 0.8025 - mcrmse: 0.6576\n",
      "Epoch 2 avg_val_loss: 0.1992  time: 3497s\n",
      "Epoch 2 - ema_content_rmse: 0.5167 - ema_wording_rmse: 0.8090 - ema_mcrmse: 0.6629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][300/2578] Elapsed 60m 49s (remain 460m 7s) Loss: 0.0735(0.0782) Grad: 6.9161  LR: 0.00000386  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1873  time: 4181s\n",
      "Epoch 2 - content_rmse: 0.5310 - wording_rmse: 0.7410 - mcrmse: 0.6360\n",
      "Epoch 2 avg_val_loss: 0.1988  time: 4712s\n",
      "Epoch 2 - ema_content_rmse: 0.5060 - ema_wording_rmse: 0.8245 - ema_mcrmse: 0.6653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][400/2578] Elapsed 81m 3s (remain 440m 5s) Loss: 0.0183(0.0770) Grad: 3.0849  LR: 0.00000381  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1906  time: 5395s\n",
      "Epoch 2 - content_rmse: 0.5085 - wording_rmse: 0.7770 - mcrmse: 0.6428\n",
      "Epoch 2 avg_val_loss: 0.1928  time: 5925s\n",
      "Epoch 2 - ema_content_rmse: 0.5101 - ema_wording_rmse: 0.7917 - ema_mcrmse: 0.6509\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][500/2578] Elapsed 101m 17s (remain 419m 56s) Loss: 0.1683(0.0793) Grad: 10.2183  LR: 0.00000375  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2722  time: 6610s\n",
      "Epoch 2 - content_rmse: 0.5564 - wording_rmse: 1.0260 - mcrmse: 0.7912\n",
      "Epoch 2 avg_val_loss: 0.2118  time: 7142s\n",
      "Epoch 2 - ema_content_rmse: 0.5231 - ema_wording_rmse: 0.8632 - ema_mcrmse: 0.6932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][600/2578] Elapsed 121m 34s (remain 399m 56s) Loss: 0.0043(0.0812) Grad: 1.3493  LR: 0.00000369  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1783  time: 7826s\n",
      "Epoch 2 - content_rmse: 0.5179 - wording_rmse: 0.7199 - mcrmse: 0.6189\n",
      "Epoch 2 - Save Best Score: 0.6189 Model\n",
      "Epoch 2 avg_val_loss: 0.1828  time: 8360s\n",
      "Epoch 2 - ema_content_rmse: 0.5064 - ema_wording_rmse: 0.7559 - ema_mcrmse: 0.6311\n",
      "Epoch 2 - ema_Save Best Score: 0.6311 Model\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][700/2578] Elapsed 141m 55s (remain 380m 0s) Loss: 0.0264(0.0811) Grad: 3.5557  LR: 0.00000363  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1889  time: 9047s\n",
      "Epoch 2 - content_rmse: 0.4952 - wording_rmse: 0.7996 - mcrmse: 0.6474\n",
      "Epoch 2 avg_val_loss: 0.1949  time: 9579s\n",
      "Epoch 2 - ema_content_rmse: 0.4978 - ema_wording_rmse: 0.8179 - ema_mcrmse: 0.6578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][800/2578] Elapsed 162m 10s (remain 359m 47s) Loss: 0.0308(0.0812) Grad: 2.0814  LR: 0.00000357  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1971  time: 10263s\n",
      "Epoch 2 - content_rmse: 0.5106 - wording_rmse: 0.8138 - mcrmse: 0.6622\n",
      "Epoch 2 avg_val_loss: 0.2011  time: 10794s\n",
      "Epoch 2 - ema_content_rmse: 0.5051 - ema_wording_rmse: 0.8366 - ema_mcrmse: 0.6708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][900/2578] Elapsed 182m 25s (remain 339m 33s) Loss: 0.0773(0.0807) Grad: 3.3237  LR: 0.00000351  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2743  time: 11478s\n",
      "Epoch 2 - content_rmse: 0.5243 - wording_rmse: 1.0529 - mcrmse: 0.7886\n",
      "Epoch 2 avg_val_loss: 0.2367  time: 12009s\n",
      "Epoch 2 - ema_content_rmse: 0.5239 - ema_wording_rmse: 0.9402 - ema_mcrmse: 0.7320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1000/2578] Elapsed 202m 41s (remain 319m 19s) Loss: 0.0924(0.0811) Grad: 8.0790  LR: 0.00000345  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2103  time: 12692s\n",
      "Epoch 2 - content_rmse: 0.5455 - wording_rmse: 0.8315 - mcrmse: 0.6885\n",
      "Epoch 2 avg_val_loss: 0.2267  time: 13223s\n",
      "Epoch 2 - ema_content_rmse: 0.5435 - ema_wording_rmse: 0.8904 - ema_mcrmse: 0.7169\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1100/2578] Elapsed 222m 55s (remain 299m 3s) Loss: 0.0815(0.0821) Grad: 4.8788  LR: 0.00000338  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2322  time: 13906s\n",
      "Epoch 2 - content_rmse: 0.5536 - wording_rmse: 0.8934 - mcrmse: 0.7235\n",
      "Epoch 2 avg_val_loss: 0.2098  time: 14437s\n",
      "Epoch 2 - ema_content_rmse: 0.5338 - ema_wording_rmse: 0.8373 - ema_mcrmse: 0.6855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1200/2578] Elapsed 243m 9s (remain 278m 47s) Loss: 0.1289(0.0811) Grad: 8.0568  LR: 0.00000332  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1949  time: 15121s\n",
      "Epoch 2 - content_rmse: 0.5017 - wording_rmse: 0.8153 - mcrmse: 0.6585\n",
      "Epoch 2 avg_val_loss: 0.1961  time: 15652s\n",
      "Epoch 2 - ema_content_rmse: 0.5107 - ema_wording_rmse: 0.8143 - ema_mcrmse: 0.6625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1300/2578] Elapsed 263m 24s (remain 258m 32s) Loss: 0.0564(0.0812) Grad: 3.7745  LR: 0.00000325  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1948  time: 16337s\n",
      "Epoch 2 - content_rmse: 0.5147 - wording_rmse: 0.7985 - mcrmse: 0.6566\n",
      "Epoch 2 avg_val_loss: 0.2143  time: 16867s\n",
      "Epoch 2 - ema_content_rmse: 0.5229 - ema_wording_rmse: 0.8650 - ema_mcrmse: 0.6939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1400/2578] Elapsed 283m 39s (remain 238m 18s) Loss: 0.2687(0.0804) Grad: 6.7523  LR: 0.00000318  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1926  time: 17552s\n",
      "Epoch 2 - content_rmse: 0.5129 - wording_rmse: 0.7981 - mcrmse: 0.6555\n",
      "Epoch 2 avg_val_loss: 0.2042  time: 18083s\n",
      "Epoch 2 - ema_content_rmse: 0.5163 - ema_wording_rmse: 0.8422 - ema_mcrmse: 0.6792\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1500/2578] Elapsed 303m 55s (remain 218m 4s) Loss: 0.1037(0.0801) Grad: 9.1398  LR: 0.00000312  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1996  time: 18766s\n",
      "Epoch 2 - content_rmse: 0.5321 - wording_rmse: 0.7988 - mcrmse: 0.6654\n",
      "Epoch 2 avg_val_loss: 0.2151  time: 19298s\n",
      "Epoch 2 - ema_content_rmse: 0.5244 - ema_wording_rmse: 0.8656 - ema_mcrmse: 0.6950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1600/2578] Elapsed 324m 10s (remain 197m 49s) Loss: 0.1496(0.0806) Grad: 9.8350  LR: 0.00000305  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1736  time: 19982s\n",
      "Epoch 2 - content_rmse: 0.5274 - wording_rmse: 0.6830 - mcrmse: 0.6052\n",
      "Epoch 2 - Save Best Score: 0.6052 Model\n",
      "Epoch 2 avg_val_loss: 0.1909  time: 20517s\n",
      "Epoch 2 - ema_content_rmse: 0.5014 - ema_wording_rmse: 0.7941 - ema_mcrmse: 0.6478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1700/2578] Elapsed 344m 29s (remain 177m 36s) Loss: 0.0541(0.0818) Grad: 4.8944  LR: 0.00000298  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1898  time: 21201s\n",
      "Epoch 2 - content_rmse: 0.5020 - wording_rmse: 0.7875 - mcrmse: 0.6448\n",
      "Epoch 2 avg_val_loss: 0.1989  time: 21732s\n",
      "Epoch 2 - ema_content_rmse: 0.5019 - ema_wording_rmse: 0.8222 - ema_mcrmse: 0.6621\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1800/2578] Elapsed 364m 44s (remain 157m 21s) Loss: 0.1109(0.0816) Grad: 4.8034  LR: 0.00000291  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1971  time: 22416s\n",
      "Epoch 2 - content_rmse: 0.5235 - wording_rmse: 0.7961 - mcrmse: 0.6598\n",
      "Epoch 2 avg_val_loss: 0.1988  time: 22948s\n",
      "Epoch 2 - ema_content_rmse: 0.5190 - ema_wording_rmse: 0.8094 - ema_mcrmse: 0.6642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][1900/2578] Elapsed 385m 0s (remain 137m 6s) Loss: 0.0459(0.0822) Grad: 2.4808  LR: 0.00000284  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2147  time: 23632s\n",
      "Epoch 2 - content_rmse: 0.5083 - wording_rmse: 0.8819 - mcrmse: 0.6951\n",
      "Epoch 2 avg_val_loss: 0.2147  time: 24163s\n",
      "Epoch 2 - ema_content_rmse: 0.5141 - ema_wording_rmse: 0.8735 - ema_mcrmse: 0.6938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2000/2578] Elapsed 405m 15s (remain 116m 51s) Loss: 0.0221(0.0821) Grad: 3.8123  LR: 0.00000277  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2288  time: 24846s\n",
      "Epoch 2 - content_rmse: 0.5216 - wording_rmse: 0.9158 - mcrmse: 0.7187\n",
      "Epoch 2 avg_val_loss: 0.2097  time: 25377s\n",
      "Epoch 2 - ema_content_rmse: 0.5061 - ema_wording_rmse: 0.8634 - ema_mcrmse: 0.6848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2100/2578] Elapsed 425m 30s (remain 96m 36s) Loss: 0.0183(0.0813) Grad: 1.9427  LR: 0.00000270  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1989  time: 26062s\n",
      "Epoch 2 - content_rmse: 0.5076 - wording_rmse: 0.8192 - mcrmse: 0.6634\n",
      "Epoch 2 avg_val_loss: 0.1938  time: 26593s\n",
      "Epoch 2 - ema_content_rmse: 0.5079 - ema_wording_rmse: 0.8050 - ema_mcrmse: 0.6564\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2200/2578] Elapsed 445m 45s (remain 76m 21s) Loss: 0.1243(0.0813) Grad: 3.9129  LR: 0.00000263  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2143  time: 27276s\n",
      "Epoch 2 - content_rmse: 0.4982 - wording_rmse: 0.8888 - mcrmse: 0.6935\n",
      "Epoch 2 avg_val_loss: 0.1993  time: 27807s\n",
      "Epoch 2 - ema_content_rmse: 0.5057 - ema_wording_rmse: 0.8290 - ema_mcrmse: 0.6674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2300/2578] Elapsed 466m 0s (remain 56m 5s) Loss: 0.0239(0.0809) Grad: 2.1089  LR: 0.00000256  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2167  time: 28491s\n",
      "Epoch 2 - content_rmse: 0.5065 - wording_rmse: 0.8791 - mcrmse: 0.6928\n",
      "Epoch 2 avg_val_loss: 0.2018  time: 29022s\n",
      "Epoch 2 - ema_content_rmse: 0.4956 - ema_wording_rmse: 0.8379 - ema_mcrmse: 0.6668\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2400/2578] Elapsed 486m 13s (remain 35m 50s) Loss: 0.0574(0.0807) Grad: 2.7872  LR: 0.00000248  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1949  time: 29704s\n",
      "Epoch 2 - content_rmse: 0.4885 - wording_rmse: 0.8189 - mcrmse: 0.6537\n",
      "Epoch 2 avg_val_loss: 0.1916  time: 30236s\n",
      "Epoch 2 - ema_content_rmse: 0.4928 - ema_wording_rmse: 0.8029 - ema_mcrmse: 0.6478\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2500/2578] Elapsed 506m 28s (remain 15m 35s) Loss: 0.0087(0.0806) Grad: 2.0328  LR: 0.00000241  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.2232  time: 30919s\n",
      "Epoch 2 - content_rmse: 0.5175 - wording_rmse: 0.9038 - mcrmse: 0.7107\n",
      "Epoch 2 avg_val_loss: 0.2167  time: 31451s\n",
      "Epoch 2 - ema_content_rmse: 0.5059 - ema_wording_rmse: 0.8877 - ema_mcrmse: 0.6968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][2577/2578] Elapsed 526m 8s (remain 0m 0s) Loss: 0.0375(0.0803) Grad: 2.2992  LR: 0.00000236  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2 avg_val_loss: 0.1797  time: 32100s\n",
      "Epoch 2 - content_rmse: 0.4933 - wording_rmse: 0.7520 - mcrmse: 0.6226\n",
      "Epoch 2 avg_val_loss: 0.1950  time: 32631s\n",
      "Epoch 2 - ema_content_rmse: 0.4998 - ema_wording_rmse: 0.8110 - ema_mcrmse: 0.6554\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/2578] Elapsed 0m 1s (remain 62m 22s) Loss: 0.0285(0.0285) Grad: 1.9828  LR: 0.00000236  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.1803  time: 533s\n",
      "Epoch 3 - content_rmse: 0.4935 - wording_rmse: 0.7540 - mcrmse: 0.6238\n",
      "Epoch 3 avg_val_loss: 0.1947  time: 1064s\n",
      "Epoch 3 - ema_content_rmse: 0.4997 - ema_wording_rmse: 0.8099 - ema_mcrmse: 0.6548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][100/2578] Elapsed 20m 16s (remain 497m 11s) Loss: 0.0139(0.0470) Grad: 3.3137  LR: 0.00000228  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2226  time: 1748s\n",
      "Epoch 3 - content_rmse: 0.5286 - wording_rmse: 0.8889 - mcrmse: 0.7088\n",
      "Epoch 3 avg_val_loss: 0.2073  time: 2280s\n",
      "Epoch 3 - ema_content_rmse: 0.5150 - ema_wording_rmse: 0.8474 - ema_mcrmse: 0.6812\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][200/2578] Elapsed 40m 31s (remain 479m 13s) Loss: 0.1049(0.0457) Grad: 5.6100  LR: 0.00000221  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2106  time: 2962s\n",
      "Epoch 3 - content_rmse: 0.5138 - wording_rmse: 0.8578 - mcrmse: 0.6858\n",
      "Epoch 3 avg_val_loss: 0.2071  time: 3493s\n",
      "Epoch 3 - ema_content_rmse: 0.5087 - ema_wording_rmse: 0.8509 - ema_mcrmse: 0.6798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][300/2578] Elapsed 60m 45s (remain 459m 39s) Loss: 0.0136(0.0440) Grad: 1.8894  LR: 0.00000214  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2372  time: 4177s\n",
      "Epoch 3 - content_rmse: 0.5037 - wording_rmse: 0.9583 - mcrmse: 0.7310\n",
      "Epoch 3 avg_val_loss: 0.2171  time: 4708s\n",
      "Epoch 3 - ema_content_rmse: 0.5068 - ema_wording_rmse: 0.8873 - ema_mcrmse: 0.6971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][400/2578] Elapsed 81m 0s (remain 439m 47s) Loss: 0.0398(0.0454) Grad: 4.6294  LR: 0.00000207  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2047  time: 5393s\n",
      "Epoch 3 - content_rmse: 0.4953 - wording_rmse: 0.8514 - mcrmse: 0.6733\n",
      "Epoch 3 avg_val_loss: 0.2185  time: 5924s\n",
      "Epoch 3 - ema_content_rmse: 0.4958 - ema_wording_rmse: 0.9000 - ema_mcrmse: 0.6979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][500/2578] Elapsed 101m 16s (remain 419m 50s) Loss: 0.0383(0.0448) Grad: 1.7800  LR: 0.00000200  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2002  time: 6609s\n",
      "Epoch 3 - content_rmse: 0.5023 - wording_rmse: 0.8271 - mcrmse: 0.6647\n",
      "Epoch 3 avg_val_loss: 0.2079  time: 7140s\n",
      "Epoch 3 - ema_content_rmse: 0.4967 - ema_wording_rmse: 0.8608 - ema_mcrmse: 0.6788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][600/2578] Elapsed 121m 32s (remain 399m 47s) Loss: 0.0673(0.0438) Grad: 6.7331  LR: 0.00000193  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2038  time: 7822s\n",
      "Epoch 3 - content_rmse: 0.4958 - wording_rmse: 0.8507 - mcrmse: 0.6733\n",
      "Epoch 3 avg_val_loss: 0.2124  time: 8353s\n",
      "Epoch 3 - ema_content_rmse: 0.4954 - ema_wording_rmse: 0.8802 - ema_mcrmse: 0.6878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][700/2578] Elapsed 141m 46s (remain 379m 36s) Loss: 0.0286(0.0432) Grad: 4.4552  LR: 0.00000186  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2096  time: 9037s\n",
      "Epoch 3 - content_rmse: 0.5158 - wording_rmse: 0.8522 - mcrmse: 0.6840\n",
      "Epoch 3 avg_val_loss: 0.2008  time: 9569s\n",
      "Epoch 3 - ema_content_rmse: 0.4967 - ema_wording_rmse: 0.8360 - ema_mcrmse: 0.6663\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][800/2578] Elapsed 162m 2s (remain 359m 29s) Loss: 0.0130(0.0426) Grad: 2.4945  LR: 0.00000179  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2093  time: 10254s\n",
      "Epoch 3 - content_rmse: 0.5047 - wording_rmse: 0.8543 - mcrmse: 0.6795\n",
      "Epoch 3 avg_val_loss: 0.2030  time: 10785s\n",
      "Epoch 3 - ema_content_rmse: 0.4974 - ema_wording_rmse: 0.8442 - ema_mcrmse: 0.6708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][900/2578] Elapsed 182m 18s (remain 339m 18s) Loss: 0.1010(0.0428) Grad: 8.2593  LR: 0.00000172  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2041  time: 11470s\n",
      "Epoch 3 - content_rmse: 0.5107 - wording_rmse: 0.8379 - mcrmse: 0.6743\n",
      "Epoch 3 avg_val_loss: 0.2038  time: 12002s\n",
      "Epoch 3 - ema_content_rmse: 0.4969 - ema_wording_rmse: 0.8477 - ema_mcrmse: 0.6723\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1000/2578] Elapsed 202m 35s (remain 319m 9s) Loss: 0.0315(0.0425) Grad: 4.9682  LR: 0.00000165  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2023  time: 12688s\n",
      "Epoch 3 - content_rmse: 0.4911 - wording_rmse: 0.8423 - mcrmse: 0.6667\n",
      "Epoch 3 avg_val_loss: 0.2015  time: 13219s\n",
      "Epoch 3 - ema_content_rmse: 0.4890 - ema_wording_rmse: 0.8420 - ema_mcrmse: 0.6655\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1100/2578] Elapsed 222m 50s (remain 298m 56s) Loss: 0.0166(0.0427) Grad: 2.3395  LR: 0.00000158  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2239  time: 13902s\n",
      "Epoch 3 - content_rmse: 0.4979 - wording_rmse: 0.9128 - mcrmse: 0.7053\n",
      "Epoch 3 avg_val_loss: 0.2114  time: 14434s\n",
      "Epoch 3 - ema_content_rmse: 0.4929 - ema_wording_rmse: 0.8743 - ema_mcrmse: 0.6836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1200/2578] Elapsed 243m 5s (remain 278m 42s) Loss: 0.0111(0.0420) Grad: 1.7857  LR: 0.00000151  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2230  time: 15118s\n",
      "Epoch 3 - content_rmse: 0.5001 - wording_rmse: 0.9064 - mcrmse: 0.7033\n",
      "Epoch 3 avg_val_loss: 0.2144  time: 15649s\n",
      "Epoch 3 - ema_content_rmse: 0.4944 - ema_wording_rmse: 0.8852 - ema_mcrmse: 0.6898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1300/2578] Elapsed 263m 21s (remain 258m 29s) Loss: 0.0361(0.0415) Grad: 3.7881  LR: 0.00000145  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2133  time: 16333s\n",
      "Epoch 3 - content_rmse: 0.4857 - wording_rmse: 0.8801 - mcrmse: 0.6829\n",
      "Epoch 3 avg_val_loss: 0.2073  time: 16864s\n",
      "Epoch 3 - ema_content_rmse: 0.4898 - ema_wording_rmse: 0.8619 - ema_mcrmse: 0.6759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1400/2578] Elapsed 283m 34s (remain 238m 14s) Loss: 0.1938(0.0415) Grad: 9.0706  LR: 0.00000138  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2013  time: 17546s\n",
      "Epoch 3 - content_rmse: 0.4853 - wording_rmse: 0.8419 - mcrmse: 0.6636\n",
      "Epoch 3 avg_val_loss: 0.2021  time: 18076s\n",
      "Epoch 3 - ema_content_rmse: 0.4859 - ema_wording_rmse: 0.8447 - ema_mcrmse: 0.6653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1500/2578] Elapsed 303m 46s (remain 217m 58s) Loss: 0.0743(0.0413) Grad: 5.0483  LR: 0.00000132  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2228  time: 18758s\n",
      "Epoch 3 - content_rmse: 0.4978 - wording_rmse: 0.9101 - mcrmse: 0.7039\n",
      "Epoch 3 avg_val_loss: 0.2055  time: 19289s\n",
      "Epoch 3 - ema_content_rmse: 0.4878 - ema_wording_rmse: 0.8577 - ema_mcrmse: 0.6728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][1600/2578] Elapsed 323m 58s (remain 197m 42s) Loss: 0.0354(0.0412) Grad: 3.8115  LR: 0.00000125  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3 avg_val_loss: 0.2076  time: 19971s\n",
      "Epoch 3 - content_rmse: 0.4980 - wording_rmse: 0.8636 - mcrmse: 0.6808\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 221\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[0;32m--> 221\u001b[0m     avg_loss, best_score, best_score_ema \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_score_ema\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mema_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwandb\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[1;32m    225\u001b[0m gc\u001b[38;5;241m.\u001b[39mcollect()\n",
      "Cell \u001b[0;32mIn[18], line 93\u001b[0m, in \u001b[0;36mtrain_fn\u001b[0;34m(train_loader, model, optimizer, epoch, scheduler, device, valid_loader, start_time, best_score, best_score_ema, ema_hook, wandb, fold)\u001b[0m\n\u001b[1;32m     87\u001b[0m     LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Save Best Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     88\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m: model\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     89\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredictions\u001b[39m\u001b[38;5;124m'\u001b[39m: predictions},\n\u001b[1;32m     90\u001b[0m                CFG\u001b[38;5;241m.\u001b[39mOUTPUT_DIR \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_best\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(CFG\u001b[38;5;241m.\u001b[39mmodel_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m),fold, best_score))\n\u001b[0;32m---> 93\u001b[0m avg_val_loss, predictions, valid_labels \u001b[38;5;241m=\u001b[39m \u001b[43mvalid_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mema_hook\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mema_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# ema scoring\u001b[39;00m\n\u001b[1;32m     95\u001b[0m ema_score \u001b[38;5;241m=\u001b[39m compute_mcrmse((predictions, valid_labels))\n",
      "Cell \u001b[0;32mIn[18], line 15\u001b[0m, in \u001b[0;36mvalid_fn\u001b[0;34m(valid_loader, model, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m label \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m loss, logits \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mloss, model_output\u001b[38;5;241m.\u001b[39mlogits\n\u001b[0;32m---> 15\u001b[0m losses\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, batch_size)\n\u001b[1;32m     16\u001b[0m preds\u001b[38;5;241m.\u001b[39mappend(logits\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     17\u001b[0m labels\u001b[38;5;241m.\u001b[39mappend(label\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "BV3Rp1bbXUQ-"
   },
   "outputs": [],
   "source": [
    "## total_complex = []\n",
    "# for fold in range(4):\n",
    "#     va_data = train_df[train_df['fold'] == fold]\n",
    "#     preds = torch.load('/content/drive/MyDrive/deb_simple/microsoft_deberta-v3-large_best{}.pth'.format(fold))['predictions']\n",
    "#     va_data['preds'] = preds\n",
    "#     va_data = va_data[['id', 'preds', 'score']]\n",
    "#     print(compute_metrics(va_data['preds'].values.reshape(-1,1), va_data['score'].values))\n",
    "#     total_complex.append(va_data)\n",
    "# total_complex = pd.concat(total_complex)\n",
    "# compute_metrics(total_complex['preds'].values.reshape(-1,1), total_complex['score'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "11_KW3qSx1IK",
    "outputId": "e5423c46-d702-49da-d29b-c5d125a665e4"
   },
   "outputs": [],
   "source": [
    "# !mkdir -p /root/.kaggle\n",
    "# !cp /content/drive/MyDrive/kaggle/kaggle.json /root/.kaggle/\n",
    "# !chmod 600 /root/.kaggle/kaggle.json\n",
    "# !kaggle datasets init -p /content/drive/MyDrive/elc_mean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cN6F3A_hKthE",
    "outputId": "0358b1da-6195-44ba-8ab5-2eacb5268fff"
   },
   "outputs": [],
   "source": [
    "#!kaggle datasets create -p /content/drive/MyDrive/elc_mean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "BGinNBYJFw3O"
   },
   "outputs": [],
   "source": [
    "# deberta v3 large\n",
    "# 1.5 0.8228\n",
    "# 2 0.8197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVfKappB86jX",
    "tags": []
   },
   "source": [
    "# 1.5  8137\n",
    "#2 8175\n",
    "#2.5 8181\n",
    "#3 8181\n",
    "#3.5 8175\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
