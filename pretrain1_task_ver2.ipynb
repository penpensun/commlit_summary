{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d62c666",
   "metadata": {},
   "source": [
    "### Pretrain task\n",
    "\n",
    "**MLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ca14e",
   "metadata": {},
   "source": [
    "#### Read Feedback Prize - Evaluating Student Writing data\n",
    "\n",
    "#### Log\n",
    "\n",
    "* 09.01: do not join the text together, use short text to do MLM task, and reduce max_seq from 2048 to 1024, do not use Feedback Prize dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c1eec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "data_files = os.listdir('./input/pretrain/train')\n",
    "train_text = []\n",
    "for data_file in data_files:\n",
    "    with open('./input/pretrain/train/'+ data_file, 'r') as reader:\n",
    "        # join all text in a file together\n",
    "        #train_text += [' '.join(re.split(r'\\n+', reader.read()))]\n",
    "        # do not join the split text segments, use shorter texts for pretraining\n",
    "        train_text +=re.split(r'\\n+', reader.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7971ae5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86707"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efceda2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'That too will take away from the students learning time. Cell phones are suppose to be helpful but not disruptive.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "train_text[random.randint(0,len(train_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c79c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text[random.randint(0,len(train_text))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f62e99",
   "metadata": {},
   "source": [
    "#### Read Feedback Prize - English Language Learning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0a376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pretrain_df = pd.read_csv('./input/pretrain/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2929b142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Line 1', 'Line 3', 'Line 4']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(bool, 'Line 1\\n\\nLine 3\\rLine 4\\r\\n'.splitlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2534a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21661\n"
     ]
    }
   ],
   "source": [
    "train_lines = []\n",
    "\n",
    "for text in pretrain_df.full_text:\n",
    "    ## join all text in a file together\n",
    "    train_lines += list(filter(bool, text.splitlines()));\n",
    "    \n",
    "    ## do not join the split text segments, use shorter texts for pretraining\n",
    "    #train_lines += [' '.join(re.split(r'\\n+', text))]\n",
    "\n",
    "#print(len(train_lines))    \n",
    "#train_lines += train_text # 01.09 do not use feedback student prize dataset\n",
    "print(len(train_lines))\n",
    "train_lines = pd.DataFrame(train_lines, columns = ['train_lines'])\n",
    "\n",
    "train_lines.to_csv('./input/pretrain/train_MLM_ver3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55dae640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"Your own opinion matters do not let other people make you think your opinion does not matter or that it ain't important, you have the freedom of speech in the U.S., meaning you have the freedom to say anything you want to who you want in a whatever way you need to say or express it.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines.iloc[random.randint(0,len(train_lines))].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96e255a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74f2b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42;\n",
    "    model_name = 'microsoft/deberta-v3-large'\n",
    "    epochs = 3;\n",
    "    batch_size = 8;\n",
    "    lr = 1e-6;\n",
    "    weight_decay = 1e-6\n",
    "    max_len = 1024 # use max length as 2048 #01.09, use max_length as 1024\n",
    "    mask_prob = 0.15;\n",
    "    n_accumulate = 4\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da2f6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import os\n",
    "def set_seed(seed = CFG.seed):\n",
    "    np.random.seed(seed);\n",
    "    torch.manual_seed(seed);\n",
    "    torch.cuda.manual_seed(seed);\n",
    "    torch.backends.cudnn.deterministic = True;\n",
    "    torch.backends.cudnn.benchmark = True;\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f66fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1423: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name);\n",
    "model = AutoModelWithLMHead.from_pretrained(CFG.model_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00a45f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     1,      2, 128000,      0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = tokenizer.encode_plus('[CLS] [SEP] [MASK] [PAD]',\n",
    "                                      add_special_tokens = False,\n",
    "                                      return_tensors='pt')\n",
    "special_tokens = torch.flatten(special_tokens['input_ids'])\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f9f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaskedLabels(input_ids):\n",
    "    rand = torch.rand(input_ids.shape);\n",
    "    mask_arr = (rand < CFG.mask_prob);\n",
    "    \n",
    "    for special_token in special_tokens:\n",
    "        token = special_token.item();\n",
    "        mask_arr *= (input_ids != token);\n",
    "    selection = torch.flatten(mask_arr[0].nonzero()).tolist()\n",
    "    input_ids[selection] = 128000\n",
    "    \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "341cc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset:\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data;\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data);\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        \n",
    "        tokenized_data = self.tokenizer.encode_plus(\n",
    "                            text,\n",
    "                            max_length = CFG.max_len,\n",
    "                            truncation = True,\n",
    "                            padding = 'max_length',\n",
    "                            add_special_tokens = True,\n",
    "                            return_tensors = 'pt'\n",
    "                        )\n",
    "        input_ids = torch.flatten(tokenized_data.input_ids);\n",
    "        attention_mask = torch.flatten(tokenized_data.input_ids);\n",
    "        labels = getMaskedLabels(input_ids)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10c9bd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21386"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lines.train_lines.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99f6d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MLMDataset(train_lines.train_lines.unique(), tokenizer)\n",
    "dataloader = DataLoader(train_data, batch_size = CFG.batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22f25b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21386, 2674)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f08abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = CFG.lr, weight_decay = CFG.weight_decay);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d79718e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, device):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    for batch_num, batch in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        batch_loss = loss / CFG.n_accumulate\n",
    "        batch_losses.append(batch_loss.item())\n",
    "    \n",
    "        loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "        loop.set_postfix(loss=batch_loss.item())\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        if batch_num % CFG.n_accumulate == 0 or batch_num == len(dataloader):\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "    return np.mean(batch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca03af20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Checkpointing: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2674/2674 [3:24:02<00:00,  4.58s/it, loss=0.811]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.5129047858019595\n",
      "New Best Loss inf -> 1.5129, Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2674/2674 [3:23:59<00:00,  4.58s/it, loss=0.658]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6735168587006823\n",
      "New Best Loss 1.5129 -> 0.6735, Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 2674/2674 [3:23:55<00:00,  4.58s/it, loss=0.255]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4130115715999967\n",
      "New Best Loss 0.6735 -> 0.4130, Saving Model\n"
     ]
    }
   ],
   "source": [
    "device = CFG.device\n",
    "model.to(device)\n",
    "history = []\n",
    "best_loss = np.inf\n",
    "prev_loss = np.inf\n",
    "model.gradient_checkpointing_enable()\n",
    "print(f\"Gradient Checkpointing: {model.is_gradient_checkpointing}\")\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    loss = train_loop(model, device)\n",
    "    history.append(loss)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    if loss < best_loss:\n",
    "        print(\"New Best Loss {:.4f} -> {:.4f}, Saving Model\".format(prev_loss, loss))\n",
    "        # torch.save(model.state_dict(), \"./deberta_mlm.pt\")\n",
    "        model.save_pretrained('./input/pretrain/pretrained_model/')\n",
    "        best_loss = loss\n",
    "    prev_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fb4feff",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2449021342.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[22], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Gradient Checkpointing: True\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Gradient Checkpointing: True\n",
    "Epoch 1: 100%|██████████| 4848/4848 [9:03:47<00:00,  6.73s/it, loss=0.753]  \n",
    "Loss: 1.3033109624615007\n",
    "New Best Loss inf -> 1.3033, Saving Model\n",
    "Epoch 2: 100%|██████████| 4848/4848 [9:03:44<00:00,  6.73s/it, loss=0.404]  \n",
    "Loss: 0.6164927809336299\n",
    "New Best Loss 1.3033 -> 0.6165, Saving Model\n",
    "Epoch 3: 100%|██████████| 4848/4848 [9:03:37<00:00,  6.73s/it, loss=0.359]  \n",
    "Loss: 0.46205416228314833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "695f6b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0816e13",
   "metadata": {},
   "source": [
    "### Try to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d699139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained('./input/pretrain/pretrained_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa751b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ./input/pretrain/pretrained_model/ and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./input/pretrain/pretrained_model/', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff30cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
