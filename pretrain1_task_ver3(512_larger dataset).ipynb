{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d62c666",
   "metadata": {},
   "source": [
    "### Pretrain task\n",
    "\n",
    "**MLM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7ca14e",
   "metadata": {},
   "source": [
    "#### Read Feedback Prize - Evaluating Student Writing data\n",
    "\n",
    "#### Log\n",
    "\n",
    "* 09.01: do not join the text together, use short text to do MLM task, and reduce max_seq from 2048 to 1024, do not use Feedback Prize dataset\n",
    "* 09.10: do not join the text together, reduce max_len from 1024 to 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c1eec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "data_files = os.listdir('./input/pretrain/train')\n",
    "train_text = []\n",
    "for data_file in data_files:\n",
    "    with open('./input/pretrain/train/'+ data_file, 'r') as reader:\n",
    "        # join all text in a file together\n",
    "        #train_text += [' '.join(re.split(r'\\n+', reader.read()))]\n",
    "        # do not join the split text segments, use shorter texts for pretraining\n",
    "        train_text +=re.split(r'\\n+', reader.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7971ae5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86707"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "efceda2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Not only do traditional classes keep distractions to a minimum, they also provide multiple ways of learning material. Students are unique in how they learn, some requiring hands-on experiments while others prefer reading and lectures. Online learning oftentimes is unable to accommodate those unique patterns. From personal experience, hands-on learning is often kept to a minimum, and video-conferencing is limited as connections suffer difficulties resulting in poor audio quality. Students are left to fend for themselves in this scenario because their needs cannot be provided for. If students\\' needs aren\\'t met, the material is wasted on them as it won\\'t \"click\" in their heads. No learning is being done, so students are not benefiting like they would in an environment designed to meet their learning needs. Students who otherwise wouldn\\'t be at school due to a preexisting condition could continue to learn and avoid missing \"instruction\" time, but they aren\\'t really learning much either if there is no way to accommodate them like in a physical classroom.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "train_text[random.randint(0,len(train_text))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c79c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text[random.randint(0,len(train_text))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f62e99",
   "metadata": {},
   "source": [
    "#### Read Feedback Prize - English Language Learning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df0a376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pretrain_df = pd.read_csv('./input/pretrain/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2929b142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Line 1', 'Line 3', 'Line 4']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(bool, 'Line 1\\n\\nLine 3\\rLine 4\\r\\n'.splitlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2534a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21661\n",
      "108368\n"
     ]
    }
   ],
   "source": [
    "train_lines = []\n",
    "\n",
    "for text in pretrain_df.full_text:\n",
    "    ## join all text in a file together\n",
    "    train_lines += list(filter(bool, text.splitlines()));\n",
    "    \n",
    "    ## do not join the split text segments, use shorter texts for pretraining\n",
    "    #train_lines += [' '.join(re.split(r'\\n+', text))]\n",
    "\n",
    "print(len(train_lines))    \n",
    "train_lines += train_text\n",
    "print(len(train_lines))\n",
    "train_lines = pd.DataFrame(train_lines, columns = ['train_lines'])\n",
    "\n",
    "train_lines.to_csv('./input/pretrain/train_MLM_ver4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "55dae640",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lines.iloc[random.randint(0,len(train_lines))].values[0].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96e255a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74f2b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42;\n",
    "    model_name = 'microsoft/deberta-v3-large'\n",
    "    epochs = 3;\n",
    "    batch_size = 32;\n",
    "    lr = 1e-6;\n",
    "    weight_decay = 1e-6\n",
    "    max_len = 256 # use max length as 2048 #01.09, use max_length as 1024\n",
    "    mask_prob = 0.15;\n",
    "    n_accumulate = 4\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da2f6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import os\n",
    "def set_seed(seed = CFG.seed):\n",
    "    np.random.seed(seed);\n",
    "    torch.manual_seed(seed);\n",
    "    torch.cuda.manual_seed(seed);\n",
    "    torch.backends.cudnn.deterministic = True;\n",
    "    torch.backends.cudnn.benchmark = True;\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "44f66fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/deberta-v3-large/resolve/main/tokenizer_config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fd32cd25430>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 6673d0ef-0338-49a6-b46d-139e3c31ad1d)')' thrown while requesting HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/tokenizer_config.json\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1423: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "'(MaxRetryError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Max retries exceeded with url: /microsoft/deberta-v3-large/resolve/main/config.json (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fd32cd25520>, 'Connection to huggingface.co timed out. (connect timeout=10)'))\"), '(Request ID: 621d65df-f569-409b-a7eb-1d2502bf9c8d)')' thrown while requesting HEAD https://huggingface.co/microsoft/deberta-v3-large/resolve/main/config.json\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name);\n",
    "model = AutoModelWithLMHead.from_pretrained(CFG.model_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00a45f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     1,      2, 128000,      0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = tokenizer.encode_plus('[CLS] [SEP] [MASK] [PAD]',\n",
    "                                      add_special_tokens = False,\n",
    "                                      return_tensors='pt')\n",
    "special_tokens = torch.flatten(special_tokens['input_ids'])\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f9f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaskedLabels(input_ids):\n",
    "    rand = torch.rand(input_ids.shape);\n",
    "    mask_arr = (rand < CFG.mask_prob);\n",
    "    \n",
    "    for special_token in special_tokens:\n",
    "        token = special_token.item();\n",
    "        mask_arr *= (input_ids != token);\n",
    "    selection = torch.flatten(mask_arr[0].nonzero()).tolist()\n",
    "    input_ids[selection] = 128000\n",
    "    \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "341cc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset:\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data;\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data);\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        \n",
    "        tokenized_data = self.tokenizer.encode_plus(\n",
    "                            text,\n",
    "                            max_length = CFG.max_len,\n",
    "                            truncation = True,\n",
    "                            padding = 'max_length',\n",
    "                            add_special_tokens = True,\n",
    "                            return_tensors = 'pt'\n",
    "                        )\n",
    "        input_ids = torch.flatten(tokenized_data.input_ids);\n",
    "        attention_mask = torch.flatten(tokenized_data.input_ids);\n",
    "        labels = getMaskedLabels(input_ids)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10c9bd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101089"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_lines.train_lines.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99f6d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MLMDataset(train_lines.train_lines.unique(), tokenizer)\n",
    "dataloader = DataLoader(train_data, batch_size = CFG.batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22f25b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101089, 3160)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f08abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = CFG.lr, weight_decay = CFG.weight_decay);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d79718e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, device):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    for batch_num, batch in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        batch_loss = loss / CFG.n_accumulate\n",
    "        batch_losses.append(batch_loss.item())\n",
    "    \n",
    "        loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "        loop.set_postfix(loss=batch_loss.item())\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        if batch_num % CFG.n_accumulate == 0 or batch_num == len(dataloader):\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "    return np.mean(batch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca03af20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Checkpointing: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 3160/3160 [2:41:01<00:00,  3.06s/it, loss=1.18]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.5799526847804648\n",
      "New Best Loss inf -> 1.5800, Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 3160/3160 [2:41:01<00:00,  3.06s/it, loss=0.796]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.9484926597038402\n",
      "New Best Loss 1.5800 -> 0.9485, Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 3160/3160 [2:41:08<00:00,  3.06s/it, loss=1.17]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.759340821395192\n",
      "New Best Loss 0.9485 -> 0.7593, Saving Model\n"
     ]
    }
   ],
   "source": [
    "device = CFG.device\n",
    "model.to(device)\n",
    "history = []\n",
    "best_loss = np.inf\n",
    "prev_loss = np.inf\n",
    "model.gradient_checkpointing_enable()\n",
    "print(f\"Gradient Checkpointing: {model.is_gradient_checkpointing}\")\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    loss = train_loop(model, device)\n",
    "    history.append(loss)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    if loss < best_loss:\n",
    "        print(\"New Best Loss {:.4f} -> {:.4f}, Saving Model\".format(prev_loss, loss))\n",
    "        # torch.save(model.state_dict(), \"./deberta_mlm.pt\")\n",
    "        model.save_pretrained('./input/pretrain/pretrained_model_2109/')\n",
    "        best_loss = loss\n",
    "    prev_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb4feff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Gradient Checkpointing: True\n",
    "Epoch 1: 100%|██████████| 4848/4848 [9:03:47<00:00,  6.73s/it, loss=0.753]  \n",
    "Loss: 1.3033109624615007\n",
    "New Best Loss inf -> 1.3033, Saving Model\n",
    "Epoch 2: 100%|██████████| 4848/4848 [9:03:44<00:00,  6.73s/it, loss=0.404]  \n",
    "Loss: 0.6164927809336299\n",
    "New Best Loss 1.3033 -> 0.6165, Saving Model\n",
    "Epoch 3: 100%|██████████| 4848/4848 [9:03:37<00:00,  6.73s/it, loss=0.359]  \n",
    "Loss: 0.46205416228314833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "695f6b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DebertaV2ForMaskedLM(\n",
       "  (deberta): DebertaV2Model(\n",
       "    (embeddings): DebertaV2Embeddings(\n",
       "      (word_embeddings): Embedding(128100, 1024, padding_idx=0)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "      (dropout): StableDropout()\n",
       "    )\n",
       "    (encoder): DebertaV2Encoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x DebertaV2Layer(\n",
       "          (attention): DebertaV2Attention(\n",
       "            (self): DisentangledSelfAttention(\n",
       "              (query_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (pos_dropout): StableDropout()\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "            (output): DebertaV2SelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "              (dropout): StableDropout()\n",
       "            )\n",
       "          )\n",
       "          (intermediate): DebertaV2Intermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): DebertaV2Output(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "            (dropout): StableDropout()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (rel_embeddings): Embedding(512, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (cls): DebertaV2OnlyMLMHead(\n",
       "    (predictions): DebertaV2LMPredictionHead(\n",
       "      (transform): DebertaV2PredictionHeadTransform(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((1024,), eps=1e-07, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=1024, out_features=128100, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0816e13",
   "metadata": {},
   "source": [
    "### Try to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d699139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoConfig\n",
    "# config = AutoConfig.from_pretrained('./input/pretrain/pretrained_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa751b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ./input/pretrain/pretrained_model/ and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoModelForSequenceClassification\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('./input/pretrain/pretrained_model/', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff30cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
