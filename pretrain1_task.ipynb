{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9153be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df0a376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_df = pd.read_csv('./input/pretrain/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2929b142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Line 1', 'Line 3', 'Line 4']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(bool, 'Line 1\\n\\nLine 3\\rLine 4\\r\\n'.splitlines()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d2534a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines = []\n",
    "\n",
    "for text in pretrain_df.full_text:\n",
    "    train_lines += list(filter(bool, text.splitlines()));\n",
    "    \n",
    "train_lines = pd.DataFrame(train_lines, columns = ['train_lines'])\n",
    "train_lines.to_csv('./input/pretrain/train_MLM.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96e255a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd;\n",
    "import numpy as np;\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74f2b4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42;\n",
    "    model_name = 'microsoft/deberta-v3-large'\n",
    "    epochs = 3;\n",
    "    batch_size = 4;\n",
    "    lr = 1e-6;\n",
    "    weight_decay = 1e-6\n",
    "    max_len = 512\n",
    "    mask_prob = 0.15;\n",
    "    n_accumulate = 4\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da2f6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np;\n",
    "import os\n",
    "def set_seed(seed = CFG.seed):\n",
    "    np.random.seed(seed);\n",
    "    torch.manual_seed(seed);\n",
    "    torch.cuda.manual_seed(seed);\n",
    "    torch.backends.cudnn.deterministic = True;\n",
    "    torch.backends.cudnn.benchmark = True;\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44f66fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:1423: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "Some weights of DebertaV2ForMaskedLM were not initialized from the model checkpoint at microsoft/deberta-v3-large and are newly initialized: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.model_name);\n",
    "model = AutoModelWithLMHead.from_pretrained(CFG.model_name);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00a45f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     1,      2, 128000,      0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_tokens = tokenizer.encode_plus('[CLS] [SEP] [MASK] [PAD]',\n",
    "                                      add_special_tokens = False,\n",
    "                                      return_tensors='pt')\n",
    "special_tokens = torch.flatten(special_tokens['input_ids'])\n",
    "special_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8f9f7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaskedLabels(input_ids):\n",
    "    rand = torch.rand(input_ids.shape);\n",
    "    mask_arr = (rand < CFG.mask_prob);\n",
    "    \n",
    "    for special_token in special_tokens:\n",
    "        token = special_token.item();\n",
    "        mask_arr *= (input_ids != token);\n",
    "    selection = torch.flatten(mask_arr[0].nonzero()).tolist()\n",
    "    input_ids[selection] = 128000\n",
    "    \n",
    "    return input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "341cc347",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLMDataset:\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data;\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data);\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        \n",
    "        tokenized_data = self.tokenizer.encode_plus(\n",
    "                            text,\n",
    "                            max_length = CFG.max_len,\n",
    "                            truncation = True,\n",
    "                            padding = 'max_length',\n",
    "                            add_special_tokens = True,\n",
    "                            return_tensors = 'pt'\n",
    "                        )\n",
    "        input_ids = torch.flatten(tokenized_data.input_ids);\n",
    "        attention_mask = torch.flatten(tokenized_data.input_ids);\n",
    "        labels = getMaskedLabels(input_ids)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10c9bd66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"I think that students would benefit from learning at home,because they wont have to change and get up early in the morning to shower and do there hair. taking only classes helps them because at there house they'll be pay more attention. they will be comfortable at home.\",\n",
       "       \"The hardest part of school is getting ready. you wake up go brush your teeth and go to your closet and look at your cloths. after you think you picked a outfit u go look in the mirror and youll either not like it or you look and see a stain. Then you'll have to change. with the online classes you can wear anything and stay home and you wont need to stress about what to wear.\",\n",
       "       'most students usually take showers before school. they either take it before they sleep or when they wake up. some students do both to smell good. that causes them do miss the bus and effects on there lesson time cause they come late to school. when u have online classes u wont need to miss lessons cause you can get everything set up and go take a shower and when u get out your ready to go.',\n",
       "       ...,\n",
       "       \"Finally, they always need to keep in mind that there always would be a chance of failure and they have the right to be afraid but, that's something they need to live aside and they need to be focus on their goal,\",\n",
       "       \"they need to have the power to stop themselves whenever they want, and they have to be the ones to say to their self to stop when they think they already accomplish what they want but, they can't let people decide for them. Because sometimes people get jealous to see the other one's success in life and they try to guide people into failure.\",\n",
       "       \"In conclusion, there have been nothing that can't stop people to reach their goal's and success in life because everything is possible, people have to believe in them self and always be positive they always need to keep in mind that they are capable's of everything and they need to learn to not give up. In that way they would be able to do everything and be successful the key is always have and excellent attitude and be prepared for people to judge, and be prepared to failure and get up and make it see like their have been nothing wrong and keep going. They need to be positive in every movement they make and be positive in the way that everything is going to be exactly how the planned.\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_lines.train_lines.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99f6d96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MLMDataset(train_lines.train_lines.unique(), tokenizer)\n",
    "dataloader = DataLoader(train_data, batch_size = CFG.batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22f25b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21386, 5347)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data), len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f08abac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr = CFG.lr, weight_decay = CFG.weight_decay);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d79718e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, device):\n",
    "    model.train()\n",
    "    batch_losses = []\n",
    "    loop = tqdm(dataloader, leave=True)\n",
    "    for batch_num, batch in enumerate(loop):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        batch_loss = loss / CFG.n_accumulate\n",
    "        batch_losses.append(batch_loss.item())\n",
    "    \n",
    "        loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "        loop.set_postfix(loss=batch_loss.item())\n",
    "        batch_loss.backward()\n",
    "        \n",
    "        if batch_num % CFG.n_accumulate == 0 or batch_num == len(dataloader):\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "\n",
    "    return np.mean(batch_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ca03af20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Checkpointing: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 5347/5347 [1:22:51<00:00,  1.08it/s, loss=0.553]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.1724517982051108\n",
      "New Best Loss inf -> 1.1725, Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 5347/5347 [1:22:50<00:00,  1.08it/s, loss=0.521]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.4743986433357619\n",
      "New Best Loss 1.1725 -> 0.4744, Saving Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 5347/5347 [1:22:50<00:00,  1.08it/s, loss=0.311]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.33300676586829947\n",
      "New Best Loss 0.4744 -> 0.3330, Saving Model\n"
     ]
    }
   ],
   "source": [
    "device = CFG.device\n",
    "model.to(device)\n",
    "history = []\n",
    "best_loss = np.inf\n",
    "prev_loss = np.inf\n",
    "model.gradient_checkpointing_enable()\n",
    "print(f\"Gradient Checkpointing: {model.is_gradient_checkpointing}\")\n",
    "\n",
    "for epoch in range(CFG.epochs):\n",
    "    loss = train_loop(model, device)\n",
    "    history.append(loss)\n",
    "    print(f\"Loss: {loss}\")\n",
    "    if loss < best_loss:\n",
    "        print(\"New Best Loss {:.4f} -> {:.4f}, Saving Model\".format(prev_loss, loss))\n",
    "        # torch.save(model.state_dict(), \"./deberta_mlm.pt\")\n",
    "        model.save_pretrained('./input/pretrain/pretrained_model/')\n",
    "        best_loss = loss\n",
    "    prev_loss = loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "695f6b6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0816e13",
   "metadata": {},
   "source": [
    "### Try to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d699139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "config = AutoConfig.from_pretrained('./input/pretrain/pretrained_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa751b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at ./input/pretrain/pretrained_model/ and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained('./input/pretrain/pretrained_model/', config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff30cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
